export const timetableData = {
    '20230508': {
      b12084: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 30.0,
        endDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12084',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom (3rd floor)',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-08',
          time: '10:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'AM Break',
        uniqueId: 'b12084',
      },
      b12090: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12090',
        inheritLoc: false,
        inheritRoom: false,
        location: '',
        room: '',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'Lunch on Own',
        uniqueId: 'b12090',
      },
      b12093: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#8ec473',
        conferenceId: 459,
        description: '',
        duration: 60.0,
        endDate: {
          date: '2023-05-08',
          time: '19:30:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12093',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom \u0026 Foyer (3rd floor)',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-08',
          time: '18:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#021f03',
        title: 'Reception',
        uniqueId: 'b12093',
      },
      b12095: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 30.0,
        endDate: {
          date: '2023-05-08',
          time: '16:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12095',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom (3rd floor)',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'PM Break',
        uniqueId: 'b12095',
      },
      s11977: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'Olde Dominion University',
            displayOrderKey: [0, 'Dodge, Gail'],
            emailHash: '832ea7136353a537ddffc87165395e92',
            familyName: 'Dodge',
            firstName: 'Gail',
            name: 'Gail Dodge',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '10:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17264: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12488/attachments/9272/13660/CHEP23-LOC-welcome.pdf',
                  id: 13660,
                  title: 'CHEP23-LOC-welcome.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12488,
            description: '',
            duration: 30.0,
            endDate: {
              date: '2023-05-08',
              time: '09:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 642,
            id: 'c17264',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12488/contribution.pdf',
            presenters: [
              {
                affiliation: 'JLAB',
                displayOrderKey: [0, 'Boehnlein, Amber'],
                emailHash: '124394665132c96fac6db265b8f6d4ba',
                familyName: 'Boehnlein',
                firstName: 'Amber',
                name: 'Amber Boehnlein',
              },
              {
                affiliation: 'Old Dominion University',
                displayOrderKey: [0, 'Dodge, Gail'],
                emailHash: '832ea7136353a537ddffc87165395e92',
                familyName: 'Dodge',
                firstName: 'Gail',
                name: 'Gail Dodge',
              },
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [0, 'Henderson, Stuart'],
                emailHash: '6fcb6c4573b02b3ab479d7ce28db75e7',
                familyName: 'Henderson',
                firstName: 'Stuart',
                name: 'Stuart Henderson',
              },
              {
                affiliation: 'United States Congressman',
                displayOrderKey: [0, 'Scott, Bobby'],
                emailHash: null,
                familyName: 'Scott',
                firstName: 'Bobby',
                name: 'Bobby Scott',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 11977,
            sessionSlotId: 2639,
            startDate: {
              date: '2023-05-08',
              time: '09:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Welcome and Introduction to CHEP23',
            uniqueId: 'c17264',
            url: '/event/459/contributions/12488/',
          },
          c17265: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12489/attachments/9224/13387/20230508%20CHEP%20Dean.pptx',
                  id: 13387,
                  title: '20230508 CHEP Dean.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12489,
            description:
              'The world is full of computing devices that calculate, monitor, analyze, and control processes. The underlying technical advances within computing hardware have been further enhanced by tremendous algorithmic advances across the spectrum of the sciences. The quest -- ever present in humans -- to push the frontiers of knowledge and understanding requires continuing advances in the development and use of computation, with an increasing emphasis on the analysis of complex data originating from experiments and observations. How this move toward data intensive computing affects our underlying processes in the sciences remains to be fully appreciated. In this talk, I will briefly describe how we arrived at this point, and also give a prospective toward the end of the talk.',
            duration: 30.0,
            endDate: {
              date: '2023-05-08',
              time: '10:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 643,
            id: 'c17265',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12489/contribution.pdf',
            presenters: [
              {
                affiliation: 'TJNAF',
                displayOrderKey: [0, 'Dean, David'],
                emailHash: 'ae62fd843f77523fe28941f10b441b71',
                familyName: 'Dean',
                firstName: 'David',
                name: 'David Dean',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 11977,
            sessionSlotId: 2639,
            startDate: {
              date: '2023-05-08',
              time: '09:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Keynote: Evolutions and Revolutions in Computing: Science at the Frontier',
            uniqueId: 'c17265',
            url: '/event/459/contributions/12489/',
          },
          c17266: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12490/attachments/9468/13733/Diefenthaler-CHEP2023-Keynote.pdf',
                  id: 13733,
                  title: 'Diefenthaler-CHEP2023-Keynote.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12490,
            description:
              'In today\u0027s Nuclear Physics (NP), the exploration of the origin, evolution, and structure of the universe\u0027s matter is pursued through a broad research program at various collaborative scales, ranging from small groups to large experiments comparable in size to those in high-energy physics (HEP). Consequently, software and computing efforts vary from DIY approaches among a few researchers to well-organized activities within large experiments. With new experiments underway and on the horizon, and data volumes rapidly increasing even at small experiments, the NP community has been considering the next generation of data processing and analysis workflows that will optimize scientific output. In my keynote, I will discuss the unique aspects of software and computing in NP and explore how the NP community can strengthen collective efforts to chart a path forward for the next decade. This decade promises to be an exciting one, with diverse scientific programs ongoing at facilities such as CEBAF, FRIB, RHIC, and many others. I will also demonstrate how this path informs the software and computing at the future Electron-Ion Collider.\u00b7',
            duration: 30.0,
            endDate: {
              date: '2023-05-08',
              time: '10:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 644,
            id: 'c17266',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12490/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [0, 'Diefenthaler, Markus'],
                emailHash: 'af834e0ec8ce37c7bc6e53f554561c99',
                familyName: 'Diefenthaler',
                firstName: 'Markus',
                name: 'Markus Diefenthaler',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 11977,
            sessionSlotId: 2639,
            startDate: {
              date: '2023-05-08',
              time: '10:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Kenote: Future Trends in Nuclear Physics Computing',
            uniqueId: 'c17266',
            url: '/event/459/contributions/12490/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's11977',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2639,
        slotTitle: 'Welcome / Highlighted Keynotes',
        startDate: {
          date: '2023-05-08',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's11977',
        url: '/event/459/sessions/2024/',
      },
      s12083: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfdfdf',
        conferenceId: 459,
        contribDuration: 600.0,
        conveners: [],
        description: '',
        duration: 60.0,
        endDate: {
          date: '2023-05-08',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        entries: {},
        entryType: 'Session',
        friendlyId: 4,
        id: 's12083',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2034/session-timetable.pdf',
        room: '',
        sessionCode: '',
        sessionId: 2034,
        sessionSlotId: 2658,
        slotTitle: 'Registration',
        startDate: {
          date: '2023-05-08',
          time: '08:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#151515',
        title: 'Registration',
        uniqueId: 's12083',
        url: '/event/459/sessions/2034/',
      },
      s12085: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#6f390d',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Lassnig, Mario'],
            emailHash: 'ef2db26b320ecf7a9b75abbaef349287',
            familyName: 'Lassnig',
            firstName: 'Mario',
            name: 'Mario Lassnig',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Barisits, Martin'],
            emailHash: '46207806844c107a8fefe566f669b206',
            familyName: 'Barisits',
            firstName: 'Martin',
            name: 'Martin Barisits',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16929: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11300/attachments/9368/13685/dcache-sci-storage.pdf',
                  id: 13685,
                  title: 'dcache-sci-storage.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11300/attachments/9368/13686/dcache-sci-storage.pptx',
                  id: 13686,
                  title: 'dcache-sci-storage.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11300,
            description:
              'The dCache project provides open-source software deployed internationally to satisfy\r\never more demanding storage requirements. Its multifaceted approach provides an integrated\r\nway of supporting different use-cases with the same storage, from high throughput data\r\ningest, data sharing over wide area networks, efficient access from HPC clusters and long\r\nterm data persistence on a tertiary storage. Though it was originally developed for the\r\nHEP experiments, today it is used by various scientific communities, including astrophysics,\r\nbiomed, life science, which have their specific requirements. With this contribution we\r\nwould like to highlight the recent developments in the dCache regarding integration with\r\nCERN Tape Archive (CTA), advanced metadata handling, bulk API for QoS transitions, RESTAPI\r\nto control interaction with tape system and the future development directions.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 25,
            id: 'c16929',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11300/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [5, 'Litvintsev, Dmitry'],
                emailHash: '6400c2cc73bf4ffb1d6d4985b52d6786',
                familyName: 'Litvintsev',
                firstName: 'Dmitry',
                name: 'Mr Dmitry Litvintsev',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12085,
            sessionSlotId: 2659,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'dCache: storage system for data intensive science',
            uniqueId: 'c16929',
            url: '/event/459/contributions/11300/',
          },
          c16930: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11325/attachments/9234/13400/CHEP2023ErasureCodingXrootdObjectStore.v1.pdf',
                  id: 13400,
                  title: 'CHEP2023ErasureCodingXrootdObjectStore.v1.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11325/attachments/9234/13399/go',
                  id: 13399,
                  title: 'Xrootd EC',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11325,
            description:
              'XRootD implemented a client-side erasure coding (EC) algorithm utilizing the Intel Intelligent Storage Acceleration Library. At SLAC, a prototype of XRootD EC storage was set up for evaluation. The architecture and configuration of the prototype is almost identical to that of a traditional non-EC XRootD storage behind a firewall: a backend XRootD storage cluster in its simplest form, and an internet facing XRootD proxy that handles EC and spreads the data stripes of a file/object across several backend nodes. This prototype supports all functions used on a WLCG storage system: HTTP(s) and XRootD protocols, Third Party Copy, X509/VOMS/Token, etc. The cross-node EC architecture brings significant advantages in both performance and resilience: e.g. parallel data access, tolerance of downtime and hardware failure. This paper will describe the prototype\u2019s architecture and its design choices, the performance in high concurrent throughputs and file/object operations, failure modes and their handling, data recovery methods, and administration. This paper also describes the work that explores the HTTP protocol feature in XRootD to support data access via industry standard Boto3 S3 client library.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 105,
            id: 'c16930',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11325/contribution.pdf',
            presenters: [
              {
                affiliation: 'SLAC National Accelerator Laboratory',
                displayOrderKey: [2, 'Yang, Wei'],
                emailHash: '454586d94ec6a7ebad3c68ceb1774c96',
                familyName: 'Yang',
                firstName: 'Wei',
                name: 'Wei Yang',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12085,
            sessionSlotId: 2659,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Erasure Coding Xrootd Object Store',
            uniqueId: 'c16930',
            url: '/event/459/contributions/11325/',
          },
          c16931: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11312/attachments/9255/13624/POSIX_access_to_remote_storage_with_OIDC_AuthN_AuthZ.pdf',
                  id: 13624,
                  title: 'POSIX_access_to_remote_storage_with_OIDC_AuthN_AuthZ.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11312,
            description:
              'INFN-CNAF is one of the Worldwide LHC Computing Grid (WLCG) Tier-1 data centers, providing support in terms of computing, networking, storage resources and services also to a wide variety of scientific collaborations, ranging from physics to bioinformatics and industrial engineering.\r\nRecently, several collaborations working with our data center have developed computing and data management workflows that require access to S3 storage services and the integration with POSIX capabilities.\r\nTo accomplish this requirement in distributed environments, where computing and storage resources are located at geographically distant physical sites, the possibility to locally mount a file system from a remote site to directly perform operations on files and directories becomes crucial.\r\nNevertheless, the access to the data must be regulated by standard, federated authentication and authorization mechanisms, such as OpenID Connect (OIDC), which is already adopted as AuthN/AuthZ mechanism within WLCG and the European Open Science Cloud (EOSC).\r\nStarting from such principles, we evaluated the possibility to regulate data access by integrating JSON Web Token (JWT) authentication, provided by INDIGO-IAM as Identity Provider (IdP), with solutions based on S3 (for object storage) and HTTP (for hierarchical storage) protocols.\r\nIn particular, in regard to S3 data exposition, we integrated MinIO and CEPH RADOS Gateway with s3fs-fuse, providing the needed custom libraries to mount an S3 bucket via FUSE by preserving the native object format for files. Both solutions support Secure Token Service (STS), providing a client with temporary credentials to perform a given operation on a storage resource by checking the value of a JWT claim associated with the request. \r\nNative MinIO STS does not support IAM JWT profile, thus we delegated STS service to Hashicorp Vault in the case of MinIO.\r\nRADOS Gateway is an object storage interface for Ceph. It provides a RESTful S3-compatible API and a feature for integration with OIDC IdP. Access tokens produced for OIDC clients can be used by the STS implemented within RADOS Gateway for authorizing specific S3 operations. \r\nOn the other hand, HTTP data access has been managed by using Rclone and WebDAV protocol, to mount a storage area via INDIGO-IAM token authentication. In this case the storage area is exposed via HTTP by using the StoRM-WebDAV application, but the solution is general enough to be used with other HTTP data management servers (e.g. Apache, NGINX). \r\nIn such respect, a comparison between the performances yielded by S3 and WebDAV protocols has been carried out within the same Red Hat OpenShift environment, in order to better understand which solution is most suitable for each of the use cases of interest.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 433,
            id: 'c16931',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11312/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN-CNAF',
                displayOrderKey: [1, 'Fornari, Federico'],
                emailHash: '9c0af1fbe3d7736bd5994d266db6bd1f',
                familyName: 'Fornari',
                firstName: 'Federico',
                name: 'Dr Federico Fornari',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12085,
            sessionSlotId: 2659,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'POSIX access to remote storage with OpenID Connect AuthN/AuthZ',
            uniqueId: 'c16931',
            url: '/event/459/contributions/11312/',
          },
          c16932: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11323/attachments/9348/13551/CHEP,%20Ceph%20BCDR.pdf',
                  id: 13551,
                  title: 'CHEP, Ceph BCDR.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11323,
            description:
              'The Storage Group in the CERN IT Department operates several Ceph storage clusters with an overall capacity exceeding 100 PB. Ceph is a crucial component of the infrastructure delivering IT services to all the users of the Organization as it provides: i) Block storage for the OpenStack infrastructure, ii) CephFS used as persistent storage by containers (OpenShift and Kubernetes) and as shared filesystems by HPC clusters, and iii) S3 object storage for cloud-native applications, monitoring, and software distribution across the WLCG.\r\n\r\nThe Ceph infrastructure at CERN has been rationalized and restructured to offer storage solutions for high(er) availability and Disaster Recovery / Business Continuity. In this contribution, we give an overview of how we transitioned from a single RBD zone to multiple ones enabling Storage Availability zones and how RBD mirroring functionalities available in Ceph upstream have been hardened. Also, we illustrate future plans for storage BC/DR including backups via restic to S3 and Tape, replication of objects across multiple storage zones, and the instantiation of clusters spanning different computing centres.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 276,
            id: 'c16932',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11323/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Bocchi, Enrico'],
                emailHash: '305e6a33685e9a0fc98912c9f4c52cf0',
                familyName: 'Bocchi',
                firstName: 'Enrico',
                name: 'Enrico Bocchi',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12085,
            sessionSlotId: 2659,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title:
              'Enabling Storage Business Continuity and Disaster Recovery with Ceph distributed storage',
            uniqueId: 'c16932',
            url: '/event/459/contributions/11323/',
          },
          c16933: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11324/attachments/9407/13786/Walder_CHEP23_echo_final.pdf',
                  id: 13786,
                  title: 'Walder_CHEP23_echo_final.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11324,
            description:
              'Data access at the UK Tier-1 facility at RAL is provided through its ECHO storage, serving the requirements for the WLGC and increasing numbers of other HEP and astronomy related communities.\r\nECHO is a Ceph-backed erasure-coded object store, currently providing in excess of 40PB of usable space, with frontend access to data provided via XRootD or gridFTP, using the libradosstriper library of Ceph. \r\nThe storage must service the needs of: high-throughput compute, with staged and direct file access passing through an XCache on each workernode; data access to compute running at storageless satellite sites;  and, managed inter-site data transfers using the recently adopted HTTPS protocol (via WebDav), which includes multi-hop data transfers to and from RAL\u2019s newly commissioned CTA tape endpoint.\r\nA review of the experiences of providing data access via an object store within these data workflows is presented, including the details of the improvements necessary for the transition to WebDav, used for most inter-site data movements, and enhancements for direct-IO file access, where the development and optimisation of buffering and range coalescence strategies is explored.\r\nIn addition to serving the requirements of LHC Run-3, preparations for Run-4 and for large astronomy experiments is underway. One example is with ROOT-based data formats, where the evolution from a TTree to RNTuple data structure provides an opportunity  for storage providers to benchmark and optimise against this new format. A comparison of the current performance between data formats within ECHO is presented and the details of potential improvements presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 606,
            id: 'c16933',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11324/contribution.pdf',
            presenters: [
              {
                affiliation: 'RAL, STFC, UKRI',
                displayOrderKey: [0, 'Walder, James'],
                emailHash: '577b56f3b0e849a03f951b8a5e9e8e2b',
                familyName: 'Walder',
                firstName: 'James',
                name: 'James Walder',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12085,
            sessionSlotId: 2659,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title:
              'ECHO: optimising object store data access for Run-3  and the evolution towards HL-LHC',
            uniqueId: 'c16933',
            url: '/event/459/contributions/11324/',
          },
          c16934: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11327/attachments/9297/13491/CHEP%202023%20-%20EOS%20Software%20Evolution%20Enabling%20LHC%20Run-3.pdf',
                  id: 13491,
                  title: 'CHEP 2023 - EOS Software Evolution Enabling LHC Run-3.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11327/attachments/9297/13492/go',
                  id: 13492,
                  title: 'Google slides presentation',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11327,
            description:
              'EOS has been the main storage system at CERN for more than a decade, continuously improving in order to meet the ever evolving requirements of the LHC experiments and the whole physics user community. In order to satisfy the demands of LHC Run-3, in terms of storage performance and tradeoff between cost and capacity, EOS was enhanced with a set of new functionalities and features that we will detail in this paper.\r\n\r\nFirst of all, we describe the use of erasure coded layouts in a large-scale deployment which enables an efficient use of available storage capacity, while at the same time providing end-users with better throughput when accessing their data. This new operating model implies more coupling between the machines in a cluster, which in turn leads to the next set of EOS improvements that we discuss, targeting I/O traffic shaping, better I/O scheduling policies and tagged traffic prioritization. Increasing the size of the EOS clusters to cope with experiment demands, means stringent constraints on the data integrity and durability that we addressed by a re-designed consistency check engine. Another focus area of EOS development was to minimize the operational load by making the internal operational procedures (draining, balancing or conversions) more robust and efficient, to allow  managing easily multiple clusters and avoid possible scaling issues. \r\n\r\nAll these improvements available in the EOS 5 release series, are coupled with the new XRootD 5 framework which brings additional security features like TLS support and optimizations for large data transfers like page read and page write functionalities. Last but not least, the area of authentication/authorization methods has seen important developments by adding support for different types of bearer tokens that we will describe along with EOS specific token extensions. We conclude by highlighting potential areas of the EOS architecture that might require further developments or re-design in order to cope with the ever-increasing demands of our end-users.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 209,
            id: 'c16934',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11327/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Caffy, Cedric'],
                emailHash: 'bad35bb0a1c7724b7d80fa3f1d8edd33',
                familyName: 'Caffy',
                firstName: 'Cedric',
                name: 'Mr Cedric Caffy',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12085,
            sessionSlotId: 2659,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'EOS Software Evolution Enabling LHC Run-3',
            uniqueId: 'c16934',
            url: '/event/459/contributions/11327/',
          },
        },
        entryType: 'Session',
        friendlyId: 5,
        id: 's12085',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2035/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2035,
        sessionSlotId: 2659,
        slotTitle: 'Storage',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffeddf',
        title: 'Track 1 - Data and Metadata Organization, Management and Access',
        uniqueId: 's12085',
        url: '/event/459/sessions/2035/',
      },
      s12086: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d0c296',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Universite de Geneve (CH)',
            displayOrderKey: [0, 'Antel, Claire'],
            emailHash: '0700a8e2f8e6dd930897159bde503e7d',
            familyName: 'Antel',
            firstName: 'Claire',
            name: 'Claire Antel',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Pantaleo, Felice'],
            emailHash: '27ffa3f8059859baeebd18c023b215a8',
            familyName: 'Pantaleo',
            firstName: 'Felice',
            name: 'Felice Pantaleo',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16994: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11394/attachments/9352/13557/fpisani_CHEP_2023.pdf',
                  id: 13557,
                  title: 'fpisani_CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11394,
            description:
              'The LHCb experiment is one of the four large experiments on the LHC at CERN. This forward spectrometer is designed to investigate differences between matter and antimatter by studying beauty and charm Physics. The detector and the entire DAQ chain have been upgraded, to profit from the higher luminosity delivered by the particle accelerator during Run3. The new DAQ system introduces a substantially different model for reading-out the detector data, which has not been used in systems of similar scale up to now. We designed a system capable of performing read-out, event-building and online reconstruction of the full event-rate produced by the LHC, without incurring the inefficiencies that a low-level hardware trigger would introduce. This design paradigm requires a DAQ system capable of ingesting an aggregated throughput of ~32 Tb/s, this poses significant technical challenges which have been solved by using both off-the-shelf solutions - like InfiniBand HDR - and customly developed FPGA-based electronics. \r\nIn this contribution, we will: provide an overview on the final system design, with a special focus on the event-building infrastructure;  present quantitative measurements taken during the commissioning of the system; discuss the resiliency of the system concerning latency and fault tolerance; and  provide feedback on the first year of operations of the system.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 574,
            id: 'c16994',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11394/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Pisani, Flavio'],
                emailHash: '2b14525e680c8cd085d5b92f54ddd6a0',
                familyName: 'Pisani',
                firstName: 'Flavio',
                name: 'Flavio Pisani',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12086,
            sessionSlotId: 2660,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'One year of LHCb triggerless DAQ: achievements and lessons learned.',
            uniqueId: 'c16994',
            url: '/event/459/contributions/11394/',
          },
          c16995: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11361/attachments/9365/13577/2023-05-08%20CHEP%20The%20new%20ALICE%20Data%20Acquisition%20system.pptx',
                  id: 13577,
                  title: '2023-05-08 CHEP The new ALICE Data Acquisition system.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11361,
            description:
              'ALICE (A Large Ion Collider Experiment) has undertaken a major upgrade during the LHC Long Shutdown 2. The increase in the detector data rates led to a hundredfold increase in the input raw data, up to 3.5 TB/s. To cope with it, a new common Online and Offline computing system, called O2, has been developed and put in production. \r\n\r\nThe O2/FLP system, successor of the ALICE DAQ system, implements the critical functions of detector readout, data quality control and operational services running in the CR1 data centre at the experimental site. Data from the 15 ALICE subdetectors are read out via 8000 optical links by 500 custom PCIe cards hosted in 200 nodes. It addresses novel challenges such as the continuous readout of the TPC detector while keeping compatibility with legacy detector front-end electronics. \r\n\r\nThis paper discusses the final architecture and design of the O2/FLP system and provides an overview of all its components, both hardware and software. It presents the selection process for the FLP nodes, the different commissioning steps and the main accomplishments so far. It will conclude with the challenges that lie ahead and how they will be addressed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 206,
            id: 'c16995',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11361/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Barroso, Vasco'],
                emailHash: '8ac848e59836d46924c54197586d9810',
                familyName: 'Barroso',
                firstName: 'Vasco',
                name: 'Vasco Barroso',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12086,
            sessionSlotId: 2660,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'The new ALICE Data Acquisition system (O2/FLP) for LHC Run 3',
            uniqueId: 'c16995',
            url: '/event/459/contributions/11361/',
          },
          c16996: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11403/attachments/9253/13422/aporeba_chep_2023.pdf',
                  id: 13422,
                  title: 'aporeba_chep_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11403,
            description:
              'Athena is the software framework used in the ATLAS experiment throughout the data processing path, from the software trigger system through offline event reconstruction to physics analysis. For Run 3 data taking (which started in 2022) the framework has been reimplemented into a multi-threaded framework. In addition to having to be remodelled to work in this new framework, the ATLAS High Level Trigger (HLT) system has also been updated to rely on common solutions between online and offline software to a greater extent than in Run 2 (data taking between 2015-2018). We present the now operational new HLT system, reporting on how the system was tested, commissioned and optimised. In addition, we show developments that have been made in tools that are used to monitor and configure the HLT, some of which are designed from scratch for Run 3.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 297,
            id: 'c16996',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11403/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Poreba, Aleksandra'],
                emailHash: '51c91be63eeece2655b0bb1b56279a6f',
                familyName: 'Poreba',
                firstName: 'Aleksandra',
                name: 'Aleksandra Poreba',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12086,
            sessionSlotId: 2660,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Operational experience with the new ATLAS HLT framework for LHC Run 3',
            uniqueId: 'c16996',
            url: '/event/459/contributions/11403/',
          },
          c16998: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11368/attachments/9243/13631/230508.chep23_negri.v6.pdf',
                  id: 13631,
                  title: '230508.chep23_negri.v6.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11368,
            description:
              'The ATLAS experiment at CERN is constructing upgraded system\r\nfor the "High Luminosity LHC", with collisions due to start in\r\n2029. In order to deliver an order of magnitude more data than\r\nprevious LHC runs, 14 TeV protons will collide with an instantaneous\r\nluminosity of up to 7.5 x 10e34 cm^-2s^-1, resulting in much higher pileup and\r\ndata rates than the current experiment was designed to handle. While\r\nthis is essential to realise the physics programme, it presents a huge\r\nchallenge for the detector, trigger, data acquisition and computing.\r\nThe detector upgrades themselves also present new requirements and\r\nopportunities for the trigger and data acquisition system.\r\n\r\nThe design of the TDAQ upgrade comprises: a hardware-based low-latency \r\nreal-time Trigger operating at 40 MHz, Data Acquisition which combines \r\ncustom readout with commodity hardware and networking to deal with \r\n4.6 TB/s input, and an Event Filter running at 1 MHz which combines \r\noffline-like algorithms on a large commodity compute service\r\nwith the potential to be augmented by commercial accelerators . \r\nCommodity servers and networks are used as far as possible, with \r\ncustom ATCA boards, high speed links and powerful FPGAs deployed \r\nin the low-latency parts of the system. Offline-style clustering and \r\njet-finding in FPGAs, and accelerated track reconstruction are\r\ndesigned to combat pileup in the Trigger and Event Filter\r\nrespectively.\r\n\r\nThis contribution will report recent progress on the design, technology and\r\nconstruction of the system. The physics motivation and expected\r\nperformance will be shown for key physics processes.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 305,
            id: 'c16998',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11368/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Negri, Andrea'],
                emailHash: 'd8ab166755845d2a843d1c0f1b6e2eb5',
                familyName: 'Negri',
                firstName: 'Andrea',
                name: 'Andrea Negri',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12086,
            sessionSlotId: 2660,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'ATLAS Trigger and Data Acquisition Upgrades for the High Luminosity LHC',
            uniqueId: 'c16998',
            url: '/event/459/contributions/11368/',
          },
          c17287: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11390/attachments/9264/13582/INDRA_ASTRA_RonglongFang.pdf',
                  id: 13582,
                  title: 'INDRA_ASTRA_RonglongFang.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11390,
            description:
              'The INDRA-ASTRA project is part of the ongoing R\u0026D on streaming readout and AI/ML at Jefferson Lab. In the  interdisciplinary project, nuclear physicists and data scientists work towards a prototype for an autonomous, responsive detector system as a first step towards a fully autonomous experiment. In our presentation, we will present our method for autonomous calibration of DIS experiments using baseline calibrations and autonomous change detection via the multiscale method. We will demonstrate how the versatile multiscale method we have developed can be used to increase reliability of data and find and fix issues in near real time. We will show test results from a prototype detector and the running, large-scale SBS experiment at Jefferson Lab.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 607,
            id: 'c17287',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11390/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [5, 'Fang, Ronglong'],
                emailHash: 'f8565b9a5152310d43927cc762fb2fa2',
                familyName: 'Fang',
                firstName: 'Ronglong',
                name: 'Mr Ronglong Fang',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12086,
            sessionSlotId: 2660,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'INDRA-ASTRA',
            uniqueId: 'c17287',
            url: '/event/459/contributions/11390/',
          },
          c17292: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11865/attachments/9303/13499/Kisel%20CHEP%202023%20v2.pdf',
                  id: 13499,
                  title: 'Kisel CHEP 2023 v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11865,
            description:
              'The fast algorithms for data reconstruction and analysis of the FLES (First Level Event Selection) package of the CBM (FAIR/GSI) experiment were successfully adapted to work on the High Level Trigger (HLT) of the STAR (BNL) experiment online. For this purpose, a so-called express data stream was created on the HLT, which enabled full processing and analysis of the experimental data in real time.\r\n\r\nWith this express data processing, including online calibration, reconstruction of tracks and short-lived particles, as well as search and analysis of hyperons and hypernuclei, approximately 30% of all the data collected in 2019-2021 within the Beam Energy Scan (BES-II) program at energies down to 3 GeV has been processed on the free resources of the HLT computer farm.\r\n\r\nA block diagram of the express data processing and analysis will be presented, particular features of the online calibration and application of the reconstruction algorithms, work under pile-up conditions at low collision energies in the fixed-target mode, and results of the real-time search for hyperons and hypernuclei up to $^5_\\Lambda$He with 11.6$\\cdot\\sigma$ at HLT will be presented and discussed. The high quality of the express data enabled preliminary analysis results in several physics measurements.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 208,
            id: 'c17292',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11865/contribution.pdf',
            presenters: [
              {
                affiliation: 'Goethe University Frankfurt am Main',
                displayOrderKey: [1, 'Kisel, Ivan'],
                emailHash: '5b24c25f8410067b741ae3a6e3833803',
                familyName: 'Kisel',
                firstName: 'Ivan',
                name: 'Prof. Ivan Kisel',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12086,
            sessionSlotId: 2660,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'From hyperons to hypernuclei online',
            uniqueId: 'c17292',
            url: '/event/459/contributions/11865/',
          },
        },
        entryType: 'Session',
        friendlyId: 6,
        id: 's12086',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2036/session-timetable.pdf',
        room: 'Marriott Ballroom V-VI',
        sessionCode: '',
        sessionId: 2036,
        sessionSlotId: 2660,
        slotTitle: 'DAQ Frameworks',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#000000',
        title: 'Track 2 - Online Computing',
        uniqueId: 's12086',
        url: '/event/459/sessions/2036/',
      },
      s12087: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d9dfc3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Chulalongkorn University',
            displayOrderKey: [0, 'Srpimanobhas, Norraphat'],
            emailHash: '545290bb86b1edf24b84a6682630eadc',
            familyName: 'Srpimanobhas',
            firstName: 'Norraphat',
            name: 'Dr Norraphat Srpimanobhas',
          },
          {
            affiliation: 'University of Pittsburgh',
            displayOrderKey: [0, 'Bandieramonte, Marilena'],
            emailHash: 'f4b45dbfcfef5c90c1b484b28e777c71',
            familyName: 'Bandieramonte',
            firstName: 'Marilena',
            name: 'Marilena Bandieramonte',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17035: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11453/attachments/9329/13530/Machine-learning-for-Ambiguity-Resolution-in-Acts.pdf',
                  id: 13530,
                  title: 'Machine-learning-for-Ambiguity-Resolution-in-Acts.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11453,
            description:
              'The reconstruction of particle trajectories is a key challenge of particle physics experiments, as it directly impacts particle identification and physics performances while also representing one of the main CPU consumers of many high energy physics experiments. As the luminosity of particle collider increases, this reconstruction will become more challenging and resource intensive. New algorithms are thus needed to address these challenges efficiently. One potential step of track reconstruction is the ambiguity resolution. In this step, performed at the end of the tracking chain, we select which tracks candidates should to be kept and which ones need to be discarded. In the ATLAS experiment, for example, this is achieved by identifying fakes tracks, removing duplicates and determining via a Neural Network which hits should be shared by multiple tracks. The speed of this algorithm is directly driven by the number of track candidates, which can be reduced at the cost of some physics performance. Since this problem is fundamentally an issue of comparison and classification, we propose to use a machine learning based approach to the Ambiguity Resolution itself. Using a nearest neighbour search, we can efficiently determine which candidates belong to the same truth particle. Afterward, we can apply a Neural Network (NN) to compare those tracks and determine which ones are the duplicate and which one should be kept. Finally, another NN is applied to all the remaining candidates to identify which ones are fakes and remove those. This approach is implemented within A Common Tracking Software (ACTS) framework and tested on the Open Data Detector (ODD) a realistic virtual detector, similar to a future ATLAS one, to fully evaluate the potential of this approach.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 174,
            id: 'c17035',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11453/contribution.pdf',
            presenters: [
              {
                affiliation: 'IJCLab',
                displayOrderKey: [1, 'Allaire, Corentin'],
                emailHash: 'b4931226fb604bd6db2ad23e72812b36',
                familyName: 'Allaire',
                firstName: 'Corentin',
                name: 'Corentin Allaire',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12087,
            sessionSlotId: 2661,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Machine learning for ambiguity resolution in ACTS',
            uniqueId: 'c17035',
            url: '/event/459/contributions/11453/',
          },
          c17036: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11432/attachments/9330/13614/mkFit-CHEP-2023.pdf',
                  id: 13614,
                  title: 'mkFit-CHEP-2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11432,
            description:
              'MkFit is an implementation of the Kalman filter-based track reconstruction algorithm that exploits both thread- and data-level parallelism. In the past few years the project transitioned from the R\u0026D phase to deployment in the Run-3 offline workflow of the CMS experiment. The CMS tracking performs a series of iterations, targeting reconstruction of tracks of increasing difficulty after removing hits associated to tracks found in previous iterations. MkFit has been adopted for several of the tracking iterations, which contribute to the majority of reconstructed tracks. When tested in the standard conditions for production jobs, speedups in track pattern recognition are on average of the order of 3.5x for the iterations where it is used (3-7x depending on the iteration). Multiple factors contribute to the observed speedups, including vectorization and a lightweight geometry description, as well as improved memory management and single precision. Efficient vectorization is achieved with both the icc and the gcc (default in CMSSW) compilers and relies on a dedicated library for small matrix operations, Matriplex, which has recently been released in a public repository. While the mkFit geometry description already featured levels of abstraction from the actual Phase-1 CMS tracker, several components of the implementations were still tied to that specific geometry. We have further generalized the geometry description and the configuration of the run-time parameters, in order to enable support for the Phase-2 upgraded tracker geometry for the HL-LHC and potentially other detector configurations. The implementation strategy and preliminary results with the HL-LHC geometry will be presented. Speedups in track building from mkFit imply that track fitting becomes a comparably time consuming step of the tracking chain. Prospects for an mkFit implementation of the track fit will also be discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 222,
            id: 'c17036',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11432/contribution.pdf',
            presenters: [
              {
                affiliation: 'UCSD',
                displayOrderKey: [0, 'Tadel, Matevz'],
                emailHash: 'b6910c94580994105a7b03af3c3b2c33',
                familyName: 'Tadel',
                firstName: 'Matevz',
                name: 'Matevz Tadel',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12087,
            sessionSlotId: 2661,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Generalizing mkFit and its Application to HL-LHC',
            uniqueId: 'c17036',
            url: '/event/459/contributions/11432/',
          },
          c17037: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11420/attachments/9416/13657/traccc%20-%20A%20(Close%20To)%20Single-Source%20Tracking%20Demonstrator%20on%20CPUs_GPUs%202023.05.08..pdf',
                  id: 13657,
                  title:
                    'traccc - A (Close To) Single-Source Tracking Demonstrator on CPUs_GPUs 2023.05.08..pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11420,
            description:
              'Despite recent advances in optimising the track reconstruction problem for high particle multiplicities in high energy physics experiments, it remains one of the most demanding reconstruction steps in regards to complexity and computing ressources. Several attemps have been made in the past to deploy suitable algorithms for track reconstruction on hardware accelerators, often by tailoring the algorithmic strategy to the hardware design. This led in certain cases to algorithmic compromises, and often came along with simplified descriptions of detector geometry, input data and magnetic field.\r\nThe traccc project is an R\u0026D initiative of the ACTS common track reconstruction; it aims to provide a complete track reconstruction chain for both CPU and GPU architectures. Emphasis has been put on sharing as much common source code as possible while trying to avoid algorithmic and physics performance compromises. Within traccc, dedicated components have been developed that are usable on standard CPU and GPU architectures: an astraction layer for linear algebra operations that allows to customize the mathematical backend (algebra-plugin), a host and device memory management system (vecmem), a generic vector field library (covfie) for the magneic field description, and a geometry and propagation library (detray). They serve as building blocks of a fully developed track reconstruction demonstrator based on clustering (connected component labelling), space point formation, track seeding and combinatorial track finding.\r\nWe present the concepts and implementation of the traccc demonstrator and classify the physics and computational performance on selected hardware using the Open Data Detector in an scenario minicking the HL-LHC run condition. In addition, we give insight in our attempts to use different native language and portability solutions for GPUs, and summarize our main findings during the development of the entire traccc project.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 382,
            id: 'c17037',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11420/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [2, 'Krasznahorkay, Attila'],
                emailHash: '8d4c0721de129e328dffeb903264d729',
                familyName: 'Krasznahorkay',
                firstName: 'Attila',
                name: 'Attila Krasznahorkay',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12087,
            sessionSlotId: 2661,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'traccc - a close to single-source track reconstruction demonstrator for CPU and GPU',
            uniqueId: 'c17037',
            url: '/event/459/contributions/11420/',
          },
          c17038: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11412/attachments/9403/13641/PVOffline_GPU_CHEP2023-5.pdf',
                  id: 13641,
                  title: 'PVOffline_GPU_CHEP2023-5.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11412,
            description:
              'The high luminosity expected from the LHC during the Run 3 and, especially, the HL-LHC of data taking introduces significant challenges in the CMS event reconstruction chain. The additional computational resources needed to treat this increased quantity of data surpass the expected increase in processing power for the next years. In order to fit the projected resource envelope, CMS is re-inventing its online and offline reconstruction algorithms, with their execution on CPU+GPU platforms in mind. Track clustering and primary vertex reconstruction accounts today about 10% of the reconstruction chain at 200 pileup and involves similar computations over hundreds to thousands of reconstructed tracks. This makes it a natural candidate for the development of a GPU-based algorithm that parallelizes it dividing the work in blocks. In this contribution we discuss the physics performance as well as the runtime performance of a new vertex clustering algorithm CMS developed for heterogeneous plarforms. We\u0027ll show that the physics results achieved are better than the current CMS vertexing algorithm in production, that the algorithm is up to 8 times faster on CPU and runs as well on GPUs. We will also discuss the plans for using this algorithm in production in Run 3 and for extending it to make use of timing information provided by the CMS Phase-2 MIP Timing Detector (MTD).',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 387,
            id: 'c17038',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11412/contribution.pdf',
            presenters: [
              {
                affiliation: 'Boston University (US)',
                displayOrderKey: [1, 'Erice Cid, Carlos Francisco'],
                emailHash: 'fb2fa414233919b9672d9b975dca6cdc',
                familyName: 'Erice Cid',
                firstName: 'Carlos Francisco',
                name: 'Carlos Francisco Erice Cid',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12087,
            sessionSlotId: 2661,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'GPU-based algorithms for primary vertex reconstruction at CMS',
            uniqueId: 'c17038',
            url: '/event/459/contributions/11412/',
          },
          c17039: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11447/attachments/9425/13754/CHEP%202023%20ExaTrkX%20overview.pptx',
                  id: 13754,
                  title: 'CHEP 2023 ExaTrkX overview.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11447,
            description:
              'Building on the pioneering work of the HEP.TrkX project [1], Exa.TrkX developed geometric learning tracking pipelines that include metric learning and graph networks. These end-to-end pipelines capture the relationships between spacepoint measurements belonging to a particle track. We tested the pipelines on simulated data from HL-LHC tracking detectors [2,5], Liquid Argon TPCs for neutrino experiments [3,8], and the straw tube tracker of the PANDA experiment[4]. The HL-LHC pipeline provides state-of-the-art tracking performance (Fig. 2), scales linearly with spacepoint density (Fig. 1), and has been optimized to run end-to-end on GP-GPUs, achieving a 20x speed-up with respect to the baseline implementation [6,9]. \r\nThe Exa.TrkX geometric learning approach also has shown promise in less traditional tracking applications, like large-radius tracking for new physics searches at the LHC [7].\r\nExa.TrkX also contributed to developing and optimizing common data formats for ML training and inference targeting both neutrino detectors and LHC trackers.\r\n\r\nWhen applied to LArTPC neutrino experiments, the Exa.TrkX message-passing graph neural network classifies nodes, defined as the charge measurements or hits, according to the underlying particle type that produced them (Fig 3). Thanks to special 3D edges, our network can connect nodes within and across wire planes and achieve 94% accuracy with 96% consistency across wire planes [8].\r\nFrom the very beginning, the Exa.TrkX project has functioned as a collaboration open beyond its three original institutions  (CalTech, FNAL, and LBNL). We released the code associated with every publication and produced tutorials and quickstart examples to test our pipeline.\r\nEight US universities and six international institutions have contributed significantly to our research program and publications. The collaboration currently includes members of the ATLAS, CMS, DUNE, and PANDA experiments. Members of the FNAL muon g-2 experiment and CERN MUonE projects have tested the Exa.TrkX pipeline on their datasets.\r\n\r\nExa.TrkX profits from multi-year partnerships with related research projects, namely the ACTS common tracking software, the ECP ExaLearn project, the NSF A3D3 institute, and the Fast ML Lab. More recently, as our pipeline matured and became applicable to more complex datasets, we started a partnership with HPE Lab, which uses our pipeline as a benchmark for its hyperparameter optimization and common metadata framework. NVIDIA (through the NERSC NESAP program) is evaluating the Exa.TrkX pipeline as an advanced use case for their R\u0026D in Graph neural networks optimization.\r\nAt this stage of the project, a necessary focus of the Exa.TrkX team is on consolidation and dissemination of the results obtained so far. We are re-engineering the LHC pipeline to improve its modularity and usability across experiment frameworks. We aim to integrate our pipelines with online and offline reconstruction chains of neutrino and collider detectors and release a repository of production-quality HEP pattern recognition models that can be readily composed into an experiment-specific pipeline.\r\nWe are investigating heterogeneous graph networks to improve our pipelines\u0027 physics performance and make our models more easily generalizable [11]. Heterogeneity allows mixing and matching information from multiple detector geometries and types (strips vs. pixels, calorimeters vs. trackers vs. timing detectors, etc.).\r\nWe have demonstrated that it is possible to recover \u201cdifficult\u201d tracks (e.g., tracks with a missing spacepoint) by using hierarchical graph networks [10]. Next, we need to scale these models to more challenging datasets, including full HL-LHC simulations.\r\nWe are also investigating how to parallelize our pipeline across multiple GPUs. Data parallelism for graph networks is an active research area in geometric learning. The unique setting of our problem, with large graphs that change structure with every event, makes parallelizing the inference step particularly challenging.\r\nA future research project\u0027s ultimate goal would be to combine these four R\u0026D threads into a generic pipeline for HEP pattern recognition that operates on heterogeneous data at different scales, from raw data to particles.\r\n\r\n[1 ]Farrell, S., Calafiura, P., et al. . Novel deep learning methods for track reconstruction. (2018). arXiv. https://doi.org/10.48550/arXiv.1810.06111\r\n\r\n[2] Ju, X., Murnane, D., et al. Performance of a geometric deep learning pipeline for HL-LHC particle tracking. Eur. Phys. J. C 81, 876 (2021). https://doi.org/10.1140/epjc/s10052-021-09675-8\r\n\r\n[3] Hewes, J., Aurisano, A., et al. Graph Neural Network for Object Reconstruction in Liquid Argon Time Projection Chambers. EPJ Web of Conferences 251, 03054 (2021).\r\nhttps://doi.org/10.1051/epjconf/202125103054\r\n\r\n[4] Akram, A., \u0026 Ju, X. Track Reconstruction using Geometric Deep Learning in the Straw Tube Tracker (STT) at the PANDA Experiment. (2022) arXiv. https://doi.org/10.48550/arXiv.2208.12178\r\n\r\n[5] Caillou, S., Calafiura, P. et al. ATLAS ITk Track Reconstruction with a GNN-based pipeline. (2022). ATL-ITK-PROC-2022-006. https://cds.cern.ch/record/2815578\r\n\r\n[6] Lazar, A., Ju, X., et al. Accelerating the Inference of the Exa.TrkX Pipeline. (2022). arXiv. https://doi.org/10.48550/arXiv.2202.06929\r\n\r\n[7] Wang, C., Ju, X., et al. Reconstruction of Large Radius Tracks with the Exa.TrkX pipeline. (2022).  arXiv. https://doi.org/10.48550/arXiv.2203.08800\r\n\r\n[8] Gumpula, K., et al., Graph Neural Network for Three Dimensional Object Reconstruction in Liquid Argon Time Projection Chambers. (2022). Presented at the Connecting the Dots 2022 workshop.\r\nhttps://indico.cern.ch/event/1103637/contributions/4821839\r\n\r\n\r\n[9] Acharya, N., Liu, E., Lucas, A., Lazar, A. Optimizing the Exa.TrkX Inference Pipeline for Manycore CPUs. (2022). Presented at the Connecting the Dots 2022 workshop. https://indico.cern.ch/event/1103637/contributions/4821918\r\n\r\n[10] Liu, R., Murnane, D., et al. Hierarchical Graph Neural Networks for Particle Reconstruction. (2022). Presented at the ACAT 2022 conference. https://indico.cern.ch/event/1106990/contributions/4996236/\r\n\r\n[11] Murnane, D., Caillou, S.,. Heterogeneous GNN for tracking. (2022). Presented at the Princeton Mini-workshop on Graph Neural Networks for Tracking. https://indico.cern.ch/event/1128328/contributions/4900744',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 408,
            id: 'c17039',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11447/contribution.pdf',
            presenters: [
              {
                affiliation: 'LBNL',
                displayOrderKey: [1, 'Calafiura, Paolo'],
                emailHash: 'abe1eab839bcd16b6fe1dc8165c71774',
                familyName: 'Calafiura',
                firstName: 'Paolo',
                name: 'Paolo Calafiura',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12087,
            sessionSlotId: 2661,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'The Exa.TrkX Project',
            uniqueId: 'c17039',
            url: '/event/459/contributions/11447/',
          },
          c17040: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11440/attachments/9429/13672/track%20overlay%20chep.pdf',
                  id: 13672,
                  title: 'track overlay chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11440,
            description:
              'The production of simulated datasets for use by physics analyses consumes a large fraction of ATLAS computing resources, a problem that will only get worse as increases in the instantaneous luminosity provided by the LHC lead to more collisions per bunch crossing (pile-up). One of the more resource-intensive steps in the Monte Carlo production is reconstructing the tracks in the ATLAS Inner Detector (ID), which takes up about 60% of the total detector reconstruction time [1]. This talk discusses a novel technique called track overlay, which substantially speeds up the ID reconstruction. In track overlay the pile-up ID tracks are reconstructed ahead of time and overlaid onto the ID tracks from the simulated hard-scatter event. We present our implementation of this track overlay approach as part of the ATLAS Fast Chain simulation, as well as a method for deciding in which cases it is possible to use track overlay in the reconstruction of simulated data without performance degradation.\r\n\r\n[1] ATL-PHYS-PUB-2021-012 (60% refers to Run3, mu=50, including large-radius tracking, p11)',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 430,
            id: 'c17040',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11440/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Massachusetts, Amherst',
                displayOrderKey: [1, 'Leight, William'],
                emailHash: 'daa8a225209f5e253d0b634ceeff2899',
                familyName: 'Leight',
                firstName: 'William',
                name: 'Dr William Leight',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12087,
            sessionSlotId: 2661,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Faster simulated track reconstruction in the ATLAS Fast Chain',
            uniqueId: 'c17040',
            url: '/event/459/contributions/11440/',
          },
        },
        entryType: 'Session',
        friendlyId: 7,
        id: 's12087',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2037/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VI',
        sessionCode: '',
        sessionId: 2037,
        sessionSlotId: 2661,
        slotTitle: 'Reconstruction',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#272f09',
        title: 'Track 3 - Offline Computing',
        uniqueId: 's12087',
        url: '/event/459/sessions/2037/',
      },
      s12088: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#efebc2',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Unive',
            displayOrderKey: [0, 'Barreiro Megino, Fernando'],
            emailHash: 'f04de6eee7b8dcc20dedefcb9fe659da',
            familyName: 'Barreiro Megino',
            firstName: 'Fernando',
            name: 'Fernando Barreiro Megino',
          },
          {
            affiliation: 'SKAO',
            displayOrderKey: [0, 'Joshi, Rohini'],
            emailHash: '7fd55f634d18a43003ce815c039c70ab',
            familyName: 'Joshi',
            firstName: 'Rohini',
            name: 'Rohini Joshi',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16836: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11472/attachments/9422/13778/dml_idds_20230405%20-%20annotated%20draft%202.pptx',
                  id: 13778,
                  title: 'dml_idds_20230405 - annotated draft 2.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11472,
            description:
              'Machine learning has become one of the important tools for High Energy Physics analysis. As the size of the dataset increases at the Large Hadron Collider (LHC), and at the same time the search spaces become bigger and bigger in order to exploit the physics potentials, more and more computing resources are required for processing these machine learning tasks. In addition, complex advanced machine learning workflows are developed in which one task may depend on the results of previous tasks. How to make use of vast distributed CPUs/GPUs in WLCG for these big complex machine learning tasks has become a popular area. In this presentation, we will present our efforts on distributed machine learning in PanDA and iDDS (intelligent Data Delivery Service). We will at first address the difficulties to run machine learning tasks on distributed WLCG resources. Then we will present our implementation with DAG (Directed Acyclic Graph) and sliced parameters in iDDS to distribute machine learning tasks to distributed computing resources to execute them in parallel through PanDA. Next we will demonstrate some use cases we have implemented, such as Hyperparameter Optimization, Monte Carlo Toy confidence limits calculation and Active Learning. Finally we will describe some directions to perform in the future.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 197,
            id: 'c16836',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11472/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [0, 'Weber, Christian'],
                emailHash: '3ba512a6ea1d473cca46be186e399a8f',
                familyName: 'Weber',
                firstName: 'Christian',
                name: 'Christian Weber',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12088,
            sessionSlotId: 2662,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Distributed Machine Learning with PanDA and iDDS in LHC ATLAS',
            uniqueId: 'c16836',
            url: '/event/459/contributions/11472/',
          },
          c16837: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11480/attachments/9316/13513/CHEP_2023_JaySandesara_vFinal.pdf',
                  id: 13513,
                  title: 'CHEP_2023_JaySandesara_vFinal.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11480,
            description:
              'We present a new implementation of simulation-based inference using data collected by the ATLAS experiment at the LHC. The method relies on large ensembles of deep neural networks to approximate the exact likelihood. Additional neural networks are introduced to model systematic uncertainties in the measurement. Training of the large number of deep neural networks is automated using a parallelized workflow with distributed computing infrastructure integrated with cloud-based services. We will show an example workflow using the ATLAS PanDA framework integrated with GPU infrastructure from Google Cloud Platform. Numerical analysis of the neural networks is optimized with JAX and JIT. The novel machine-learning method and cloud-based parallel workflow can be used to improve the sensitivity of several other analyses of LHC data.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 423,
            id: 'c16837',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11480/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Massachusetts, Amherst',
                displayOrderKey: [1, 'Sandesara, Jay'],
                emailHash: 'dc16ee28621299f8c1805292f8197d9b',
                familyName: 'Sandesara',
                firstName: 'Jay',
                name: 'Jay Sandesara',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12088,
            sessionSlotId: 2662,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'ATLAS data analysis using a parallelized workflow on distributed cloud-based services with GPUs',
            uniqueId: 'c16837',
            url: '/event/459/contributions/11480/',
          },
          c16838: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11490/attachments/9254/13423/DCSim_CHEP_Talk.pdf',
                  id: 13423,
                  title: 'DCSim_CHEP_Talk.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11490,
            description:
              'Predicting the performance of various infrastructure design options in complex federated infrastructures with computing sites distributed over a wide area that support a plethora of users and workflows, such as the Worldwide LHC Computing Grid (WLCG), is not trivial. Due to the complexity and size of these infrastructures, it is not feasible to deploy experimental test-beds at large scales merely for the purpose of comparing and evaluating alternate designs.\r\n\r\nAn alternative is to simulate the behaviours of these systems based on realistic simulation models. This approach has been used successfully in the past to identify efficient and practical infrastructure designs for High Energy Physics (HEP). A prominent example is the Monarc simulation framework, which was used to study the initial structure of the WLCG. However, new simulation capabilities are needed to simulate large-scale heterogeneous infrastructures with complex networks as well as application behaviours that include various data access and caching patterns.\r\n\r\nIn this context, a modern tool to simulate high energy physics workloads that execute on distributed computing infrastructures based on the SimGrid and WRENCH simulation frameworks is outlined. Studies of its accuracy and scalability are presented using HEP as a case-study.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 306,
            id: 'c16838',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11490/contribution.pdf',
            presenters: [
              {
                affiliation: 'Karlsruhe Institute of Technology',
                displayOrderKey: [1, 'Horzela, Maximilian'],
                emailHash: 'b645545a49228648a731dc8cadede06e',
                familyName: 'Horzela',
                firstName: 'Maximilian',
                name: 'Mr Maximilian Horzela',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12088,
            sessionSlotId: 2662,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Modelling Distributed Heterogeneous Computing Infrastructures for HEP Applications',
            uniqueId: 'c16838',
            url: '/event/459/contributions/11490/',
          },
          c16839: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11464/attachments/9376/13649/interTwin-CHEP-2023-V3.pptx',
                  id: 13649,
                  title: 'interTwin-CHEP-2023-V3.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11464,
            description:
              'InterTwin is an EU-funded project that started on the 1st of September 2022.  The project will work with domain experts from different scientific domains in building a technology to support digital twins within scientific research.  Digital twins are models for predicting the behaviour and evolution of real-world systems and applications.\r\n\r\nInterTwin will focus on employing machine-learning techniques to create and train models that are able to quickly and accurately reflect their physical counterparts in a broad range of scientific domains. The project will develop, deploy and \u201croad harden\u201d a blueprint for supporting digital twins on federated resources.  For that purpose, it will support a diverse set of science use-cases, in the domains of radio telescopes (Meerkat), particle physics (CERN/LHC and Lattice-QCD), gravitational waves (Virgo), as well as climate research and environment monitoring (e.g. prediction of flooding and other extreme weather due to climate change).  The ultimate goal is to provide a flexible infrastructure that can accommodate the needs of many additional scientific fields.\r\n\r\nIn the talk, we will present an overview of the interTwin project along with the corresponding Digital Twin Engine (DTE) architecture for federating the different, heterogeneous resources available to the scientific use-cases (storage, HPC, HTC, quantum) when training and exploitation of digital twins within the different scientific domains.  The challenges faced when designing the architecture will be described, along with the solutions being developed to address them.  interTwin is required to be interoperable with other infrastructures, including EuroHPC-based Destination Earth Initiative (DestinE) and an infrastructure for accessing Copernicus satellite data, C-SCALE.  We will also present our strategy for making DTE available within the European Open Science Cloud (EOSC). The details of all such interoperability will also be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 544,
            id: 'c16839',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11464/contribution.pdf',
            presenters: [
              {
                affiliation: 'DESY',
                displayOrderKey: [3, 'Fuhrmann, Patrick'],
                emailHash: 'a198240a6b21354217ead633867b3e31',
                familyName: 'Fuhrmann',
                firstName: 'Patrick',
                name: 'Patrick Fuhrmann',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12088,
            sessionSlotId: 2662,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Digital Twin Engine Infrastructure in the interTwin Project',
            uniqueId: 'c16839',
            url: '/event/459/contributions/11464/',
          },
          c16840: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11485/attachments/9280/13463/CHEP%202023%20-%20SkyDriver.pdf',
                  id: 13463,
                  title: 'CHEP 2023 - SkyDriver.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11485/attachments/9280/13462/CHEP%202023%20-%20SkyDriver.pptx',
                  id: 13462,
                  title: 'CHEP 2023 - SkyDriver.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11485,
            description:
              'The IceCube Neutrino Observatory is a cubic kilometer neutrino telescope located at the geographic South Pole. To accurately and promptly reconstruct the arrival direction of candidate neutrino events for Multi-Messenger Astrophysics use cases, IceCube employs Skymap Scanner workflows managed by the SkyDriver service. The Skymap Scanner performs maximum-likelihood tests on individual pixels generated from the Hierarchical Equal Area isoLatitude Pixelation (HEALPix) algorithm. Each test is computationally independent, which allows for massive parallelization. This workload is distributed using the Event Workflow Management System (EWMS)\u2014a message-based workflow management system designed to scale to trillions of pixels per day. SkyDriver orchestrates multiple distinct Skymap Scanner workflows behind a REST interface, providing an easy-to-use reconstruction service for real-time candidate, cataloged, and simulated events. Here, we outline the SkyDriver service technique and the initial development of EWMS.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 349,
            id: 'c16840',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11485/contribution.pdf',
            presenters: [
              {
                affiliation: 'IceCube, University of Wisconsin-Madison',
                displayOrderKey: [0, 'Evans, Ric'],
                emailHash: 'a6ce280c80fc8eb9edb255ab88e27d9e',
                familyName: 'Evans',
                firstName: 'Ric',
                name: 'Ric Evans',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12088,
            sessionSlotId: 2662,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'IceCube SkyDriver - A SaaS Solution for Event Reconstruction using the Skymap Scanner',
            uniqueId: 'c16840',
            url: '/event/459/contributions/11485/',
          },
          c16841: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11494/attachments/9266/13438/agc_benchmarking_tests.pdf',
                  id: 13438,
                  title: 'agc_benchmarking_tests.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11494,
            description:
              'A fast turn-around time and ease of use are important factors for systems supporting the analysis of large HEP data samples. We study and compare multiple technical approaches.\r\nThis presentation will be about setting up and benchmarking the Analysis Grand Challenge (AGC) [1] using CMS Open Data. The AGC is an effort to provide a realistic physics analysis with the intent of showcasing the functionality, scalability and feature-completeness of the Scikit-HEP Python ecosystem.\r\nI will present the results of setting up the necessary software environment for the AGC and benchmarking the analysis\u0027 runtime on various computing clusters: the institute SLURM cluster at my home institute, LMU Munich, a SLURM cluster at LRZ (WLCG Tier-2 site) and the analysis facility Vispa [2], operated by RWTH Aachen.\r\nEach site provides slightly different software environments and modes of operation which poses interesting challenges on the flexibility of a setup like that intended for the AGC.\r\nComparing these benchmarks to each other also provides insights about different storage and caching systems. At LRZ and LMU we have regular Grid storage (HDD) as well as and SSD-based XCache server and on Vispa a sophisticated per-node caching system is used.\r\n\r\n[1] https://github.com/iris-hep/analysis-grand-challenge\r\n[2] https://vispa.physik.rwth-aachen.de/',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 511,
            id: 'c16841',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11494/contribution.pdf',
            presenters: [
              {
                affiliation: 'LMU',
                displayOrderKey: [0, 'Koch, David'],
                emailHash: '54221da51c66e5e8a5e7d69a63c84ff2',
                familyName: 'Koch',
                firstName: 'David',
                name: 'David Koch',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12088,
            sessionSlotId: 2662,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Analysis Grand Challenge benchmarking tests on selected sites',
            uniqueId: 'c16841',
            url: '/event/459/contributions/11494/',
          },
        },
        entryType: 'Session',
        friendlyId: 8,
        id: 's12088',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2038/session-timetable.pdf',
        room: 'Marriott Ballroom II-III',
        sessionCode: '',
        sessionId: 2038,
        sessionSlotId: 2662,
        slotTitle: 'Analysis Workflows, Modeling and Optimisation',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 4 - Distributed Computing',
        uniqueId: 's12088',
        url: '/event/459/sessions/2038/',
      },
      s12089: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfe555',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Sailer, Andre'],
            emailHash: '84f3f721370dba12b47a611b1a36081e',
            familyName: 'Sailer',
            firstName: 'Andre',
            name: 'Andre Sailer',
          },
          {
            affiliation: 'FNAL',
            displayOrderKey: [0, 'Sexton, Elizabeth'],
            emailHash: 'e8fb00f4f09841337efaeff677169cd2',
            familyName: 'Sexton',
            firstName: 'Elizabeth',
            name: 'Elizabeth Sexton',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16878: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11521/attachments/9507/13783/Julia%20in%20HEP.pdf',
                  id: 13783,
                  title: 'Julia in HEP.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11521/attachments/9507/13782/Julia%20in%20HEP%20(with%20animation%20steps).pdf',
                  id: 13782,
                  title: 'Julia in HEP (with animation steps).pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11521,
            description:
              'The Julia programming language was created 10 years ago and is now a mature and stable language with a large ecosystem including more than 8,000 third-party packages. It was designed for scientific programming to be a high-level and dynamic language as Python is, while achieving runtime performances comparable to C/C++ or even faster. With this, we ask ourselves if the Julia language and its ecosystem is ready now for its adoption by the High Energy Physics community. We will report on a number of investigations and studies of the Julia language that have been done for various representative HEP applications, ranging from computing intensive initial data processing of experimental data and simulation, to final interactive data analysis and plotting. Aspects of collaborative code development of large software within a HEP experiment has also been investigated: scalability with large development teams, continuous integration and code test, code reuse, language interoperability to enable a adiabatic migration of packages and tools, software installation and distribution, training of the community, benefit from development from industry and academia from other fields.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 171,
            id: 'c16878',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11521/contribution.pdf',
            presenters: [
              {
                affiliation: 'Erlangen Centre for Astroparticle Physics',
                displayOrderKey: [10, 'G\u00e1l, Tam\u00e1s'],
                emailHash: '33716cdc69b19d112556c1a0bcb7b57c',
                familyName: 'G\u00e1l',
                firstName: 'Tam\u00e1s',
                name: 'Tam\u00e1s G\u00e1l',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12089,
            sessionSlotId: 2663,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Is Julia ready to be adopted by HEP?',
            uniqueId: 'c16878',
            url: '/event/459/contributions/11521/',
          },
          c16879: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11540/attachments/9410/13651/CHEP2023%20Polyglot%20Jet%20Finding.pdf',
                  id: 13651,
                  title: 'CHEP2023 Polyglot Jet Finding.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11540,
            description:
              'The evaluation of new computing languages for a large community, like HEP, involves comparison of many aspects of the languages\u0027 behaviour, ecosystem and interactions with other languages. In this paper we compare a number of languages using a common, yet non-trivial, HEP algorithm: the tiled $N^2$ clustering algorithm used for jet finding. We compare specifically the algorithm implemented in Python, using numpy, Julia and Rust, with respect to the reference implementation in C++, from Fastjet. As well as the speed of the implementation we describe the ergonomics of the language for the coder, as well as the efforts required to achieve the best performance, which can directly impact on code readability and sustainability.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 286,
            id: 'c16879',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11540/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Stewart, Graeme'],
                emailHash: '9f60b1ecea45598b615f77b2b41150f1',
                familyName: 'Stewart',
                firstName: 'Graeme',
                name: 'Graeme Stewart',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12089,
            sessionSlotId: 2663,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Polyglot Jet Finding',
            uniqueId: 'c16879',
            url: '/event/459/contributions/11540/',
          },
          c16880: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11553/attachments/9217/13378/c_080523.pdf',
                  id: 13378,
                  title: 'c_080523.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11553,
            description:
              'With an increased dataset obtained during the Run-3 of the LHC at CERN and the even larger expected increase of the dataset by more than one order of magnitude for the HL-LHC, the ATLAS experiment is reaching the limits of the current data processing model in terms of traditional CPU resources based on x86_64 architectures and an extensive program for software upgrades towards the HL-LHC has been set up. The ARM architecture is becoming a competitive and energy efficient alternative. Some surveys indicate its increased presence in HPCs and commercial clouds, and some WLCG sites have expressed their interest. Chip makers are also developing their next generation solutions on ARM architectures, sometimes combining ARM and GPU processors in the same chip. Therefore it is important that the Athena software embraces the change and is able to successfully exploit this architecture.\r\n\r\nWe report on the successful port of the ATLAS experiment offline and online software framework Athena to ARM and the successful physics validation of simulation workflows. For this we have set up an ATLAS Grid site using ARM compatible middleware and containers on Amazon Web Services (AWS) ARM resources. The ARM version of Athena is fully integrated in the regular software build system and distributed like default software releases. In addition, the workflows have been integrated into the HepScore benchmark suite which is the planned WLCG wide replacement of the HepSpec06 benchmark used for Grid site pledges. In the overall porting process we have used resources on AWS, Google Cloud Platform (GCP) and CERN. A performance comparison of different architectures and resources will be discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 335,
            id: 'c16880',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11553/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [1, 'Elmsheuser, Johannes'],
                emailHash: 'f45317b785fdcd62e8c3fcf140113d12',
                familyName: 'Elmsheuser',
                firstName: 'Johannes',
                name: 'Johannes Elmsheuser',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12089,
            sessionSlotId: 2663,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'The ATLAS experiment software on ARM',
            uniqueId: 'c16880',
            url: '/event/459/contributions/11553/',
          },
          c16881: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11526/attachments/9347/15236/Lang-Licence.pdf',
                  id: 15236,
                  title: 'Lang-Licence.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11526/attachments/9347/13735/Multilanguage%20Frameworks.pdf',
                  id: 13735,
                  title: 'Multilanguage Frameworks.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11526/attachments/9347/13550/go',
                  id: 13550,
                  title: 'slides',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11526,
            description:
              'High Energy Physics software has been a victim of the necessity to choose one implementation language as no really usable multi-language environment existed. Even a co-existence of two languages in the same framework (typically C++ and Python) imposes a heavy burden on the system. The role of different languages was generally limited to well encapsulated domains (like Web applications, databases, graphics), with very limited connection to the central framework.\r\n\r\nThe new development in the domain of the compilers and run-time environments has enabled ways for creating really multilanguage frameworks, with seamless, user-friendly and high-performance  inter-operation of many languages, which traditionally live in disconnected domains (like C-based languages vs JVM languages or Web languages). \r\n\r\nVarious possibilities and strategies for creation of the true multi-language frameworks will be discussed, emphasizing their advantages and possible road blocks.\r\n\r\nA prototype of massively multilanguage application will be presented, using very wide spectrum of languages working together (C++, Python, JVM languages, JavaScript,...). Each language will be used in the domain where it offers a strong comparative advantage (speed, user-friendliness, availability of third-party libraries and tools, graphical and web capabilities).\r\n\r\nThe performance gain from the modern multi-language environments will be also demonstrated, as well as gains in the overall memory footprint.\r\n\r\nPossibilities of converting existing HEP frameworks into multilanguage environments will be discussed in concrete examples and demonstrations.\r\n\r\nA real life example of widely multilanguage environment will be demonstrated  on the case of the multi-language access to the data storage of the LSST telescope Fink project.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 451,
            id: 'c16881',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11526/contribution.pdf',
            presenters: [
              {
                affiliation: 'IJCLab',
                displayOrderKey: [1, 'Hrivnac, Julius'],
                emailHash: 'a5efba4a70913e2df05be5f3fdfb3ee5',
                familyName: 'Hrivnac',
                firstName: 'Julius',
                name: 'Julius Hrivnac',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12089,
            sessionSlotId: 2663,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Multilanguage Frameworks',
            uniqueId: 'c16881',
            url: '/event/459/contributions/11526/',
          },
          c16882: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11523/attachments/9522/13810/Diefenthaler-CHEP2023-UCD.pdf',
                  id: 13810,
                  title: 'Diefenthaler-CHEP2023-UCD.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11523,
            description:
              'Software and computing are an integral part of our research. According to the survey for the \u201cFuture Trends in Nuclear Physics Computing\u201d workshop in September 2020, students and postdocs spent 80% of their time on the software and computing aspects of your research. For the Electron-Ion Collider, we are looking for ways to make software (and computing) "easier" to use. All scientists of all levels worldwide should be empowered to participate in Electron-Ion Collider simulations and analyses actively. \r\n\r\nIn this presentation, we will summarize our work on user-centered design for the Electron-Ion Collider. We have collected information on the community\u0027s specific software tools and practices on an annual basis. We have also organized focus group discussions with the broader community and developed user archetypes based on the feedback from the focus groups. The user archetypes represent a common class of users and provide input to software developers as to which users they are writing software for and help with structuring documentation.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 603,
            id: 'c16882',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11523/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [1, 'Diefenthaler, Markus'],
                emailHash: 'af834e0ec8ce37c7bc6e53f554561c99',
                familyName: 'Diefenthaler',
                firstName: 'Markus',
                name: 'Markus Diefenthaler',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12089,
            sessionSlotId: 2663,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'User-Centered Design for the Electron-Ion Collider',
            uniqueId: 'c16882',
            url: '/event/459/contributions/11523/',
          },
          c16883: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11538/attachments/9476/13737/Train_to_Sustain_SudhirMalik_CHEP2023.pdf',
                  id: 13737,
                  title: 'Train_to_Sustain_SudhirMalik_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11538,
            description:
              'The HSF/IRIS-HEP Software Training group provides software training skills to new researchers in High Energy Physics (HEP) and related communities. These skills are essential to produce high-quality and sustainable software needed to do the research. Given the thousands of users in the community, sustainability, though challenging, is the centerpiece of its approach. The training modules are open source and collaborative. Different tools and platforms, like GitHub, enable technical continuity, collaboration and nurture the sense to develop software that is reproducible and reusable. This contribution describes these efforts.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 615,
            id: 'c16883',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11538/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Puerto Rico Mayaguez',
                displayOrderKey: [1, 'Malik, Sudhir'],
                emailHash: 'e5820ae3beb3dcf8e65af9f1b607516c',
                familyName: 'Malik',
                firstName: 'Sudhir',
                name: 'Sudhir Malik',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12089,
            sessionSlotId: 2663,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Train to Sustain',
            uniqueId: 'c16883',
            url: '/event/459/contributions/11538/',
          },
        },
        entryType: 'Session',
        friendlyId: 9,
        id: 's12089',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2039/session-timetable.pdf',
        room: 'Chesapeake Meeting Room',
        sessionCode: '',
        sessionId: 2039,
        sessionSlotId: 2663,
        slotTitle: 'Sustainable Languages  and Architectures',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 5 - Sustainable and Collaborative Software Engineering',
        uniqueId: 's12089',
        url: '/event/459/sessions/2039/',
      },
      s12091: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#6f390d',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of California, San Diego',
            displayOrderKey: [0, 'Davila, Diego'],
            emailHash: 'be09e73b4230a27dffdb0d8305b8a1d0',
            familyName: 'Davila',
            firstName: 'Diego',
            name: 'Diego Davila',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Lassnig, Mario'],
            emailHash: 'ef2db26b320ecf7a9b75abbaef349287',
            familyName: 'Lassnig',
            firstName: 'Mario',
            name: 'Mario Lassnig',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16949: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11306/attachments/9319/13517/Automated-Network-Services-Justas-Balcas.pdf',
                  id: 13517,
                  title: 'Automated-Network-Services-Justas-Balcas.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11306,
            description:
              'The Large Hadron Collider (LHC) experiments distribute data by leveraging a diverse array of National Research and Education Networks (NRENs), where experiment data management systems treat networks as a \u201cblackbox\u201d resource. After the High Luminosity upgrade, the Compact Muon Solenoid (CMS) experiment alone will produce roughly 0.5 exabytes of data per year. NREN Networks are a critical part of the success of CMS and other LHC experiments. However, during data movement, NRENs are unaware of data priorities, importance, or need for quality of service, and this poses a challenge for operators to coordinate the movement of data and have predictable data flows across multi-domain networks. The overarching goal of SENSE (The Software-defined network for End-to-end Networked Science at Exascale) is to enable National Labs and universities to request and provision end-to-end intelligent network services for their application workflows leveraging SDN (Software-Defined Networking) capabilities. This work aims to allow LHC Experiments and Rucio, the data management software used by CMS Experiment, to allocate and prioritize certain data transfers over the wide area network. In this paper, we will present the current progress of the integration of SENSE, Multi-domain end-to-end SDN Orchestration with QoS (Quality of Service) capabilities, with Rucio, the data management software used by CMS Experiment.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 58,
            id: 'c16949',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11306/contribution.pdf',
            presenters: [
              {
                affiliation: 'California Institute of Technology',
                displayOrderKey: [1, 'Balcas, Justas'],
                emailHash: '7613c8c60c21277df334265b3852ce87',
                familyName: 'Balcas',
                firstName: 'Justas',
                name: 'Justas Balcas',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12091,
            sessionSlotId: 2664,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Automated Network Services for Exascale Data Movement',
            uniqueId: 'c16949',
            url: '/event/459/contributions/11306/',
          },
          c16950: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11305/attachments/9400/13638/N-DISE%40CEPH23%20v7%5B68%5D.pptx',
                  id: 13638,
                  title: 'N-DISE@CEPH23 v7[68].pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11305,
            description:
              'We present an NDN-based Open Storage System (OSS) plugin for XRootD instrumented with an accelerated packet forwarder, built for data access in the CMS and other experiments at the LHC, together with its current status, performance as compared to other tools and applications, and plans for ongoing developments.\r\n\r\nNamed Data Networking (NDN) is a leading Future Internet Architecture where data in the network is accessed directly by its name rather than the location of the data containers (hosts). NDN enables the joint design of multipath forwarding and caching to achieve superior latency and failover performance. The Caltech team, together with Northeastern University, UCLA, Tennessee Tech and other collaborators from the NDN for Data Intensive Science Experiments (N-DISE) project, has implemented (1) a small C++ NDN library (NDNc) to bridge the existing NDN libraries with the new high-throughput NDN-DPDK forwarder developed by NIST, (2) a corresponding NDN naming scheme for accessing datasets in the network, (3) two basic classes of entities for transferring data in NDN: consumer and producer, and (4) an NDN-based OSS plugin for XRootD.\r\n\r\nThe XRootD plugin offers implementation for all filesystem related calls (e.g., open, read, close) and it embeds the NDN consumer that translates these calls to NDN Interest packets using well-established naming conventions. For example, the Interest for a read operation for the third segment from a file at */path/to/foo* location on disk has the corresponding name */ndnc/ft/path/to/foo/v=1/seg=3*. Once Interest packets are assembled, they are passed to a proxy entity which forwards them to the local interface. The proxy provides reliable data fetching by handling timeouts and retransmissions, and can adopt different congestion control algorithms (e.g., fixed window size, or congestion-aware AIMD). The local interface implements a memif shared memory packet interface, providing high-performance packet transmission to and from the local NDN-DPDK forwarder. NDN Interest packets find nearest copies of requested data on the NDN network, from either in-network caches or data producers. Alongside this plugin, a corresponding producer has been implemented, which can communicate with multiple file systems (CEPH, HDFS); upon receiving Interest packets, the producer responds with data packets that encapsulate byte ranges at proper offsets from an existing file indicated by the segment numbers of received Interest packets.\r\n\r\nIn this paper we present the architecture of the NDNc library, the consumer application and the NDN-based XRootD plugin. We will also present the throughput performance of the plugin over a continental-scale wide area network testbed, in comparison with other tools and applications used for accessing data at the CMS experiment.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 125,
            id: 'c16950',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11305/contribution.pdf',
            presenters: [
              {
                affiliation: 'Tennessee Tech University',
                displayOrderKey: [7, 'Shannigrahi, Susmit'],
                emailHash: '179e853b6e023e4b52f16b1f961973fb',
                familyName: 'Shannigrahi',
                firstName: 'Susmit',
                name: 'Susmit Shannigrahi',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12091,
            sessionSlotId: 2664,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'A Named Data Networking Based Fast Open Storage System plugin for XRootD',
            uniqueId: 'c16950',
            url: '/event/459/contributions/11305/',
          },
          c16951: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11309/attachments/9313/13510/2023-05-08-chep2023-alto-tcn.pptx',
                  id: 13510,
                  title: '2023-05-08-chep2023-alto-tcn.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11309,
            description:
              'There is increasing demand for the efficiency and flexibility of data transport systems supporting data-intensive sciences. With growing data volume, it is essential that the transport system of a data-intensive science project fully utilize all available transport resources (e.g., network bandwidth); to achieve statistical multiplexing gain, there is an increasing trend that multiple projects share the same transport infrastructure, but the wide deployment of a shared infrastructure requires flexible resource control. In this talk, we first conduct a rigorous analysis of existing data transport systems and show that considering the infrastructures as a black box can limit efficiency and flexibility. We then introduce ALTO/TCN, a new architecture that introduces deep infrastructure visibility to achieve efficient, flexible data transport. We will provide additional details on 3 key components to realize the architecture: (1) how to achieve infrastructure visibility in multi-domain networks, using the Internet Engineering Task Force (IETF) Application-Layer Traffic Optimization (ALTO) protocol and the openalto.org visibility orchestrator; (2) how to integrate visibility into transport scheduling optimization, with zero-orde/first-order gradient and time-multiplexing control, using FTS integration as an example; and (3) how to integrate visibility into data selection orchestration, with general distances as a visibility abstraction, using Rucio integration as an example. We will report evaluation results and implementation lessons. We conclude with planning for the next steps, in particular, how the project complements existing related efforts in HEP, such as application awareness (e.g., packet marking) and adaptive networking resource allocation (e.g., NOTED/SENSE/AutoGOLE).',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 584,
            id: 'c16951',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11309/contribution.pdf',
            presenters: [
              {
                affiliation: 'Choate Rosemary Hall',
                displayOrderKey: [6, 'Yang, Ryan'],
                emailHash: 'd2b5b6b5aeec2fadc9cf1b0926ead0bb',
                familyName: 'Yang',
                firstName: 'Ryan',
                name: 'Ryan Yang',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12091,
            sessionSlotId: 2664,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title:
              'ALTO/TCN: Toward an Architecture of Efficient and Flexible Data Transport Control for Data-Intensive Sciences using Deep Infrastructure Visibility',
            uniqueId: 'c16951',
            url: '/event/459/contributions/11309/',
          },
          c16952: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11330/attachments/9299/13833/CHEP%202023-v.pdf',
                  id: 13833,
                  title: 'CHEP 2023-v.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11330,
            description:
              'In 2029 the LHC will start the High-Luminosity LHC (HL-LHC) program, with a boost in the integrated luminosity resulting in an unprecedented amount of experimental and simulated data samples to be transferred, processed and stored in disk and tape systems across the Worldwide LHC Computing Grid (WLCG). Content delivery network (CDN) solutions are being explored with the purposes of improving the performance of the compute tasks reading input data via the Wide Area Network (WAN), and also to provide a mechanism for cost-effective deployment of lightweight storages supporting traditional or opportunistic compute resources. In this contribution we study the benefits of applying cache solutions for the CMS experiment, in particular the configuration and deployment of xCache serving data to two Spanish WLCG sites supporting CMS: the Tier-1 site at PIC and the Tier-2 site at CIEMAT. The deployment and configuration of the system and the developed monitoring tools will be shown, as well as data popularity studies in relation to the optimization of the cache configuration, the effects on CPU efficiency improvements for analysis tasks, and the cost benefits and impact of including this solution in the region.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 285,
            id: 'c16952',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11330/contribution.pdf',
            presenters: [
              {
                affiliation: 'CIEMAT/PIC',
                displayOrderKey: [1, 'Dengra, Carlos'],
                emailHash: '36861d0a2a87104d499a7489a028ac72',
                familyName: 'Dengra',
                firstName: 'Carlos',
                name: 'Carlos Dengra',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12091,
            sessionSlotId: 2664,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'A study case of Content Delivery Network solutions for the CMS experiment',
            uniqueId: 'c16952',
            url: '/event/459/contributions/11330/',
          },
          c16953: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11321/attachments/9318/13516/Identifying%20and%20Understanding%20Scientific%20Network%20Flows.pdf',
                  id: 13516,
                  title: 'Identifying and Understanding Scientific Network Flows.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11321,
            description:
              'The High-Energy Physics (HEP) and Worldwide LHC Computing Grid (WLCG) communities have faced significant challenges in understanding their global network flows across the world\u2019s research and education (R\u0026E) networks.  When critical links, such as transatlantic or transpacific connections, experience high traffic or saturation, it is very challenging to clearly identify which collaborations are generating the traffic and what activity that traffic represents.  Without knowing the owner and the purpose of the traffic, we are unable to alert them or mitigate the issue.  In general, the HEP and WLCG communities found they have insufficient visibility into which experiments are creating the flows and their purpose.  Having such visibility also allows new understanding of scientific workflows and their associated resource use, and allows organizations and network providers to demonstrate the value of their participation\r\n\r\nThe Research Networking Technical Working Group was formed in the spring of 2020, partially in response to this challenge.   The first of its three working areas concerns network visibility; specifically, the use of packet marking or flow marking to identify the owner and associated activity of network traffic.  The SciTags initiative was created to push this into production, not just for HEP/WLCG, but for any global users of R\u0026E networks.\r\n\r\nWe will describe the status of the work to date, including the evolving architecture and tools, as well as our plans to get this capability into production before the next WLCG Network Data Challenge in early 2024.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 318,
            id: 'c16953',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11321/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Michigan Physics',
                displayOrderKey: [0, 'McKee, Shawn'],
                emailHash: 'a7d121dcc3185f3121537c7f6f762db8',
                familyName: 'McKee',
                firstName: 'Shawn',
                name: 'Shawn McKee',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12091,
            sessionSlotId: 2664,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Identifying and Understanding Scientific Network Flows',
            uniqueId: 'c16953',
            url: '/event/459/contributions/11321/',
          },
          c16954: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11295/attachments/9218/13724/CHEP2023_GS_05082023.pdf',
                  id: 13724,
                  title: 'CHEP2023_GS_05082023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11295,
            description:
              'The capture and curation of all primary instrument data is a potentially valuable source of added insight into experiments or diagnostics in laboratory experiments. The data can, when properly curated, enable analysis beyond the current practice that uses just a subset of the as-measured data. Complete curated data can also be input for machine learning and other data exploration tools. Conveniently storing and accessing instrument data requires that the instruments are connected to databases and users through a networking infrastructure. This infrastructure needs to accommodate a wide array of instruments which can range from single laboratory mounted probes for environment monitoring to computers managing multiple instruments. These resources may also include mobile devices on which researchers record instrument and experiment state related notes. These varied data sources bring with them the challenges of different communications capabilities and protocols as well as the primary data typically being produced in proprietary formats. These challenges are further compounded when the instruments need to operate in secure environments such as required in national laboratories.\r\n\r\nWe will discuss the SmartLab, an ongoing effort to set up a system for instrument and simulation data curation at NASA Langley Research Center. We will outline the challenges faced in managing the data sources required for ongoing research activities and the solutions that are being considered and implemented to address those challenges.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 373,
            id: 'c16954',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11295/contribution.pdf',
            presenters: [
              {
                affiliation: 'NASA',
                displayOrderKey: [1, 'Sauti, Godfrey'],
                emailHash: 'b49f1e21667dc42b294d535e447917a1',
                familyName: 'Sauti',
                firstName: 'Godfrey',
                name: 'Dr Godfrey Sauti',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12091,
            sessionSlotId: 2664,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Managing multi-instrument data streams in secure environments',
            uniqueId: 'c16954',
            url: '/event/459/contributions/11295/',
          },
        },
        entryType: 'Session',
        friendlyId: 5,
        id: 's12091',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2035/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2035,
        sessionSlotId: 2664,
        slotTitle: 'Networks',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffeddf',
        title: 'Track 1 - Data and Metadata Organization, Management and Access',
        uniqueId: 's12091',
        url: '/event/459/sessions/2035/',
      },
      s12092: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d0c296',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Pantaleo, Felice'],
            emailHash: '27ffa3f8059859baeebd18c023b215a8',
            familyName: 'Pantaleo',
            firstName: 'Felice',
            name: 'Felice Pantaleo',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Shahoyan, Ruben'],
            emailHash: '1db06f137fc13354d671d2e50bcdf999',
            familyName: 'Shahoyan',
            firstName: 'Ruben',
            name: 'Ruben Shahoyan',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16999: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11376/attachments/9373/13594/CHEP.pdf',
                  id: 13594,
                  title: 'CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11376,
            description:
              'The CMS collaboration has chosen a novel high granularity calorimeter (HGCAL) for  the endcap regions as part of its planned upgrade for the high luminosity LHC. The calorimeter will have fine segmentation in both the transverse and longitudinal directions and will be the first such calorimeter specifically optimised for particle flow reconstruction to operate at a colliding-beam experiment. The calorimeter data will be part of the Level 1 trigger of the CMS experiment and, together with tracking information that will also be available, will allow particle-flow techniques to be used as part of this trigger. The trigger has tight constraints on latency and rate, and will need to be implemented in hardware. The high granularity results in around six million readout channels in total, reduced to one million that are used at 40 MHz as part of the Level 1 trigger, presenting a significant challenge in terms of data manipulation and processing for the trigger system; the trigger data volumes will be an order of magnitude above those currently handled at CMS. In addition, the high luminosity will result in an average of 140 (or more) interactions per bunch crossing that give a huge background rate in the forward region and these will need to be efficiently rejected by the trigger algorithms. Furthermore, reconstruction of the particle clusters to be used for particle flow in events with high hit rates is also a complex computational problem for the trigger. The status of the cluster reconstruction algorithms developed to tackle these major challenges, as well as the associated trigger architecture, will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 273,
            id: 'c16999',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11376/contribution.pdf',
            presenters: [
              {
                affiliation: 'LLR, \u00c9cole Polytechnique, Paris',
                displayOrderKey: [1, 'Alves, Bruno'],
                emailHash: '074a817f1c477c187d58dd369eaecaa5',
                familyName: 'Alves',
                firstName: 'Bruno',
                name: 'Bruno Alves',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12092,
            sessionSlotId: 2665,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Cluster reconstruction in the HGCAL at the Level 1 trigger',
            uniqueId: 'c16999',
            url: '/event/459/contributions/11376/',
          },
          c17000: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11366/attachments/9251/13483/CHEP_AITrigger_May2023.pdf',
                  id: 13483,
                  title: 'CHEP_AITrigger_May2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11366,
            description:
              'Fast, efficient and accurate triggers are a critical requirement for modern high energy physics experiments given the increasingly large quantities of data that they produce. The CEBAF Large Acceptance Spectrometer (CLAS12) employs a highly efficient Level 3 electron trigger to filter the amount of data recorded by requiring at least one electron in each event, at the cost of a low purity in electron identification. However, machine learning algorithms are increasingly employed for classification tasks such as particle identification due to their high accuracy and fast processing times. In this article we show how a convolutional neural network could be deployed as a Level 3 electron trigger at CLAS12. We demonstrate that the AI trigger would achieve a significant data reduction compared to the traditional trigger, whilst preserving a 99.5% electron identification efficiency. The AI trigger purity also improves relative to the traditional trigger with increased luminosity, as the AI trigger can achieve a reduction in recorded data with respect to the traditional trigger that increases at a rate of 0.32% per nA whilst keeping a stable efficiency above 99.5%.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 20,
            id: 'c17000',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11366/contribution.pdf',
            presenters: [
              {
                affiliation: 'Glasgow University',
                displayOrderKey: [1, 'Tyson, Richard'],
                emailHash: '3c20fe5f0d9e446783181fec421e2b50',
                familyName: 'Tyson',
                firstName: 'Richard',
                name: 'Mr Richard Tyson',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12092,
            sessionSlotId: 2665,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Level-3 trigger for CLAS12 with Artificial Intelligence',
            uniqueId: 'c17000',
            url: '/event/459/contributions/11366/',
          },
          c17001: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11379/attachments/9229/13393/Trigger2023.pdf',
                  id: 13393,
                  title: 'Trigger2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11379,
            description:
              'Long-lived particles (LLPs) are very challenging to search for with current detectors and computing requirements, due to their very displaced vertices. This study evaluates the ability of the trigger algorithms used in the Large Hadron Collider beauty (LHCb) experiment to detect long-lived particles and attempts to adapt them to enhance the sensitivity of this experiment to undiscovered long-lived particles. One of the challenges in the track reconstruction is to deal with the large amount of combinatorics of hits. A dedicated algorithm has been developed to cope with the large data output. When fully implemented, this algorithm would greatly increase the available statistics for any long-lived particle search in the forward region, for the Standard Model of particle physics and beyond.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 68,
            id: 'c17001',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11379/contribution.pdf',
            presenters: [
              {
                affiliation: 'La Salle URL',
                displayOrderKey: [2, 'Vilas\u00eds-Cardona, Xavier'],
                emailHash: '77bd9720451fecfcbc7e81925e606ba8',
                familyName: 'Vilas\u00eds-Cardona',
                firstName: 'Xavier',
                name: 'Prof. Xavier Vilas\u00eds-Cardona',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12092,
            sessionSlotId: 2665,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Impact of the high-level trigger for detecting long-lived particles at LHCb',
            uniqueId: 'c17001',
            url: '/event/459/contributions/11379/',
          },
          c17002: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11386/attachments/9466/13723/summers_l1tscj.pdf',
                  id: 13723,
                  title: 'summers_l1tscj.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11386,
            description:
              'The Phase-2 Upgrade of the CMS Level-1 Trigger will reconstruct particles using the Particle Flow algorithm, connecting information from the tracker, muon, and calorimeter detectors, and enabling fine-grained reconstruction of high level physics objects like jets. We have developed a jet reconstruction algorithm using a cone centred on an energetic seed from these Particle Flow candidates. The implementation is designed to find up to 16 jets in each Xilinx Ultrascale+ FPGA, with a latency of less than 1 \u03bcs, and event throughput of 6.7 MHz to fit within the L1T system constraints. Pipelined processing enables reconstruction of jet collections with different cone sizes for little additional resource cost. The design of the algorithm also provides a platform for additional computation using the jet constituents, such as jet tagging using neural networks. In this talk we will describe the implementation, its jet reconstruction performance, computational metrics, and the developments towards jet tagging.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 287,
            id: 'c17002',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11386/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Summers, Sioni'],
                emailHash: 'c4d59ceecb2e654eb0791b51dff488ee',
                familyName: 'Summers',
                firstName: 'Sioni',
                name: 'Sioni Summers',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12092,
            sessionSlotId: 2665,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'Reconstructing jets in the Phase-2 upgrade of the CMS Level-1 Trigger with a seeded cone algorithm',
            uniqueId: 'c17002',
            url: '/event/459/contributions/11386/',
          },
          c17003: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11365/attachments/9465/13722/summers_l1pfp.pdf',
                  id: 13722,
                  title: 'summers_l1pfp.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11365,
            description:
              'The CMS experiment has greatly benefited from the utilization of the particle-flow (PF) algorithm for the offline reconstruction of the data. The Phase II upgrade of the CMS detector for the High Luminosity upgrade of the LHC (HL-LHC) includes the introduction of tracking in the Level-1 trigger, thus offering the possibility of developing a simplified PF algorithm in the Level-1 trigger. We present the logic of the algorithm, along with its inputs and its firmware implementation. We show that this implementation is capable of operating under the limited timing and processing resources available in the Level-1 trigger environment. The expected performance and physics implications of such an algorithm are shown using Monte Carlo samples with h\u03b9gh pile-up, simulating the harsh conditions of the HL-LHC. New calorimeter features allow for better performance under high pileup (PU) to be achieved, provided that careful tuning and selection of the prompt clusters has been made. Additionally, advanced pile-up techniques are needed to preserve the physics performance in the high-intensity environment. We present a method that combines all information yielding PF candidates and performs Pile-Up Per Particle Identification (PUPPI) capable of running in the low latency level-1 trigger environment. Demonstration of the algorithm on dedicated hardware relying on ATCA platform is presented',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 233,
            id: 'c17003',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11365/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Summers, Sioni'],
                emailHash: 'c4d59ceecb2e654eb0791b51dff488ee',
                familyName: 'Summers',
                firstName: 'Sioni',
                name: 'Sioni Summers',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12092,
            sessionSlotId: 2665,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'The Particle Flow Algorithm in the Phase II Upgrade of the CMS Level-1 Trigger.',
            uniqueId: 'c17003',
            url: '/event/459/contributions/11365/',
          },
          c17004: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11387/attachments/9524/13813/CHEP_2023.pdf',
                  id: 13813,
                  title: 'CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11387,
            description:
              'The current and future programs for accelerator-based neutrino imaging detectors feature the use of Liquid Argon Time Projection Chambers (LArTPC) as the fundamental detection technology. These detectors combine high-resolution imaging and precision calorimetry to enable the study of neutrino interactions with unparalleled capabilities. However, the volume of data from LArTPCs will exceed 25 Petabytes each year for DUNE (Deep Underground Neutrino Experiment) and event reconstruction techniques are complex, requiring significant computational resources. These aspects of LArTPC data make utilization of real-time event triggering and event filtering algorithms that can distinguish signal from background important, but still challenging to accomplish with reasonable efficiency especially for low energy neutrino interactions. At Fermilab, we are developing a machine-learning-based trigger and filtering algorithm for the lab\u0027s flagship experiment DUNE, to extend the sensitivity of the detector, particularly for low energy neutrinos that do not come from an accelerator beam. Building off of recent research in machine learning to improve artificial intelligence, this new trigger algorithm will employ software to optimize data collection, pre-processing, and to make a final event selection decision. Development and testing of the trigger decision system will leverage data from MicroBooNE, ProtoDUNE, and Short Baseline Neutrino (SBN) LArTPC detectors, and will also provide benefits to the physics programs of those experiments. \r\nThis talk will focus on application of a Convolutional Neural Network (CNN) to MicroBooNE data and will study performance metrices such as memory usage and latency. We will also discuss progress towards applying a Semantic Segmentation with Sparse Convolutional Network (SparseCNN) on the same data and compare the performance of the two algorithms.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 619,
            id: 'c17004',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11387/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Bhattacharya, Meghna'],
                emailHash: '51ca869ff9af20bcc2b841920ef686f9',
                familyName: 'Bhattacharya',
                firstName: 'Meghna',
                name: 'Meghna Bhattacharya',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12092,
            sessionSlotId: 2665,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'Online tagging and triggering with deep learning AI for next generation particle imaging detector',
            uniqueId: 'c17004',
            url: '/event/459/contributions/11387/',
          },
        },
        entryType: 'Session',
        friendlyId: 6,
        id: 's12092',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2036/session-timetable.pdf',
        room: 'Marriott Ballroom V-VI',
        sessionCode: '',
        sessionId: 2036,
        sessionSlotId: 2665,
        slotTitle: 'Online Reconstruction',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#000000',
        title: 'Track 2 - Online Computing',
        uniqueId: 's12092',
        url: '/event/459/sessions/2036/',
      },
      s12094: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Girone, Maria'],
            emailHash: '059c67f423c94e588bce78fb5a617463',
            familyName: 'Girone',
            firstName: 'Maria',
            name: 'Maria Girone',
          },
        ],
        description: '',
        duration: 120.0,
        endDate: {
          date: '2023-05-08',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17267: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/12491/attachments/9561/13874/CHEP-2023.pdf',
                  id: 13874,
                  title: 'CHEP-2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12491,
            description:
              'Instead of focusing on the concrete challenges of incremental changes to HEP driven by AI/ML, it is perhaps a useful exercise to think through more radical, speculative changes. What might be enabled if we embraced a dramatically different approach? What would we lose? How would those changes impact the computational, organizational, and epistemological nature of the field?',
            duration: 30.0,
            endDate: {
              date: '2023-05-08',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 645,
            id: 'c17267',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12491/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Wisconsin-Madison',
                displayOrderKey: [0, 'Cranmer, Kyle'],
                emailHash: 'e98725ee562a640cae3cb7c20b97071b',
                familyName: 'Cranmer',
                firstName: 'Kyle',
                name: 'Kyle Cranmer',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12094,
            sessionSlotId: 2666,
            startDate: {
              date: '2023-05-08',
              time: '16:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Radically different futures for HEP enabled by AI/ML',
            uniqueId: 'c17267',
            url: '/event/459/contributions/12491/',
          },
          c17268: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12492/attachments/9428/13857/AI%20and%20beyond.pdf',
                  id: 13857,
                  title: 'AI and beyond.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12492,
            description:
              'Simulation is a critical component of high energy physics research, with a corresponding computing footprint. Generative AI has emerged as a promising complement to intensive full simulation with relatively high accuracy compared to existing classical fast simulation alternatives. Such algorithms are naturally suited to acceleration on coprocessors, potentially running fast enough to match the high data volumes at next-generation experiments. Numerous techniques are currently being explored, each with its own advantages and challenges. Looking beyond the next generation, foundational building blocks of AI such as automatic differentiation and gradient descent are now being incorporated into fully differentiable programming. This new paradigm will provide revolutionary tools for designing and optimizing future experiments.',
            duration: 30.0,
            endDate: {
              date: '2023-05-08',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 646,
            id: 'c17268',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12492/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [0, 'Pedro, Kevin'],
                emailHash: '7a5074de312c74765891001ef6287822',
                familyName: 'Pedro',
                firstName: 'Kevin',
                name: 'Kevin Pedro',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12094,
            sessionSlotId: 2666,
            startDate: {
              date: '2023-05-08',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'AI and Beyond: New Techniques for Simulation and Design in HEP',
            uniqueId: 'c17268',
            url: '/event/459/contributions/12492/',
          },
          c17269: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11544/attachments/9383/13607/DM-TSP-CHEP.pdf',
                  id: 13607,
                  title: 'DM-TSP-CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 11544,
            description:
              'A [Dark Matter Science Project][1] is being developed in the context of the [ESCAPE project][2] as a collaboration between scientists in European Research Infrastructures and experiments seeking to explain the nature of dark matter (such as HL-LHC, KM3NeT, CTA, DarkSide). \r\nThe goal of this ESCAPE Science Project is to highlight the synergies between different dark matter communities and experiments, by producing new scientific results as well as by making the necessary data and software tools fully available. \r\nAs part of this Science Project, we use experimental data and software algorithms from selected direct detection, indirect detection, and particle collider experiments involved in ESCAPE as prototypes for end-to-end analysis pipelines on a Virtual Research Environment that is being prepared as one of the building blocks of the European Open Science Cloud (EOSC).\r\nThis contribution focuses on the implementation of the workflows on the Virtual Research Environment using ESCAPE tools (such as the Data Lake and REANA), and on the prospects for data management, data analysis and computing in the EOSC-Future project. \r\n\r\n\r\n  [1]: https://projectescape.eu/dark-matter-test-science-project\r\n  [2]: https://www.projectescape.eu',
            duration: 30.0,
            endDate: {
              date: '2023-05-08',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 516,
            id: 'c17269',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11544/contribution.pdf',
            presenters: [
              {
                affiliation: 'LAPP/CNRS',
                displayOrderKey: [0, 'Little, Jared'],
                emailHash: 'd8b7f9f4e4e7ee3d4c4386b1fb62352e',
                familyName: 'Little',
                firstName: 'Jared',
                name: 'Jared Little',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12094,
            sessionSlotId: 2666,
            startDate: {
              date: '2023-05-08',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'The ESCAPE Dark Matter Science Project in the European Science Cloud',
            uniqueId: 'c17269',
            url: '/event/459/contributions/11544/',
          },
          c17270: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11808/attachments/9354/13721/Chep23_GLAMANNA_v1.pptx',
                  id: 13721,
                  title: 'Chep23_GLAMANNA_v1.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 11808,
            description:
              'The EU-funded ESCAPE project has brought together the ESFRI and other world class Research Infrastructures in High Energy and Nuclear Physics, Astro-Particle Physics, and Astronomy.\u00a0 In the 3 years of the project many synergistic and collaborative aspects have been highlighted and explored,\u00a0 from pure technical collaboration on common solutions for data management, AAI, and workflows, through development of new tools, such as AI/ML, and in education and training, for example in the area of Research Software.\u00a0 In addition, the project has shown that the communities have a lot in common, and can act as a single voice towards the funding agencies, the EC, and other key developments such as the European Open Science Cloud.\u00a0 \u00a0ESCAPE is one of five such "cluster" projects, and the communities involved have found that the cluster concept is extremely useful in structuring the overall scientific community, and with many cross-domain commonalities are very important in acting together towards various political and funding bodies.\u00a0 Consequently we consider forming a long-term ESCAPE collaboration, that would exist independently of specific project funding, that can maintain the synergistic aspects of the ESCAPE scientific communities, while bringing new ones into the forum, and coordinate the interaction of those communities with the broader scientific and funding landscape.\u00a0 This talk will justify such a collaboration, outline some of its goals, and discuss the possible forms that it can take.',
            duration: 30.0,
            endDate: {
              date: '2023-05-08',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 15,
            id: 'c17270',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11808/contribution.pdf',
            presenters: [
              {
                affiliation: 'CNRS-LAPP',
                displayOrderKey: [1, 'Lamanna, Giovanni'],
                emailHash: 'cdc02ae271185551be9986c9716ffde6',
                familyName: 'Lamanna',
                firstName: 'Giovanni',
                name: 'Giovanni Lamanna',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12094,
            sessionSlotId: 2666,
            startDate: {
              date: '2023-05-08',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'The ESCAPE Collaboration - long term perspective',
            uniqueId: 'c17270',
            url: '/event/459/contributions/11808/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's12094',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2666,
        slotTitle: '"Challenges in AI/ML" \u0026 "Opportunities in ESCAPE"',
        startDate: {
          date: '2023-05-08',
          time: '16:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's12094',
        url: '/event/459/sessions/2024/',
      },
      s12096: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#feffbf',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Hageboeck, Stephan'],
            emailHash: '64d6d832b57aa9e7597d4011228b0da0',
            familyName: 'Hageboeck',
            firstName: 'Stephan',
            name: 'Stephan Hageboeck',
          },
          {
            affiliation: 'University of Wisconsin\u2013Madison',
            displayOrderKey: [0, 'Held, Alexander'],
            emailHash: 'fed5998fb15c9323f97c094991d6da70',
            familyName: 'Held',
            firstName: 'Alexander',
            name: 'Alexander Held',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16911: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11581/attachments/9430/13775/(Backup)%20Making%20Likelihood%20Calculations%20Fast_%20Automatic%20Differentiation%20Applied%20to%20RooFit.pdf',
                  id: 13775,
                  title:
                    '(Backup) Making Likelihood Calculations Fast_ Automatic Differentiation Applied to RooFit.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11581/attachments/9430/13673/go',
                  id: 13673,
                  title: 'Google Slides Presentation',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11581,
            description:
              'With the growing datasets of current and next-generation High-Energy and Nuclear Physics (HEP/NP) experiments, statistical analysis has become more computationally demanding. These increasing demands elicit improvements and modernizations in existing statistical analysis software. One way to address these issues is to improve parameter estimation performance and numeric stability using automatic differentiation (AD). AD\u0027s computational efficiency and accuracy is superior to the preexisting numerical differentiation techniques and offers significant performance gains when calculating the derivatives of functions with a large number of inputs, making it particularly appealing for statistical models with many parameters. For such models, many HEP/NP experiments use RooFit, a toolkit for statistical modeling and fitting that is part of ROOT.\r\n\r\nIn this talk, we report on the effort to support the AD of RooFit likelihood functions. Our approach is to extend RooFit with a tool that generates overhead-free C++ code for a full likelihood function built from RooFit functional models. Gradients are then generated using Clad, a compiler-based source-code-transformation AD tool, using this C++ code. We present our results from applying AD to the entire minimization pipeline and profile likelihood calculations of several RooFit and HistFactory models at the LHC-experiment scale. We show significant reductions in calculation time and memory usage for the minimization of such likelihood functions. We also elaborate on this approach\u0027s current limitations and explain our plans for the future.\r\n\r\nThis contribution combines R\u0026D expertise from computer science applied at scale for HEP/NP analysis: we demonstrate that source-transformation-based AD can be incorporated into complex, domain-specific codes such as RooFit to give substantial performance and scientific capability improvements.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 389,
            id: 'c16911',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11581/contribution.pdf',
            presenters: [
              {
                affiliation: 'Princeton University (US)/CERN',
                displayOrderKey: [1, 'Singh, Garima'],
                emailHash: '1ccf5e354ed8653328614093104f1a5e',
                familyName: 'Singh',
                firstName: 'Garima',
                name: 'Garima Singh',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12096,
            sessionSlotId: 2667,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Making Likelihood Calculations Fast: Automatic Differentiation Applied to RooFit',
            uniqueId: 'c16911',
            url: '/event/459/contributions/11581/',
          },
          c16912: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11570/attachments/9440/13688/roofit_heterogeneous_chep_2023_with_transitions.pdf',
                  id: 13688,
                  title: 'roofit_heterogeneous_chep_2023_with_transitions.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11570,
            description:
              'RooFit is a library for building and fitting statistical models that is part of ROOT. It is used in most experiments in particle physics, in particular, the LHC experiments. Recently, the backend that evaluates the RooFit likelihood functions was rewritten to support performant computations of model components on different hardware. This new backend is referred to as the "batch mode". So far, it supports GPUs with CUDA and also the vectorizing instructions on the CPU. With ROOT 6.28, the new batch mode is feature-complete and speeds up all use cases targeted by RooFit, even on a single CPU thread. The GPU backend further reduces the likelihood evaluation time, particularly for unbinned fits to large datasets. The speedup is most significant when all likelihood components support GPU evaluation. Still, if this is not the case, the backend will optimally distribute the computation on the CPU and GPU to guarantee a speedup.\r\n\r\nRooFit is a very extensible library with a vast user interface to inject behavior changes at almost every point of the likelihood calculation, which the new heterogeneous computation backend must handle. This presentation discusses our approach and lessons learned when facing this challenge. The highlight of this contribution is showcasing the performance improvements for benchmark examples, fits from the RooFit tutorials, and real-world fit examples from LHC experiments. We will also elaborate on how users can implement GPU support for their custom probability density functions and explain the current limitations and future developments.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 447,
            id: 'c16912',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11570/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Rembser, Jonas'],
                emailHash: '790a0742f578f9f288e58160937a95f1',
                familyName: 'Rembser',
                firstName: 'Jonas',
                name: 'Jonas Rembser',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12096,
            sessionSlotId: 2667,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'RooFit\u0027s new heterogeneous computing backend',
            uniqueId: 'c16912',
            url: '/event/459/contributions/11570/',
          },
          c16913: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11580/attachments/9270/13480/RooFit_CHEP.pdf',
                  id: 13480,
                  title: 'RooFit_CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11580,
            description:
              'RooFit is a toolkit for statistical modeling and fitting, presented first at CHEP2003, and together with RooStats is used for measurements and statistical tests by most experiments in particle physics, particularly the LHC experiments.\r\n\r\nAs the LHC program progresses, physics analyses become more ambitious and computationally more demanding, with fits of hundreds of data samples to joint models with over a thousand parameters no longer an exception. While such complex fits can be robustly performed in RooFit,  they may take many hours on a single CPU, significantly impeding the ability of physicists to interactively understand, develop and improve them. Here were present recent RooFit developments to address this, focusing on significant improvements of wall-time performance of complex fits.\r\n\r\nA complete rewrite of the internal back-end of the RooFit likelihood calculation code in ROOT 6.28 now allows to massively parallelize RooFit likelihood fits in two ways. Gradients that are normally serially calculated inside MINUIT, and which dominate the total fit time, are now calculated in a parallel way inside RooFit. Furthermore, calculations of the likelihood in serial phases of the minimizer (initialization and gradient descent steps) are also internally parallelized. No modification of any user code is required to take advantage of these features.\r\n\r\nA key to achieving good scalability for these parallel calculations is close to perfect load balancing over the workers, which is complicated by the fact that for realistic complex fit models the calculations to parallelize cannot be split in components of equal or even comparable size. As part of this update, instruments have been added to RooFit for extensive performance monitoring that allow the user to understand the effect of algorithmic choices in task scheduling and mitigate performance bottlenecks.\r\n\r\nWe will show that that with a new dynamic scheduling strategy and a strategic choice of ordering derivative calculations excellent scalability can be achieved, resulting in an order-of-magnitude wall-time speedups for complex realistic LHC fits such as the ATLAS Run-2 combined Higgs interpretation.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 488,
            id: 'c16913',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11580/contribution.pdf',
            presenters: [
              {
                affiliation: 'Nikhef',
                displayOrderKey: [1, 'Wolffs, Zef'],
                emailHash: 'a70d096e583e620fe9e4d727eb0c57b7',
                familyName: 'Wolffs',
                firstName: 'Zef',
                name: 'Zef Wolffs',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12096,
            sessionSlotId: 2667,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'Build-a-Fit: RooFit configurable parallelization and fine-grained benchmarking tools for order of magnitude speedups in your fits',
            uniqueId: 'c16913',
            url: '/event/459/contributions/11580/',
          },
          c16914: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11597/attachments/9469/13862/Minuit2_CHEP2023.pdf',
                  id: 13862,
                  title: 'Minuit2_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11597,
            description:
              'Minuit is a program implementing a function minimisation algorithm written at CERN more than 50 years ago. It is still used by almost all statistical analysis in High Energy Physics to find optimal likelihood and best parameter values.  A new version, Minuit2,  has been re-implemented the original algorithm in C++  a few years ago and it is provided as a ROOT library or a standalone C++ module. It is also available as a Python package, IMinuit. \r\nThis new version has been recently improved by adding some new features. These include support for external gradients and hessian, allowing the use of Automatic Differentiation techniques or parallel computation of the gradients and the addition of  new minimisation algorithms such as BFGS and Fumili. We will present an overview of the new implementation showing the new added features and we will as well present a comparison with other existing minimisation packages, available in C++ or in the Python scientific ecosystem.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 535,
            id: 'c16914',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11597/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Moneta, Lorenzo'],
                emailHash: 'f93807550591a4b3698b7ee42329e540',
                familyName: 'Moneta',
                firstName: 'Lorenzo',
                name: 'Lorenzo Moneta',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12096,
            sessionSlotId: 2667,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'New developments in Minuit2',
            uniqueId: 'c16914',
            url: '/event/459/contributions/11597/',
          },
          c16915: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11587/attachments/9349/13552/CHEP_Horstmann_08-05-2023.pdf',
                  id: 13552,
                  title: 'CHEP_Horstmann_08-05-2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11587,
            description:
              'Collider physics analyses have historically favored Frequentist statistical methodologies, with some exceptions of Bayesian inference in LHC analyses through use of the Bayesian Analysis Toolkit (BAT). We demonstrate work towards an approach for performing Bayesian inference for LHC physics analyses that builds upon the existing APIs and model building technology of the pyhf and PyMC Python libraries and leverages pyhf\u2019s automatic differentiation and hardware acceleration through its JAX computational backend. This approach presents a path toward unified APIs in pyhf that allow for users to choose a Frequentist or Bayesian approach towards statistical inference, leveraging their respective strengths as needed, without having to transition between using multiple libraries or fall back to using pyhf with BAT through the Julia programming language PyCall package. Examples of Markov chain Monte Carlo implementations using Metropolis-Hastings and Hamiltonian Monte Carlo are presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 605,
            id: 'c16915',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11587/contribution.pdf',
            presenters: [
              {
                affiliation: 'TU Munich / Max Planck Institute for Physics',
                displayOrderKey: [3, 'Horstmann, Malin'],
                emailHash: 'eb2644264216df2f5a6f5aafab0698c2',
                familyName: 'Horstmann',
                firstName: 'Malin',
                name: 'Malin Horstmann',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12096,
            sessionSlotId: 2667,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Bayesian methodologies in particle physics with pyhf',
            uniqueId: 'c16915',
            url: '/event/459/contributions/11587/',
          },
          c16916: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11573/attachments/9512/13820/CHEP-2023.pdf',
                  id: 13820,
                  title: 'CHEP-2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11573,
            description:
              'Many current analyses in nuclear and particle physics are in search for signals that are encompassed by irreducible background events. These background events, entirely surrounding a signal of interest, would lead to inaccurate results when extracting physical observables from the data, due to the inability to reduce the signal to background ratio using any type of selection criteria. By looking at a data set in multiple dimensions, the phase space of a desired reaction can be characterized by a set of coordinates, where a subset of these coordinates (known as reference coordinates) contains a distinguishable distribution where the signal and background can easily be determined. The approach then uses the space defined by the non-reference coordinates, to determine the k-nearest neighbors of an event, where these events can then be fit on the reference coordinates of these k-nearest neighbors (using an unbinned maximum likelihood fit, etc.). From the fit, a quality factor can be defined for each event in the data set that states the probability that it originates from the actual signal of interest. The unique aspect of this procedure requires no *a priori* information of the signal or background distributions within the phase space in the desired reaction. This and many other useful properties for this statistical weighting procedure makes this method more advantageous in certain analyses than other methods. A detailed overview of this procedure will be shown along with examples using Monte Carlo and GlueX data.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 618,
            id: 'c16916',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11573/contribution.pdf',
            presenters: [
              {
                affiliation: 'Carnegie Mellon University',
                displayOrderKey: [1, 'Baldwin, Zachary'],
                emailHash: 'e7b596fb4a99172bb4be74ce30c25b34',
                familyName: 'Baldwin',
                firstName: 'Zachary',
                name: 'Zachary Baldwin',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12096,
            sessionSlotId: 2667,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'A multidimensional, event-by-event, statistical weighting procedure for signal to background separation',
            uniqueId: 'c16916',
            url: '/event/459/contributions/11573/',
          },
        },
        entryType: 'Session',
        friendlyId: 10,
        id: 's12096',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2040/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VIII',
        sessionCode: '',
        sessionId: 2040,
        sessionSlotId: 2667,
        slotTitle: 'Statistical Inference and Fitting',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#1f1f02',
        title: 'Track 6 - Physics Analysis Tools',
        uniqueId: 's12096',
        url: '/event/459/sessions/2040/',
      },
      s12097: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#1a3f14',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Nebraska-Lincoln',
            displayOrderKey: [0, 'Weitzel, Derek'],
            emailHash: 'ca0d239ab6ae0ec500eff8eb24779855',
            familyName: 'Weitzel',
            firstName: 'Derek',
            name: 'Derek Weitzel',
          },
          {
            affiliation: 'KEK',
            displayOrderKey: [0, 'Kishimoto, Tomoe'],
            emailHash: '1c2a9b3c698c4de9040fbe367da6851b',
            familyName: 'Kishimoto',
            firstName: 'Tomoe',
            name: 'Tomoe Kishimoto',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17082: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11632/attachments/9258/13433/che23_provisioner_rc3.pdf',
                  id: 13433,
                  title: 'che23_provisioner_rc3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11632,
            description:
              'The OSG-operated Open Science Pool is an HTCondor-based virtual cluster that aggregates resources from compute clusters provided by several organizations. A user can submit batch jobs to the OSG-maintained scheduler, and they will eventually run on a combination of supported compute clusters without any further user action. Most of the resources are not owned by, or even dedicated to OSG, so demand-based dynamic provisioning is important for maximizing usage without incurring excessive waste.\r\nOSG has long relied on GlideinWMS for most of its resource provisioning needs, but is limited to resources that provide a Grid-compliant Compute Entrypoint. To work around this limitation, the OSG software team had developed a pilot container that resource providers could use to directly contribute to the OSPool. The problem of that approach is that it is not demand-driven, relegating it to backfill scenarios only.\r\nTo address this limitation, a demand-driven direct provisioner of Kubernetes resources has been developed and successfully used on the PRP. The setup still relies on the OSG-maintained backfill container images, it just automates the provisioning matchmaking and successive requests. That provisioner has also been recently extended to support Lancium, a green computing cloud provider with a Kubernetes-like proprietary interface. The provisioner logic had been intentionally kept very simple, making this extension a low cost project.\r\nBoth PRP and Lancium resources have been provisioned exclusively using this mechanism for almost a year with great results.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 38,
            id: 'c17082',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11632/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'Andrijauskas, Fabio'],
                emailHash: '0078d106b8d33e1c6ca918958fb59267',
                familyName: 'Andrijauskas',
                firstName: 'Fabio',
                name: 'Fabio Andrijauskas',
              },
              {
                affiliation: 'IceCube, University of Wisconsin-Madison',
                displayOrderKey: [1, 'Schultz, David'],
                emailHash: '5a6b0c8862953cf0796175777a912011',
                familyName: 'Schultz',
                firstName: 'David',
                name: 'David Schultz',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12097,
            sessionSlotId: 2668,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Demand-driven provisioning of Kubernetes-like resource in OSG',
            uniqueId: 'c17082',
            url: '/event/459/contributions/11632/',
          },
          c17084: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11637/attachments/9399/13710/chep2023_alturany.pdf',
                  id: 13710,
                  title: 'chep2023_alturany.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11637,
            description:
              'New particle/nuclear physics experiments require a massive amount of computing power that is only achieved by using high performance clusters directly connected to the data acquisition systems and integrated into the online systems of the experiments. However, integrating an HPC cluster into the online system of an experiment means: Managing and synchronizing thousands of processes that handle the huge throughput. In this work, modular components that can be used to build and integrate such a HPC cluster in the experiment control systems (ECS) will be introduced.\r\n\r\nThe Online Device Control library (ODC) [1] in combination with the Dynamic Deployment System (DDS) [2, 3] and FairMQ [4] message queuing library offers a sustainable solution for integrating HPC cluster controls into an ECS.\r\n\r\nDDS as part of the ALFA framework [5] is a toolset that automates and significantly simplifies a dynamic deployment of user-defined processes and their dependencies on any resource management system (RMS) using a given process graph (topology). Where ODC is the tool to control and communicate with a topology of FairMQ processes using DDS. ODC is designed to act as a broker between a high level experiment control system and a low level task management system e.g.: DDS.\r\n\r\nIn this presentation the architecture of both DDS and ODC will be discussed, as well as the design decisions taken based on the experience gained of using these tools on production by the ALICE experiment at CERN to deploy and control thousands of processes (tasks) on the Event Processing Nodes cluster (EPN) during Run3 as a part of the ALICE O2 software ecosystem [6].\r\n\r\nReferences:\r\n 1. FairRootGroup, \u201cODC git repository\u201d, Last accessed 14th of November 2022: https://github.com/FairRootGroup/ODC\r\n 2. FairRootGroup, \u201cDDS home site\u201d, Last accessed 14th of November 2022: http://dds.gsi.de\r\n 3. FairRootGroup, \u201cDDS source code repository\u201d, Last accessed 14th of November 2022: https://github.com/FairRootGroup/DDS\r\n 4. FairMQ, \u201cFairMQ git repository\u201d, Last accessed 14th of November 2022: https://github.com/FairRootGroup/FairMQ\r\n5.https://indico.gsi.de/event/2715/contributions/11355/attachments/8580/10508/ALFA_Fias.pdf\r\n 5. ALICE Technical Design Report (2nd of June 2015), Last accessed 14th of November: https://cds.cern.ch/record/2011297/files/ALICE-TDR-019.pdf',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 173,
            id: 'c17084',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11637/contribution.pdf',
            presenters: [
              {
                affiliation: 'GSI',
                displayOrderKey: [1, 'Al-Turany, Mohammad'],
                emailHash: '7553d4db8d9901e91d5bbb73ec902007',
                familyName: 'Al-Turany',
                firstName: 'Mohammad',
                name: 'Dr Mohammad Al-Turany',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12097,
            sessionSlotId: 2668,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Modular toolsets for integrating HPC clusters in experiment control systems',
            uniqueId: 'c17084',
            url: '/event/459/contributions/11637/',
          },
          c17085: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11624/attachments/9301/13496/C4P_CHEP_2023_KIT.pdf',
                  id: 13496,
                  title: 'C4P_CHEP_2023_KIT.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11624,
            description:
              'PUNCH4NFDI, funded by the Germany Research Foundation initially for five years, is a diverse consortium of particle, astro-, astroparticle, hadron and nuclear physics embedded in the National Research Data Infrastructure initiative.\r\n\r\nIn order to provide seamless and federated access to the huge variaty of compute and storage systems provided by the participating communities covering their very diverse needs, the Compute4PUNCH and Storage4PUNCH concepts have been developed. Both concepts comprise state-of-the-art technolgies such as a token-based AAI for standardised access to compute and storage resources. The community supplied heterogenous HPC, HTC and Cloud compute resources are dynamically and transparently integrated into one federated HTCondor based overlay batch system using the COBaLD/TARDIS resource meta-scheduler. Traditional login nodes and a JupyterHub provide entry points into the entire landscape of available compute resources, while container technologies and the CERN Virtual Machine File System (CVMFS) ensure a scalable provisioning of community specific software environments. In Storage4PUNCH, community supplied storage systems mainly based on dCache or XRootD technology are being federated in a common infrastructure employing methods that are well established in the wider HEP community. Furthermore existig technologies for caching as well as metadata handling are being evaluated with the aim for a deeper integration. The combined Compute4PUNCH and Storage4PUNCH environment will allow a large variety of researchers to carry out resource-demanding analysis tasks. \r\n\r\nIn this contribution we will present the Compute4PUNCH and Storage4PUNCH concepts, the current status of the developments as well as first experiences with scientific  applications being executed on the available prototypes.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 357,
            id: 'c17085',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11624/contribution.pdf',
            presenters: [
              {
                affiliation: 'Karlsruher Institut f\u00fcr Technologie (KIT)',
                displayOrderKey: [1, 'Giffels, Manuel'],
                emailHash: 'c3b7b748f2c627ad7cf333c2513e0b07',
                familyName: 'Giffels',
                firstName: 'Manuel',
                name: 'Dr Manuel Giffels',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12097,
            sessionSlotId: 2668,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Federated Heterogenous Compute and Storage Infrastructure for the PUNCH4NFDI Consortium',
            uniqueId: 'c17085',
            url: '/event/459/contributions/11624/',
          },
          c17086: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11633/attachments/9263/13435/MLaaS4HEP.pdf',
                  id: 13435,
                  title: 'MLaaS4HEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11633,
            description:
              'Nowadays Machine Learning (ML) techniques are successfully used in many areas of High-Energy Physics (HEP) and will play a significant role also in the upcoming High-Luminosity LHC upgrade foreseen at CERN, when a huge amount of data will be produced by LHC and collected by the experiments, facing challenges at the exascale. To favor the usage of ML in HEP analyses, it would be useful to have a service allowing to perform the entire ML pipeline (in terms of reading the data, processing data, training a ML model, and serving predictions) directly using ROOT files of arbitrary size from local or remote distributed data sources. The MLaaS4HEP solution we have already proposed aims to provide such kind of service and to be HEP experiment agnostic. Recently new features have been introduced, such as the possibility to provide pre-processing operations, defining new branches, and applying cuts. To provide users with a real service and to integrate it into the INFN Cloud, we started working on MLaaS4HEP cloudification. This would allow to use cloud resources and to work in a distributed environment. In this work, we provide updates on this topic and discuss a working prototype of the service running on INFN Cloud. It includes an OAuth2 proxy server as authentication/authorization layer, a MLaaS4HEP server, an XRootD proxy server for enabling access to remote ROOT data, and the TensorFlow as a Service (TFaaS) service in charge of the inference phase. With this architecture a HEP user can submit ML pipelines, after being authenticated and authorized, using local or remote ROOT files simply using HTTP calls.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 410,
            id: 'c17086',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11633/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Bologna',
                displayOrderKey: [1, 'Giommi, Luca'],
                emailHash: 'ff1dc0210e633470aca38fb0110d8e24',
                familyName: 'Giommi',
                firstName: 'Luca',
                name: 'Luca Giommi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12097,
            sessionSlotId: 2668,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Progress on cloud native solution of Machine Learning as Service for HEP',
            uniqueId: 'c17086',
            url: '/event/459/contributions/11633/',
          },
          c17087: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11622/attachments/9326/13526/SOTERIA%20-%20CHEP.pdf',
                  id: 13526,
                  title: 'SOTERIA - CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11622,
            description:
              'Managing a secure software environment is essential to a trustworthy cyberinfrastructure. Software supply chain attacks may be a top concern for IT departments, but they are also an aspect of scientific computing. The threat to scientific reputation caused by problematic software can be just as dangerous as an environment contaminated with malware.  The issue of managing environments affects any individual researcher performing computational research but is more acute for multi-institution scientific collaborations, such as high energy physics experiments, as they often preside over complex software stacks and must manage software environments across many distributed computing resources.  We discuss a new project, Securing an Open and Trustworthy Ecosystem for Research Infrastructure and Applications (SOTERIA), to provide the HEP community with a container registry service and provide additional capabilities to assist with vulnerability assessment, authorship and provenance, and distribution.  This service is currently being used to deliver containers for a wide range of the OSG Fabric of Services, the Coffea-Casa analysis facility, and the Analysis Facility at the University of Chicago; we discuss both the functionality it currently provides and the operational experiences of running a critical service for scientific cyberinfrastructure.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 551,
            id: 'c17087',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11622/contribution.pdf',
            presenters: [
              {
                affiliation: 'Morgridge Institute for Research',
                displayOrderKey: [1, 'Bockelman, Brian'],
                emailHash: '672e6085d6c2aaaac23af4e09549c9fd',
                familyName: 'Bockelman',
                firstName: 'Brian',
                name: 'Brian Bockelman',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12097,
            sessionSlotId: 2668,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Securing an Open and Trustworthy Ecosystem for Research Infrastructure and Applications (SOTERIA)',
            uniqueId: 'c17087',
            url: '/event/459/contributions/11622/',
          },
        },
        entryType: 'Session',
        friendlyId: 11,
        id: 's12097',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2041/session-timetable.pdf',
        room: 'Marriott Ballroom IV',
        sessionCode: '',
        sessionId: 2041,
        sessionSlotId: 2668,
        slotTitle: 'Dynamic Provisioning and Anything-As-A-Service',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#f1ffef',
        title: 'Track 7 - Facilities and Virtualization',
        uniqueId: 's12097',
        url: '/event/459/sessions/2041/',
      },
      s12098: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#e3f2d3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Paul Scherrer Institut',
            displayOrderKey: [0, 'Lange, Clemens'],
            emailHash: '429cdad04fa5f47ba2f6993b0b32ec69',
            familyName: 'Lange',
            firstName: 'Clemens',
            name: 'Clemens Lange',
          },
          {
            affiliation: 'DESY',
            displayOrderKey: [0, 'Hernandez Villanueva, Michel'],
            emailHash: '19113800d40b0ae945d3d902953e2b1c',
            familyName: 'Hernandez Villanueva',
            firstName: 'Michel',
            name: 'Michel Hernandez Villanueva',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17123: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11674/attachments/9436/13682/CHEP2003-InvolvingNewGenerations.pdf',
                  id: 13682,
                  title: 'CHEP2003-InvolvingNewGenerations.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11674,
            description:
              'Since 1984 the Italian groups of the Istituto Nazionale di Fisica Nucleare (INFN) and Italian Universities, collaborating with the\r\nDOE laboratory of Fermilab (US) have been running a two-month summer training program for Italian university students. While\r\nin the first year the program involved only four physics students of the University of Pisa, in the following years it was extended\r\nto engineering students. This extension was very successful and the engineering students have been since then extremely well\r\naccepted by the Fermilab Technical, Accelerator and Scientific Computing Division groups. Over the many years of its existence,\r\nthis program has proven to be the most effective way to engage new students in Fermilab endeavours. Many students have\r\nextended their collaboration with Fermilab with their Master Thesis and PhD.\r\nSince 2004 the program has been supported in part by DOE in the frame of an exchange agreement with INFN. Over its almost\r\n40 years of history, the program has grown in scope and size and has involved more than 550 Italian students from more than\r\n20 Italian Universities, A number of Institutes of Research, including ASI and INAF in Italy, and the ISSNAF Foundation in the\r\nUS, have provided additional financial support. Since the program does not exclude appropriately selected non-italian students,\r\na handful of students of European and non-European Universities were also accepted in the years.\r\nEach intern is supervised by a Fermilab Mentor responsible for performing the training program. Training programs spanned\r\nfrom Tevatron, CMS, Muon (g-2), Mu2e and Short Baseline Neutrino Experiments and DUNE design and experimental data\r\nanalysis, development of particle detectors (silicon trackers, calorimeters, drift chambers, neutrino and dark matter detectors),\r\ndesign of electronic and accelerator components, development of infrastructures and software for tera-data handling, research\r\non superconductive elements and on accelerating cavities, theory of particle accelerators\r\nSince 2010, within an extended program supported by the Italian Space Agency and the Italian National Institute of Astrophysics,\r\na total of 30 students in physics, astrophysics and engineering have been hosted for two months in summer at US space\r\nscience Research Institutes and laboratories.\r\nIn 2015 the University of Pisa included these programs within its own educational programs. Accordingly, Summer School\r\nstudents are enrolled at the University of Pisa for the duration of the internship and are identified and ensured as such. At the\r\nend of the internship the students are required to write summary reports on their achievements. After positive evaluation by a\r\nUniversity Examining Board, interns are acknowledged 6 ECTS credits for their Diploma Supplement.\r\nInformation on student recruiting methods, on training programs of recent years and on final student\u0027s evaluation process at\r\nFermilab and at the University of Pisa will be given in the presentation.\r\nIn the ears 2020 and 2021 the Program has been canceled due to the persisting effects of the sanitary emergency which\r\nprevented researchers and students to travel to the United States. In 2022 the Program was successfully restarted and allowed\r\na cohort of 21 students in physics and engineering to be trained for nine weeks at Fermilab. In the talk we will provide a detailed descriptions of the program, which can be easily taken as a model that can be easily adopted by interested Laboratories.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 39,
            id: 'c17123',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11674/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Mambelli, Marco'],
                emailHash: 'f3deb9637a78bebf20c5c69407f5b777',
                familyName: 'Mambelli',
                firstName: 'Marco',
                name: 'Marco Mambelli',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12098,
            sessionSlotId: 2669,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Involving the new generations in Fermilab endeavours',
            uniqueId: 'c17123',
            url: '/event/459/contributions/11674/',
          },
          c17125: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11678/attachments/9226/13558/CHEP%20presentation-3.pdf',
                  id: 13558,
                  title: 'CHEP presentation-3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11678,
            description:
              'The common form of inter-institute particle physics experiment collaborations generates unique needs for member management including paper authorship, shift assignments, subscription to mailing lists and access to 3rd party applications such as Github and Slack. For smaller collaborations, typically no facility for centralized member management is available and these needs are usually manually handled by long-term members in smaller collaborations but the management becomes tedious as collaborations grow. To automate many of these tasks for the expanding XENON collaboration, we developed the XENONnT User Management Website, a web application that stores and updates data related to the collaboration members through the use of NodeJs and MongoDB. The application allows for the scheduling of shifts for members to coordinate between institutes. User manipulation of 3rd party applications are implemented using REST API integration. The XENONnT User Management Website is open source and is a show case of quick implementation of utility application using the web framework.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 257,
            id: 'c17125',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11678/contribution.pdf',
            presenters: [
              {
                affiliation: 'Rice University',
                displayOrderKey: [0, 'Liang, Shixiao'],
                emailHash: '2225c62351b67896f1ceca1c56eb730c',
                familyName: 'Liang',
                firstName: 'Shixiao',
                name: 'Shixiao Liang',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12098,
            sessionSlotId: 2669,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'A customized web application for XENON collaboration member management',
            uniqueId: 'c17125',
            url: '/event/459/contributions/11678/',
          },
          c17126: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11672/attachments/9285/13787/HSFDAWG_Training_CHEP2023.pdf',
                  id: 13787,
                  title: 'HSFDAWG_Training_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11672,
            description:
              'We will discuss the training and on-boarding initiatives currently adopted by a range of High Energy Physics (HEP) experiments. On-boarding refers to the process by which new members of a collaboration gain the knowledge and skills needed to become effective members. Fast and efficient on-boarding is increasingly important for HEP experiments as physics analyses and, as a consequence, the related software becomes ever more complex with growing datasets. The HEP Software Foundation (HSF) held a meeting series in Summer 2022 where 6 LHC and non-LHC experiments showcased their initiatives. Here we summarise and analyse these initiatives and attempt to determine a set of best practices for current and future experiments.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 158,
            id: 'c17126',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11672/contribution.pdf',
            presenters: [
              {
                affiliation: 'US Naval Academy',
                displayOrderKey: [14, 'Reinsvold Hall, Allison'],
                emailHash: '8116a688cc90d5a1cd4e2efdd5db6d63',
                familyName: 'Reinsvold Hall',
                firstName: 'Allison',
                name: 'Allison Reinsvold Hall',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12098,
            sessionSlotId: 2669,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Training and on-boarding initiatives in HEP',
            uniqueId: 'c17126',
            url: '/event/459/contributions/11672/',
          },
          c17127: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11680/attachments/9471/13729/chep2023_lange.pptx',
                  id: 13729,
                  title: 'chep2023_lange.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11680,
            description:
              'Building successful multi-national collaborations is challenging. The scientific communities in a range of physical sciences have been learning how to build collaborations that build upon regional capabilities and interests over decades, iteratively with each new generation of large scientific facilities required to advance their scientific knowledge. Much of this effort has naturally focused on collaborations for the construction of hardware and instrumentation. Software has however also become a critical element to design and maximize the physics discovery potential of large data intensive science projects. To fully realize their discovery potential a new generation of software algorithms and approaches is required. Building these research software collaborations is challenging and inherently international, matching the international nature of the experimental undertakings themselves. Initiatives such as the HEP Software Foundation have been instrumental in establishing international research software collaborations in high-energy physics, in particular between European and North American researchers.\r\n\r\nThis talk is about a new initiative, HSF-India, aiming to implement new and impactful research software collaborations between India, Europe and the U.S. The experimental scope of this project is relatively broad, aiming to bring together researchers across facilities with common problems in research. The research and development scope is on three primary topics: analysis software and integrated facilities for analysis; simulation techniques including generators and Artificial Intelligence based approaches; and enabling open science.  By exploiting national capabilities and strengths, an immediate mutual benefit of the international collaboration will be a training network that enables early-career researchers to pursue impactful research software initiatives in ways that advance their careers in experimental data-intensive science. In this presentation, we will describe the scope of this initiative, its mechanisms for fostering new collaborations, and ways for interested research groups to get involved. We will also discuss thoughts towards broadening our initiative to foster more general collaborations in research software projects between Asian researchers and European/North American researchers who are already jointly pursuing \u201cteam-science\u201d endeavors in research software for high-energy, nuclear and astro-particle physics.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 507,
            id: 'c17127',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11680/contribution.pdf',
            presenters: [
              {
                affiliation: 'Princeton University',
                displayOrderKey: [2, 'Lange, David'],
                emailHash: 'b3766e1bb47ff6f9cdf65d395d9c0ad8',
                familyName: 'Lange',
                firstName: 'David',
                name: 'David Lange',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12098,
            sessionSlotId: 2669,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Building International Research Software Collaborations in Physics',
            uniqueId: 'c17127',
            url: '/event/459/contributions/11680/',
          },
          c17128: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11691/attachments/9261/13583/support.pdf',
                  id: 13583,
                  title: 'pellegrino_infn_t1_user_support.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11691,
            description:
              'The Italian WLCG Tier-1 located in Bologna and managed by INFN-CNAF has a long tradition in supporting several research communities in the fields of High-Energy Physics, Astroparticle Physics, Gravitational Waves, Nuclear Physics and others, to which provides computing resources in the form of batch computing, both HPC, HTC and Cloud, and storage. Although the LHC experiments at CERN represent the main users of the Tier-1 resources, an increasing number of communities and experiments are also being supported in all of their computing activities. Due to this demanding user base, an efficient support system is needed in order to assure a smooth and appropriate exploitation of the computing infrastructure.\r\nIn this framework, such a role is played by the Tier-1 User Support group, which acts as the entry point for services, support requests, and problem reports. The group makes use of multiple systems to meet the different needs and specificities of the supported experiments. Moreover, the group continuously maintains detailed knowledge base in the form of an on-line user guide and develops tools to advertise specific informations about the services available to the communities in a form that is easy to access and use.\r\nThe communication channels are represented by ticketing systems and also by mailing lists used for a more direct communication, allowing to promptly notify maintenance interventions, downtimes and more in general all the new features and services provided by the center.\r\nIn this talk, the ticketing systems, tools, platforms and services that User Support offers, and the internal organization of the department will be described. Future workflow plans in view of the DATACLOUD project, which will require an increasing effort, will also be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 413,
            id: 'c17128',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11691/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN-CNAF',
                displayOrderKey: [4, 'Pellegrino, Carmelo'],
                emailHash: 'e4ac6dabf304314d7b3e624fd81a29d8',
                familyName: 'Pellegrino',
                firstName: 'Carmelo',
                name: 'Dr Carmelo Pellegrino',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12098,
            sessionSlotId: 2669,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Support for experiments at INFN-T1',
            uniqueId: 'c17128',
            url: '/event/459/contributions/11691/',
          },
          c17311: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11707/attachments/9473/13746/CHEP23-DUNE-Computing-Tutorials-DeMuth-v1.3.pdf',
                  id: 13746,
                  title: 'CHEP23-DUNE-Computing-Tutorials-DeMuth-v1.3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11707,
            description:
              'Providing computing training to the next generation of physicists is the\r\nprincipal driver for a biannual multi-day workshop hosted by the DUNE\r\nComputing Consortium. Materials are cast in the Software Carpentries\r\ntemplates, and to date topics have included storage space, data\r\nmanagement, LArSoft, grid job submission and monitoring. Moreover,\r\nexperts provide extended breakout sessions to demonstrate the\r\nintricacies of the unique software used in HEP analysis. Each workshop\r\nsession uses live documents for real time correspondence, and are\r\ncaptured on Zoom; afterwards, videos are embedded on the corresponding\r\nwebpages for review.  As a GitHub repository, shared editing of the\r\nlearning modules is straightforward, and provides a trusted framework to\r\nextend to other training topics in the future. An overview of the\r\nmachinery will be provided, post workshop statistics will be discussed,\r\nwith lessons learned will be the focus of this presentation.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 625,
            id: 'c17311',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11707/contribution.pdf',
            presenters: [
              {
                affiliation: 'Valley City State University',
                displayOrderKey: [1, ' DeMuth, David'],
                emailHash: '60ff649c3c719bbb5600329f38c1219b',
                familyName: ' DeMuth',
                firstName: 'David',
                name: 'David  DeMuth',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12098,
            sessionSlotId: 2669,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'DUNE Computing Tutorials',
            uniqueId: 'c17311',
            url: '/event/459/contributions/11707/',
          },
        },
        entryType: 'Session',
        friendlyId: 12,
        id: 's12098',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2042/session-timetable.pdf',
        room: 'Marriott Ballroom I',
        sessionCode: '',
        sessionId: 2042,
        sessionSlotId: 2669,
        slotTitle: 'Tailored Collaboration and Training Initiatives',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#253f08',
        title: 'Track 8 - Collaboration, Reinterpretation, Outreach and Education',
        uniqueId: 's12098',
        url: '/event/459/sessions/2042/',
      },
      s12099: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#92b6db',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'VALLECORSA, SOFIA'],
            emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
            familyName: 'VALLECORSA',
            firstName: 'SOFIA',
            name: 'SOFIA VALLECORSA',
          },
          {
            affiliation: 'University of Washington (US)',
            displayOrderKey: [0, 'Schaarschmidt, Jana'],
            emailHash: '6f556891cf4ef30c552a4ac4aa1c87dc',
            familyName: 'Schaarschmidt',
            firstName: 'Jana',
            name: 'Jana Schaarschmidt',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17158: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11722/attachments/9274/13448/MLhad%20-%20CHEP%202023.pdf',
                  id: 13448,
                  title: 'MLhad - CHEP 2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11722,
            description:
              'Hadronization is an important step in Monte Carlo event generators, where quarks and gluons are bound into physically observable hadrons. Today\u2019s generators rely on finely-tuned empirical models, such as the Lund string model; while these models have been quite successful overall, there remain phenomenological areas where they do not match data well. In this talk, we present MLHad, a machine-learning-based alternative for generating hadronization chains, which we intend ultimately to be data-trainable. Latent-space vectors are encoded, trained to be distributed according to a user-defined distribution using the sliced-Wasserstein distance in the loss function, then decoded to simulate hadronization.\r\n\r\nWe show that generated pion multiplicities and cumulative kinematic distributions match those generated using Pythia (arXiv:2203.04983). We also present our more-recent work using normalizing flows to generate non-pion hadrons and to propagate errors through the encoder and decoder. Finally, we present comparisons with empirical data.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 43,
            id: 'c17158',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11722/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Cincinnati',
                displayOrderKey: [5, 'Wilkinson, Michael'],
                emailHash: '7adc6570433c5d2297039e75398c956d',
                familyName: 'Wilkinson',
                firstName: 'Michael',
                name: 'Michael Wilkinson',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12099,
            sessionSlotId: 2670,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'MLHad: Simulating Hadronization with Machine Learning',
            uniqueId: 'c17158',
            url: '/event/459/contributions/11722/',
          },
          c17159: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11715/attachments/9448/13700/SYMBA_CHEP23.pdf',
                  id: 13700,
                  title: 'SYMBA_CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11715,
            description:
              'The calculation of particle interaction squared amplitudes is a key step in the calculation of cross sections in high-energy physics. These lengthy calculations are currently done using domain-specific symbolic algebra tools, where the time required for the calculations grows rapidly with the number of final state particles involved. While machine learning has proven to be highly successful in numerical calculations in high-energy physics, analytical calculations using machine learning are still in their beginning. We developed a transformer-based sequence-to-sequence model inspired by natural language processing that is able to accurately predict squared amplitudes of QCD and QED processes, respectively, when trained on symbolic sequence pairs. The goal of this work is to significantly reduce the computational time and, more importantly, build a model that scales well with the number of final state particles. To the best of our knowledge, this model (SYMBA) is the first model that encapsulates a wide range of symbolic squared amplitude calculations and, therefore, represents a potentially significant advance in using symbolic machine learning techniques for practical scientific computations.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 262,
            id: 'c17159',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11715/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Kentucky',
                displayOrderKey: [1, 'Alnuqaydan, Abdulhakim'],
                emailHash: '13dc8b797d449276a8182574939523a6',
                familyName: 'Alnuqaydan',
                firstName: 'Abdulhakim',
                name: 'Mr Abdulhakim Alnuqaydan',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12099,
            sessionSlotId: 2670,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'SYMBA: Symbolic Computation of Squared Amplitudes in High Energy Physics with Machine Learning',
            uniqueId: 'c17159',
            url: '/event/459/contributions/11715/',
          },
          c17160: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11758/attachments/9499/13767/CHEP_Gradients.pdf',
                  id: 13767,
                  title: 'CHEP_Gradients.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11758/attachments/9499/13800/CHEP_Gradients.pptx',
                  id: 13800,
                  title: 'CHEP_Gradients.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11758,
            description:
              'The recent advances in Machine Learning and high-dimensional gradient-based optimization has led to increased interest in the question of whether we can use such methods to optimize the design of future detectors for high-level physics objectives. However this program faces a fundamental obstacle: The quality of a detector design must be judged on the physics inference it enables, but both simulation and reconstruction of events are to a large degree described by discrete and thus naively non-differentiable stochastic branching (e.g. particle showers, ) and clustering processes (e.g. jet algorithms). In this work we explore the use of gradient estimation techniques based on differentiable and probabilistic programming that provide sufficiently stable estimates such that they may be used in an optimization loop. We showcase the effectiveness of such methods in benchmark scenarios ranging from a few to many thousands of optimizable parameters and discuss current limitations and future directions.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 542,
            id: 'c17160',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11758/contribution.pdf',
            presenters: [
              {
                affiliation: 'Technical University of Munich',
                displayOrderKey: [1, 'Heinrich, Lukas'],
                emailHash: 'fd0f8bf65da577a884b4bb1260da8d60',
                familyName: 'Heinrich',
                firstName: 'Lukas',
                name: 'Lukas Heinrich',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12099,
            sessionSlotId: 2670,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'On Estimating Gradients of Discrete Stochastic Programs for Detector Design Optimization',
            uniqueId: 'c17160',
            url: '/event/459/contributions/11758/',
          },
          c17161: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11753/attachments/9409/13650/HVCM_Anomaly_Detection_CHEP_2023.pdf',
                  id: 13650,
                  title: 'HVCM_Anomaly_Detection_CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11753,
            description:
              'We present a Multi-Module framework based on Conditional Variational Autoencoder (CVAE) to detect anomalies in the High Voltage Converter Modulators (HVCMs) which have historically been a cause of major down time for the Spallation Neutron Source (SNS) facility. Previous studies using machine learning techniques were to predict faults ahead of time in the SNS accelerator using a Single Modulator. Using the proposed methodology, we can detect faults in the power signals coming from multiple HVCMs that vary in design specifications and operating conditions. By conditioning the model according to the given modulator system, we can capture different representations of the normal waveforms for multiple systems. Our experiments with the SNS experimental data show that the trained model generalizes well to detecting several fault types for different systems, which can be valuable to improve the HVCM reliability and SNS as a result. We also explore several neural network architectures in our CVAE model by visualizing their loss landscapes to study the stability and generalization of the developed models and assist in hyper-parameter optimization and model selection to produce well-performed predictions.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 601,
            id: 'c17161',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11753/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [6, 'Alanazi, Yasir'],
                emailHash: '57c9bad819cb6e077e706c8c7c62cf7b',
                familyName: 'Alanazi',
                firstName: 'Yasir',
                name: 'Mr Yasir Alanazi',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12099,
            sessionSlotId: 2670,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Multi-Module based VAE to predict HVCM faults in the SNS accelerator',
            uniqueId: 'c17161',
            url: '/event/459/contributions/11753/',
          },
          c17162: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11752/attachments/9432/13678/CHEP%20Resilient%20VAE%205-8-22.pdf',
                  id: 13678,
                  title: 'CHEP Resilient VAE 5-8-22.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11752,
            description:
              'Significant advances in utilizing deep learning for anomaly detection have been made in recent years. However, these methods largely assume the existence of a normal training set (i.e., uncontaminated by anomalies), or even a completely labeled training set. In many complex engineering systems, such as particle accelerators, labels are sparse and expensive; in order to perform anomaly detection in these cases, we must drop these assumptions and utilize a completely unsupervised method. Moreover, only identifying the anomaly is insufficient: operators of these complex systems need additional localization information to identify the root cause of the anomaly and make an informed response. In this paper, we introduce the Resilient Variational Autoencoder (ResVAE), a deep generative model that is designed for anomaly detection, is resilient to anomalies in the training data, and yields feature-level anomaly attribution. During training, the ResVAE learns the anomaly probability for each sample as a whole and for each individual feature, and uses those probabilities to ignore anomalous examples in the training data. We apply our method to detecting anomalies in the accelerator status at the SLAC Linac Coherent Light Source (LCLS). Using shot-to-shot data from the beam position monitoring system, we identify and characterize several types of anomalies apparent in the accelerator, including many instances of known failures modes (e.g., beam loss) that are missed by current detection methods.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 132,
            id: 'c17162',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11752/contribution.pdf',
            presenters: [
              {
                affiliation: 'Stanford University',
                displayOrderKey: [1, 'Humble, Ryan'],
                emailHash: '582b9202ad73add0816d12c3ea2fae79',
                familyName: 'Humble',
                firstName: 'Ryan',
                name: 'Ryan Humble',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12099,
            sessionSlotId: 2670,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Resilient Variational Autencoder for Unsupervised Anomaly Detection at the SLAC Linac Cohrerent Light Source',
            uniqueId: 'c17162',
            url: '/event/459/contributions/11752/',
          },
          c17163: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11744/attachments/9392/13622/MoEDAL_ML_CHEP_2023.pdf',
                  id: 13622,
                  title: 'MoEDAL_ML_CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11744,
            description:
              'The MoEDAL experiment at CERN (https://home.cern/science/experiments/moedal-mapp) carries out searches for highly ionising exotic particles such as magnetic monopoles. One of the technologies deployed in this task is the Nuclear Track Detector (NTD). In the form of plastic films, these are passive detectors that are low cost and easy to handle. After exposure to the LHC collision environment in the LHCb cavern at point 8 on the LHC ring, they are etched and scanned under a microscope to potentially reveal the etch-pit signature of the passage of an exotic highly ionising particle. The scanning process takes place using microscopes and expert human inspection. With several 10s of metres squared of deployed plastic, and large backgrounds complicating the analysis, the process is highly time consuming.\r\n \r\nWe have studied the use of AI to identify etch-pits in scanned images of NTDs. A specially prepared stack of NTD plastic films \u2013 where one layer has been exposed to the harsh LHC environment and the others have not \u2013 is placed in a heavy ion beam to simulate the passage of particles such as magnetic monopoles. The plastic is then etched and optically scanned. The images are used to prepare training and evaluation data sets for three different approaches: a deconvolution-convolution algorithm with machine learning based thresholding, a convolutional neural network, trained as a classifier and then used in a fully convolutional mode, and a convolutional neural network making use of a U-Net based technique. \r\n\r\nWe present an overview of MoEDAL and our study, the evaluation of the methods, and the prospects for further uses of AI in this area.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 582,
            id: 'c17163',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11744/contribution.pdf',
            presenters: [
              {
                affiliation: 'Queen Mary University of London',
                displayOrderKey: [1, 'Hays, Jonathan'],
                emailHash: '719a009771001ac504d05ba141daa721',
                familyName: 'Hays',
                firstName: 'Jonathan',
                name: 'Jonathan Hays',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12099,
            sessionSlotId: 2670,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Machine Learning for Etch-Pit Identification at MoEDAL',
            uniqueId: 'c17163',
            url: '/event/459/contributions/11744/',
          },
        },
        entryType: 'Session',
        friendlyId: 13,
        id: 's12099',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2043/session-timetable.pdf',
        room: 'Hampton Roads VII',
        sessionCode: '',
        sessionId: 2043,
        sessionSlotId: 2670,
        slotTitle: 'Applications 1',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#03070f',
        title: 'Track 9 - Artificial Intelligence and Machine Learning',
        uniqueId: 's12099',
        url: '/event/459/sessions/2043/',
      },
      s12101: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#4f144e',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Fermi National Accelerator Laboratory',
            displayOrderKey: [0, 'Timm, Steven'],
            emailHash: '40848b6b99ba5ca40be682891f2cc812',
            familyName: 'Timm',
            firstName: 'Steven',
            name: 'Steven Timm',
          },
          {
            affiliation: 'CSIC',
            displayOrderKey: [0, 'Campos, Isabel'],
            emailHash: '1ba624fa939fe1db5c536ca7116de83b',
            familyName: 'Campos',
            firstName: 'Isabel',
            name: 'Isabel Campos',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17213: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11802/attachments/9232/13397/evolution_computing_040523.pdf',
                  id: 13397,
                  title: 'evolution_computing_040523.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11802,
            description:
              'INFN has been running for more than 20 years a distributed infrastructure (the Tier-1 at Bologna-CNAF and 9 Tier-2 centers) which currently offers about 140000 CPU cores, 120 PB of enterprise-level disk space and 100 PB of tape storage, serving more than 40 international scientific collaborations.\r\nThis Grid-based infrastructure was augmented in 2019 with the INFN Cloud: a production quality multi-site federated Cloud infrastructure, composed by a core backbone, and which is able to integrate other INFN sites and public or private Clouds as well. The INFN Cloud provides a customizable and extensible portfolio offering computing and storage services spanning the IaaS, PaaS and SaaS layers, with dedicated solutions to serve special purposes, such as ISO-certified regions for the handling of sensitive data.\r\nINFN is now revising and expanding its infrastructure to tackle the challenges expected in the next 10 years of scientific computing adopting a \u201ccloud-first\u201d approach, through which all the INFN data centers will be federated via the INFN Cloud middleware and integrated with key HPC centers, such as the pre-exascale Leonardo machine at CINECA.\r\nIn such a process, which involves both the infrastructures and the higher level services, initiatives and projects such as the "Italian National Centre on HPC, Big Data and Quantum Computing" (funded in the context of the Italian "National Recovery and Resilience Plan") and the Bologna Technopole are precious opportunities that will be exploited to offer advanced resources and services to Universities, research institutions and industry.\r\nIn this paper we describe how INFN is evolving its computing infrastructure, with the ambition to create and operate a national vendor-neutral, open, scalable and flexible "data lake" able to serve much more than just INFN users and experiments.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 497,
            id: 'c17213',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11802/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [10, 'Fanzago, Federica'],
                emailHash: '8a95042876907a5bf28b0bde88e522f3',
                familyName: 'Fanzago',
                firstName: 'Federica',
                name: 'Federica Fanzago',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12101,
            sessionSlotId: 2672,
            startDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'INFN and the evolution of distributed scientific computing in Italy',
            uniqueId: 'c17213',
            url: '/event/459/contributions/11802/',
          },
          c17214: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11806/attachments/9260/13429/chep2023_REDSEA_Martinelli.pdf',
                  id: 13429,
                  title: 'chep2023_REDSEA_Martinelli.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11806,
            description:
              'RED-SEA (https://redsea-project.eu/) is a European project funded in the framework of the H2020-JTI-EuroHPC-2019-1 call that started in April 2021. The goal of the project is to evaluate the architectural design of the main elements of the interconnection networks for the next generation of HPC systems supporting hundreds of thousands of computing nodes enabling the Exa-scale for HPC, HPDA and AI applications, and to provide preliminary prototypes. \r\n\r\nThe main technological feature is the BXI network, originally designed and produced by ATOS (France). The plan is to integrate in the next release of the network \u2013 BXI3 \u2013 the architectural solutions and novel IPs developed within the framework of the RED-SEA project. \r\n\r\nThe consortium is composed of 11 well-established research teams across Europe, with extensive experience in interconnects, including network design, deployment and evaluation. \r\n\r\nWithin RED-SEA INFN is adopting a hardware/software co-design approach to design APEnetX, a scalable interconnect prototyped on latest generation Xilinx FPGAs, adding innovative components for the improvement of the performance and resiliency of the interconnect. APEnetX is an FPGA-based, PCIe Gen3/4 network interface card equipped with RDMA capabilities being the endpoint of a direct multidimensional toroidal network and suitable to be integrated in the BXI environment. APEnetX design will be benchmarked on project testbeds using real scientific applications like NEST, a spiking neural network simulator. \r\n\r\nIn this presentation we introduce the main scientific and technological motivations at the basis of the project, focusing on the current status of the development.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 504,
            id: 'c17214',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11806/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN',
                displayOrderKey: [5, 'Martinelli, Michele'],
                emailHash: 'ed0edbdf6017fcaf405ed6b95caa3c8b',
                familyName: 'Martinelli',
                firstName: 'Michele',
                name: 'Dr Michele Martinelli',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12101,
            sessionSlotId: 2672,
            startDate: {
              date: '2023-05-08',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Outlines in hardware and software for new generations of exascale interconnects',
            uniqueId: 'c17214',
            url: '/event/459/contributions/11806/',
          },
          c17215: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11805/attachments/9235/13575/Grandi-CHEP23.pdf',
                  id: 13575,
                  title: 'Grandi-CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11805,
            description:
              'ICSC is one of the five Italian National Centres created in the framework of the Next Generation EU funding by the European Commission. The aim of ICSC, designed and approved through 2022 and eventually started in September 2022, is to create the national digital infrastructure for research and innovation, leveraging exixting HPC, HTC and Big Data infrastructures evolving towards a cloud datalake model. It will be accessible by the scientific and industrial communities through flexible and uniform cloud web interfaces, and will be relying on a high-level support team; as such, it will form a globally attractive ecosystem based on strategic public-private partnerships to fully exploit top level digital infrastructure for scientific and technical computing and promote the development of new computing technologies. \r\n\r\nThe ICSC IT infrastructure is built upon existing scientific digital infrastructures provided by the major national players: GARR, the Italian NREN, provides the network infrastructure, whose capacity will be upgraded to multiples of Tbps; CINECA hosts Leonardo, one of the world largest HPC systems, with a power of over 250 Pflops, that will be further increased and complemented with a quantum computer; INFN contributes with its distributed Big Data cloud infrastructure, built in the last decades to respond to the needs of the High Energy Physics community.\r\n\r\nOn top of the IT infrastructure, several thematic activities will be funded and will focus on the development of tools and applications in several research domains. Of particular relevance to this audience are the activities on "Fundamental Research and Space Economy" and "Astrophysics and Cosmos Observations", strictily aligned with the INFN and HEP core activities. Finally two technological research activities will foster research on "Future HPC and Big Data" and "Quantum Computing".\r\n\r\nIn this contribution, the organisation of the National Centre and its relevance for the HEP community will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 114,
            id: 'c17215',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11805/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Bologna',
                displayOrderKey: [1, 'Grandi, Claudio'],
                emailHash: '7dcd999bfdb7d17bdbcadf18aeba1ed9',
                familyName: 'Grandi',
                firstName: 'Claudio',
                name: 'Claudio Grandi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12101,
            sessionSlotId: 2672,
            startDate: {
              date: '2023-05-08',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'ICSC: The Italian National Research Centre on HPC, Big Data and Quantum Computing',
            uniqueId: 'c17215',
            url: '/event/459/contributions/11805/',
          },
          c17216: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11801/attachments/9401/13639/MeifengLin_LQCD_CHEP2023.pdf',
                  id: 13639,
                  title: 'MeifengLin_LQCD_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11801,
            description:
              'The upcoming exascale computers in the United States and elsewhere will have diverse node architectures, with or without compute accelerators, making it a challenge to maintain a code base that is performance portable across different systems. As part of the US Exascale Computing Project (ECP), the USQCD collaboration has embarked on a collaborative effort to prepare the lattice QCD software suites for exascale, with a particular focus on achieving performance portability across diverse exascale architectures. \r\n\r\nIn this presentation, I will focus on efforts to use compiler directives, OpenMP and OpenACC, to port the Grid C++ lattice QCD library to AMD/Intel/NVIDIA GPUs and multi/many-core CPUs. Performance comparisons with architecture-native implementations in HIP, SYCL and CUDA will be given. I will also discuss the problems encountered and pros and cons of using compiler directives for performance portability.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 224,
            id: 'c17216',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11801/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [1, 'Lin, Meifeng'],
                emailHash: '6ed65b84c3bbe01d123547c3d593ee4e',
                familyName: 'Lin',
                firstName: 'Meifeng',
                name: 'Meifeng Lin',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12101,
            sessionSlotId: 2672,
            startDate: {
              date: '2023-05-08',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'Performance Portability with Compiler Directives for Lattice QCD in the Exascale Era',
            uniqueId: 'c17216',
            url: '/event/459/contributions/11801/',
          },
          c17295: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11844/attachments/9474/13732/p2r-CHEP23.pdf',
                  id: 13732,
                  title: 'p2r-CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11844,
            description:
              'Next generation High-Energy Physics (HEP) experiments are presented with significant computational challenges, both in terms of data volume and processing power. Using compute accelerators, such as GPUs, is one of the promising ways to provide the necessary computational power to meet the challenge. The current programming models for compute accelerators often involve using architecture-specific programming languages promoted by the hardware vendors and hence limit the set of platforms that the code can run on. Developing software with platform restrictions is especially unfeasible for HEP communities as it takes significant effort to convert typical HEP algorithms into ones that are efficient for compute accelerators. Multiple performance portability solutions have recently emerged and provide an alternative path for using compute accelerators, which allow the code to be executed on hardware from different vendors. \r\nWe apply several portability solutions, such as [Kokkos][1], [SYCL][2], [std::execution::par][3] and [Alpaka][4], on two mini-apps extracted from the [mkFit][5] project: p2z and p2r. These apps include basic kernels for a Kalman filter track fit, such as propagation and update of track parameters, for detectors at a fixed z or fixed r position, respectively. The two mini-apps explore different memory layout formats.  \r\n \r\nWe report on the development experience with different portability solutions, as well as their performance on GPUs and many-core CPUs, measured as the throughput of the kernels from different GPU and CPU vendors such as NVIDIA, AMD and Intel.\r\n\r\n\r\n  [1]: https://github.com/kokkos/kokkos\r\n  [2]: https://www.khronos.org/sycl/\r\n  [3]: https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/\r\n  [4]: https://github.com/alpaka-group/alpaka\r\n  [5]: https://trackreco.github.io/',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 252,
            id: 'c17295',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11844/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Kwok, Ka Hei Martin'],
                emailHash: 'c28f0c0979e95d23cd8754deb7de4cc7',
                familyName: 'Kwok',
                firstName: 'Ka Hei Martin',
                name: 'Ka Hei Martin Kwok',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12101,
            sessionSlotId: 2672,
            startDate: {
              date: '2023-05-08',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'Application of performance portability solutions for GPUs and many-core CPUs to track reconstruction kernels',
            uniqueId: 'c17295',
            url: '/event/459/contributions/11844/',
          },
          c17296: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11811/attachments/9215/13376/opticks_20230508_chep_compressed.pdf',
                  id: 13376,
                  title: 'opticks_20230508_chep_compressed.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11811,
            description:
              'Opticks is an open source project that accelerates optical photon simulation by\r\nintegrating NVIDIA GPU ray tracing, accessed via the NVIDIA OptiX 7 API, with\r\nGeant4 toolkit based simulations. A single NVIDIA Turing architecture GPU has\r\nbeen measured to provide optical photon simulation speedup factors exceeding\r\n1500 times single threaded Geant4 with a full JUNO analytic GPU geometry\r\nautomatically translated from the Geant4 geometry. Optical physics processes of\r\nscattering, absorption, scintillator reemission and boundary processes are\r\nimplemented in CUDA based on Geant4.  Wavelength-dependent material and surface\r\nproperties as well as inverse cumulative distribution functions for reemission\r\nare interleaved into GPU textures providing fast interpolated property lookup\r\nor wavelength generation.\r\n\r\nIn this work we describe the near complete re-implementation of geometry and\r\noptical simulation required to adopt the entirely new NVIDIA OptiX 7 API, with\r\nthe implementation now directly CUDA based with OptiX usage restricted to\r\nproviding intersects.  The new Opticks features a modular many small header\r\ndesign that provides fine grained testing both on GPU and CPU as well as\r\nsubstantial code reductions from CPU/GPU sharing.  Enhanced modularity has\r\nenabled CSG tree generalization to support "list-nodes", similar to\r\nG4MultiUnion, that improve performance for complex CSG solids.  Recent addition\r\nof support for interference effects in boundaries with multiple thin layers,\r\nsuch as anti-reflection coatings and photocathodes, using CUDA compatible\r\ntransfer matrix method (TMM) calculations of reflectance, transmittance and\r\nabsorptance is also reported.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 90,
            id: 'c17296',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11811/contribution.pdf',
            presenters: [
              {
                affiliation: 'IHEP',
                displayOrderKey: [0, 'Lin, Tao'],
                emailHash: '2fbe5cd266635003b02a2e156624828b',
                familyName: 'Lin',
                firstName: 'Tao',
                name: 'Dr Tao Lin',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12101,
            sessionSlotId: 2672,
            startDate: {
              date: '2023-05-08',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Opticks : GPU Optical Photon Simulation using NVIDIA OptiX 7 and NVIDIA CUDA',
            uniqueId: 'c17296',
            url: '/event/459/contributions/11811/',
          },
        },
        entryType: 'Session',
        friendlyId: 15,
        id: 's12101',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2045/session-timetable.pdf',
        room: 'Marriott Ballroom VII',
        sessionCode: '',
        sessionId: 2045,
        sessionSlotId: 2672,
        slotTitle: 'Exascale Computing',
        startDate: {
          date: '2023-05-08',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffefff',
        title:
          'Track X - Exascale Science, Heterogeneous Computing and Accelerators, and Quantum Computing',
        uniqueId: 's12101',
        url: '/event/459/sessions/2045/',
      },
      s12103: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d9dfc3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Chulalongkorn University',
            displayOrderKey: [0, 'Srpimanobhas, Norraphat'],
            emailHash: '545290bb86b1edf24b84a6682630eadc',
            familyName: 'Srpimanobhas',
            firstName: 'Norraphat',
            name: 'Dr Norraphat Srpimanobhas',
          },
          {
            affiliation: 'Fermilab',
            displayOrderKey: [0, 'Yang, Tingjun'],
            emailHash: '521815c4f74228afa6e1c602c0d8d4a7',
            familyName: 'Yang',
            firstName: 'Tingjun',
            name: 'Tingjun Yang',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17041: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11436/attachments/9317/13515/CHEP23_RMOLINA.pdf',
                  id: 13515,
                  title: 'CHEP23_RMOLINA.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11436,
            description:
              'The AGATA project (1) aims at building a 4pi gamma-ray spectrometer consisting of 180 germanium crystals, each crystal being divided into 36 segments. Each gamma ray produces an electrical signal within several neighbouring segments, which is compared with a data base of reference signals, enabling to locate the interaction. This step is called Pulse-Shape Analysis (PSA).\r\n\r\nIn the execution chain leading to the PSA, we observe successive data conversions : the original 14 bits integers given by the electronics are finally converted to 32-bit floats. This made us wonder about the real numerical accuracy of the results, and investigate the use of shorter floats, with the hope to speedup the computation, and also reduce a major cache-miss problem previously identified with the Perf (2) tool.\r\n\r\nOur proposed talk would first report about the numerical validation of the C++ PSA code, thanks to the Discrete Stochastic Arithmetic implemented in the CADNA library (3). After the code being properly instrumented, CADNA performs each computation three times with a random rounding mode. This allows, for each operation, to evaluate the number of exact significant digits using a Student test with 95% confidence threshold.\r\n\r\nIn a second step, we will report our successes and challenges while refactoring the code so to mix different numerical formats, using high precision only when necessary, and taking benefit of hardware speedup elsewhere. Such mixed-precision appears as a promising option for high performance computation in the next years, provided we use tools such as CADNA so to keep control of the accuracy of the computed results.\r\n\r\n(1) https://www.agata.org/about\r\n(2) https://perf.wiki.kernel.org/index.php/Main_Page\r\n(3) http://cadna.lip6.fr',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 165,
            id: 'c17041',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11436/contribution.pdf',
            presenters: [
              {
                affiliation: 'IJCLab',
                displayOrderKey: [2, 'Molina, Rom\u00e9o'],
                emailHash: 'a7632154e32743eafda7f33008440cab',
                familyName: 'Molina',
                firstName: 'Rom\u00e9o',
                name: 'Rom\u00e9o Molina',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12103,
            sessionSlotId: 2674,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Investigating mixed-precision for AGATA pulse-shape analysis',
            uniqueId: 'c17041',
            url: '/event/459/contributions/11436/',
          },
          c17042: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11424/attachments/9332/14313/Kiwaku_cpp20_arrays_ACTS_chep23.pdf',
                  id: 14313,
                  title: 'Kiwaku_cpp20_arrays_ACTS_chep23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11424,
            description:
              'Track reconstruction, also known as tracking, is a vital part of the HEP event reconstruction process, and one of the largest consumers of computing resources. The upcoming HL-LHC upgrade will exacerbate the need for efficient software able to make good use of the underlying heterogeneous hardware. However, this evolution should not imply the production of code unintelligible to most of its maintainers, hence the need to provide good usability to both end users and developers.\r\n\r\nC++ has been a language of choice for efficient scientific computing tasks. The Generative Programming paradigm [CZAR98], which relies on heavy type based template meta-programming, provides a powerful solution for supporting multiple execution contexts[MASL16]. Yet, the templates are usually blamed for binary bloat, high code complexity and unreadable error messages. \r\n\r\nIn this presentation, we will discuss recent developments made to the C++ language, helping to define a new process for constructing libraries both efficient and easy to use, using a streamlined Generative Programming process:\r\n\u2022 easier code selection at compile time using \u201cif constexpr\u201d\r\n\u2022 better error reporting using \u201cConcepts\u201d, i.e. compile-time type constraints\r\n\u2022 easier meta-programming with Non-Type Template Parameters\r\n\r\nWe will then introduce Kiwaku[KWK22], a new multidimensional arrays library taking advantage of the most recent C++ usability improvements, yet providing portable performance on various execution contexts (CPU, GPU). We will finally discuss a few proofs of concept, based on use-cases borrowed from the ACTS toolkit[ACTS22]: magnetic field computation, clustering and seeding.\r\n\r\nReferences:\r\n\r\n[ACTS22] Ai, X., Allaire, C., Calace, N. et al. A Common Tracking Software Project. Comput Softw Big Sci 6, 8 (2022). https://doi.org/10.1007/s41781-021-00078-8\r\n\r\n[CZAR98] Krzysztof Czarnecki, Ulrich W. Eisenecker, Robert Gl\u00fcck, David Vandevoorde, Todd L. Veldhuizen: "Generative Programming and Active Libraries". Generic Programming 1998: 25-39\r\n\r\n[KWK22]  Kiwaku main repository - https://github.com/jfalcou/kiwaku/\r\n\r\n[MASL16] Ian Masliah, Marc Baboulin, Joel Falcou: "Meta-programming and Multi-stage Programming for GPGPUs". 2016 IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSOC)',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 279,
            id: 'c17042',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11424/contribution.pdf',
            presenters: [
              {
                affiliation: 'IJCLab - LISN, Universit Paris-Saclay, CNRS/IN2P3',
                displayOrderKey: [0, 'Joube, Sylvain'],
                emailHash: '3b82eecba057e6a8623c81a1aefa86b9',
                familyName: 'Joube',
                firstName: 'Sylvain',
                name: 'Mr Sylvain Joube',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12103,
            sessionSlotId: 2674,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Kiwaku, a C++20 library for multidimensional arrays, applied to ACTS tracking',
            uniqueId: 'c17042',
            url: '/event/459/contributions/11424/',
          },
          c17043: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11418/attachments/9355/13856/A_dimensions_aware_evaluator_for_High_Energy_Physics_applications.pdf',
                  id: 13856,
                  title: 'A_dimensions_aware_evaluator_for_High_Energy_Physics_applications.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11418,
            description:
              'The LHCb software stack is developed in C++ and uses the Gaudi framework for event processing and DD4hep for the detector description. Numerical computations are done either directly in the C++ code or by an evaluator used to process the expressions embedded in the XML describing the detector geometry.\r\n  The current system relies on conventions for the physical units used (identical as what is used in the Geant4 simulation framework) and it is up to the developers to ensure that the correct factors are applied to the values entered. Physical units are not primary entities in the framework, it is therefore not possible to check the dimensional consistency of the computation performed. In this paper we investigate the possibilities to add physical units and dimensions to the existing evaluator or to replace it by a more suitable system, and how this would integrate with the possible tools to express units in C++ code (such as boost::units).',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 407,
            id: 'c17043',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11418/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN EP-LBC',
                displayOrderKey: [0, 'Clemencic, Marco'],
                emailHash: '2d3197cea2de7705cbaf2df989cdf1da',
                familyName: 'Clemencic',
                firstName: 'Marco',
                name: 'Marco Clemencic',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12103,
            sessionSlotId: 2674,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'A physical dimensions aware evaluator for High Energy Physics applications',
            uniqueId: 'c17043',
            url: '/event/459/contributions/11418/',
          },
          c17044: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11414/attachments/9421/13699/CHEP2023%20GNN%20Performance%20for%20ITk.pdf',
                  id: 13699,
                  title: 'CHEP2023 GNN Performance for ITk.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11414,
            description:
              'Applying graph-based techniques, and graph neural networks (GNNs) in particular, has been shown to be a promising solution to the high-occupancy track reconstruction problems posed by the upcoming HL-LHC era. Simulations of this environment present noisy, heterogeneous and ambiguous data, which previous GNN-based algorithms for ATLAS ITk track reconstruction could not handle natively. We present a range of upgrades to the so-called GNN4ITk pipeline that allow detector regions to be handled heterogeneously, ambiguous and shared nodes to be reconstructed more rigorously, and tracks-of-interest to be treated with more importance in training. With these improvements, we are able to present for the first time apples-to-apples comparisons with existing reconstruction algorithms on a range of physics metrics, including reconstruction efficiency across particle type and pileup condition, jet reconstruction performance in dense environments, displaced tracking, and track parameter resolutions. We also demonstrate that our results are robust to misalignment of ITk modules, showing the GNN4ITk approach to perform well under changing experimental conditions. By integrating this solution with the offline ATLAS Athena framework, we also explore a range of reconstruction chain configurations, for example by using the GNN4ITk pipeline to build regions-of-interest while using traditional techniques for track cleaning and fitting.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 421,
            id: 'c17044',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11414/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lawrence Berkeley National Laboratory',
                displayOrderKey: [2, 'Ju, Xiangyang'],
                emailHash: 'cdae8bb08ef4238c3dbc49a841ef65f1',
                familyName: 'Ju',
                firstName: 'Xiangyang',
                name: 'Xiangyang Ju',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12103,
            sessionSlotId: 2674,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Physics Performance of the ATLAS GNN4ITk Track Reconstruction Chain',
            uniqueId: 'c17044',
            url: '/event/459/contributions/11414/',
          },
          c17045: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11452/attachments/9501/13774/Calibration%20Data%20Flow%20and%20Performance%20at%20Belle%20II.pdf',
                  id: 13774,
                  title: 'Calibration Data Flow and Performance at Belle II.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11452,
            description:
              'The Belle II experiment has been accumulating data since 2019 at the SuperKEKB $e^+e^-$ accelerator in Tsukuba, Japan. The accelerator operates at the $\\Upsilon(4S)$ resonance and is an excellent laboratory for precision flavor measurements and dark sector searches. The accumulated data are promptly reconstructed and calibrated at a dedicated calibration center in an automated process based on a directed acyclic graph to resolve dependencies in the calibration using selected prescaled data skims. After calibration, the raw data are reconstructed on the GRID and provided in an analysis-oriented format (mDST) on the GRID for the collaboration. \r\nIn this talk we will present the calibration data flow from raw data to mDST production. We will discuss the physical principles behind the calibrations and how we tune the calibration data samples accordingly. We will show performance metrics which underpin the importance of the data calibration for our precision physics results.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 489,
            id: 'c17045',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11452/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Prim, Markus'],
                emailHash: '9391bd87f68db6d5d254f861db77aa99',
                familyName: 'Prim',
                firstName: 'Markus',
                name: 'Markus Prim',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12103,
            sessionSlotId: 2674,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Calibration Data Flow and Performance at Belle II',
            uniqueId: 'c17045',
            url: '/event/459/contributions/11452/',
          },
          c17046: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11457/attachments/9219/13574/2023.05.08.EIC_Software_Overview.pptx',
                  id: 13574,
                  title: '2023.05.08.EIC_Software_Overview.pptx',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11457/attachments/9219/13381/go',
                  id: 13381,
                  title: 'Google Slides',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11457,
            description:
              'Development of the EIC project detector "ePIC" is now well underway and this includes the "single software stack" used for simulation and reconstruction. The stack combines several non-experiment-specific packages including ACTS, DD4hep, JANA2, and PODIO.  The software stack aims to be forward looking in the era of AI/ML and heterogeneous hardware.  A formal decision making process was implemented to choose the components that involved everyone in the collaboration that was interested. This talk will present an overview of the software stack currently used for development of the ePIC detector and on which we expect to execute the experiment.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 552,
            id: 'c17046',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11457/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [1, 'Lawrence, David'],
                emailHash: '762fb4d3bf38cd850e9b1a0a217d217e',
                familyName: 'Lawrence',
                firstName: 'David',
                name: 'David Lawrence',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12103,
            sessionSlotId: 2674,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'EIC Software Overview',
            uniqueId: 'c17046',
            url: '/event/459/contributions/11457/',
          },
        },
        entryType: 'Session',
        friendlyId: 7,
        id: 's12103',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2037/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VI',
        sessionCode: '',
        sessionId: 2037,
        sessionSlotId: 2674,
        slotTitle: 'Physics performance (part I)',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#272f09',
        title: 'Track 3 - Offline Computing',
        uniqueId: 's12103',
        url: '/event/459/sessions/2037/',
      },
      s12104: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#efebc2',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Unive',
            displayOrderKey: [0, 'Barreiro Megino, Fernando'],
            emailHash: 'f04de6eee7b8dcc20dedefcb9fe659da',
            familyName: 'Barreiro Megino',
            firstName: 'Fernando',
            name: 'Fernando Barreiro Megino',
          },
          {
            affiliation: 'KEK/IPNS',
            displayOrderKey: [0, 'Miyake, Hideki'],
            emailHash: '7ae6773a40c49ae5776d96621f5c7ab5',
            familyName: 'Miyake',
            firstName: 'Hideki',
            name: 'Hideki Miyake',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16842: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11469/attachments/9238/13471/JUNO%20distributed%20computing%20system%20v3.pdf',
                  id: 13471,
                  title: 'JUNO distributed computing system v3.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11469/attachments/9238/13470/JUNO%20distributed%20computing%20system%20v3.pptx',
                  id: 13470,
                  title: 'JUNO distributed computing system v3.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11469,
            description:
              'The Jiangmen Underground Neutrino Observatory (JUNO) is a multipurpose neutrino experiment and the determination of the neutrino mass hierarchy is its primary physics goal. JUNO is going to take data in 2024 with 2PB raw data each year and use distributed computing infrastructure for simulation, reconstruction and analysis tasks. The JUNO distributed computing system has been built up based on DIRAC since 2018. The official Monte Carlo production has started to run in the system and PBs of massive MC data has been shared among JUNO data centers through this system since last year. In this paper, an overview of the JUNO distributed computing system will be presented, including workload management system, data management system and calibration data access system. Also the progress of adapting the system to the token-based AAI and WebDAV TPC will be reported. The paper will also describe the preparations for the coming data-taking, and how we will arrange JUNO data processing activities in this platform for data-taking.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 267,
            id: 'c16842',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11469/contribution.pdf',
            presenters: [
              {
                affiliation: 'Institute of High Energy Physics',
                displayOrderKey: [1, 'Zhang, Xiaomei'],
                emailHash: '865949cef918f0ff18b94a42afcaddf3',
                familyName: 'Zhang',
                firstName: 'Xiaomei',
                name: 'Xiaomei Zhang',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12104,
            sessionSlotId: 2675,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'JUNO distributed computing system',
            uniqueId: 'c16842',
            url: '/event/459/contributions/11469/',
          },
          c16843: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11503/attachments/9372/13772/ET-CHEP2023-1.pdf',
                  id: 13772,
                  title: 'ET-CHEP2023-1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11503,
            description:
              'The discovery of gravitational waves, first observed in September 2015 following the merger of a binary black hole system, has already revolutionised our understanding of the Universe.  This was further enhanced in August 2017, when the coalescence of a binary neutron star system was observed both with gravitational waves and a variety of electromagnetic counterparts; this joint observation marked the beginning of gravitational multi-messenger astronomy.  The Einstein Telescope, a proposed next-generation ground-based gravitational-wave observatory, will dramatically increase the sensitivity to sources: the number of observations of gravitational waves is expected to increase from roughly 100 per year to roughly 100\u2019000 per year, and signals may be visible for hours at a time, given the low frequency cutoff of the planned instrument.  This increase in the number of observed events, and the duration with which they are observed, is hugely beneficial to the scientific goals of the community, but poses a number of significant computing challenges.  Moreover, the currently used computing algorithms do not scale to this new environment, both in terms of the amount of resources required and the speed with which each signal must be characterised.\r\n\r\nThis contribution will discuss the Einstein Telescope\u0027s computing challenges, and the activities that are underway to prepare for them.  Available computing resources and technologies will greatly evolve in the years ahead, and those working to develop the Einstein Telescope data analysis algorithms will need to take this into account.  The availability of huge parallel HPC systems and ubiquitous Cloud computing will also be important to factor into the initial development of the experiment\u0027s computing model; the design of the model will also, for the first time, include the environmental impact as one of the optimisation metrics.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 291,
            id: 'c16843',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11503/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN',
                displayOrderKey: [10, 'Pardi, Silvio'],
                emailHash: '25d9c39b3bbd63706b9069a4e9bc80d5',
                familyName: 'Pardi',
                firstName: 'Silvio',
                name: 'Silvio Pardi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12104,
            sessionSlotId: 2675,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Computing Challenges for the Einstein Telescope project',
            uniqueId: 'c16843',
            url: '/event/459/contributions/11503/',
          },
          c16844: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11477/attachments/9515/13797/VIR-0425A-23_TheLigoVirgoKAGRAComputingInfr.pdf',
                  id: 13797,
                  title: 'VIR-0425A-23_TheLigoVirgoKAGRAComputingInfr.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11477,
            description:
              'The LIGO, VIRGO and KAGRA Gravitational-wave (GW) observatories are getting ready for their fourth observational period, O4, scheduled to begin in March 2023, with improved sensitivities and thus higher event rates.\r\nGW-related computing has both large commonalities with HEP computing, particularly in the domain of offline data processing and analysis, and important differences, for example in the fact that the amount of raw data doesn\u2019t grow much with the instrument sensitivity, or the need to timely generate and distribute \u201cevent candidate alerts\u201d to EM and neutrino observatories, thus making gravitational multi-messenger astronomy possible. \r\n\r\nData from the interferometers are exchanged between collaborations both for low-latency and offline processing; in recent years, the three collaborations designed and built a common distributed computing infrastructure to prepare for a growing computing demand, and to reduce the maintenance burden of legacy custom-made tools, by increasingly adopting tools and architectures originally developed in the context of HEP computing. So for example HTCondor is used for workflow management, Rucio for many data management needs, CVMFS for code and data distribution, and more.\r\n\r\nWe will present GW computing use cases and report about the architecture of the computing infrastructure as will be used during O4, as well as some planned upgrades for the subsequent observing run O5.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 378,
            id: 'c16844',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11477/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Torino',
                displayOrderKey: [0, 'Legger, Federica'],
                emailHash: 'b4b1f2e02ec55f04fdbae307a3d37024',
                familyName: 'Legger',
                firstName: 'Federica',
                name: 'Federica Legger',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12104,
            sessionSlotId: 2675,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'The Ligo-Virgo-KAGRA Computing Infrastructure for Gravitational-wave Research',
            uniqueId: 'c16844',
            url: '/event/459/contributions/11477/',
          },
          c16845: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11470/attachments/9300/13495/210508%20-%20CHEP2023%20-%20Gutsche%20-%20U.S.%20CMS%20R%26D%20Strategic%20Plan.pdf',
                  id: 13495,
                  title: '210508 - CHEP2023 - Gutsche - U.S. CMS R\u0026D Strategic Plan.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11470,
            description:
              'The HL-LHC run is anticipated to start at the end of this decade and will pose a significant challenge for the scale of the HEP software and computing infrastructure. The mission of the U.S. CMS Software \u0026 Computing Operations Program is to develop and operate the software and computing resources necessary to process CMS data expeditiously and to enable U.S. physicists to fully participate in the physics of CMS. We have developed a strategic plan to prioritize R\u0026D efforts to reach this goal for the HL-LHC. This plan includes four grand challenges: modernizing physics software and improving algorithms, building infrastructure for exabyte-scale datasets, transforming the scientific data analysis process and transitioning from R\u0026D to operations. We are involved in a variety of R\u0026D projects that fall within these grand challenges. In this talk, we will introduce our four grand challenges and outline the R\u0026D program of the U.S. CMS Software \u0026 Computing Operations Program.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 62,
            id: 'c16845',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11470/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermi National Accelerator Laboratory',
                displayOrderKey: [1, 'Gutsche, Oliver'],
                emailHash: '8fab30d54a40149b9eb951c9344261a1',
                familyName: 'Gutsche',
                firstName: 'Oliver',
                name: 'Oliver Gutsche',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12104,
            sessionSlotId: 2675,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'The U.S. CMS HL-LHC R\u0026D Strategic Plan',
            uniqueId: 'c16845',
            url: '/event/459/contributions/11470/',
          },
          c16846: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11502/attachments/9395/14242/230508_CHEP23South.pdf',
                  id: 14242,
                  title: '230508_CHEP23South.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11502,
            description:
              'The computing challenges at HL-LHC require fundamental changes to the distributed computing models that have served experiments well throughout LHC. ATLAS planning for HL-LHC computing started back in 2020 with a Conceptual Design Report outlining various challenges to explore. This was followed in 2022 by a roadmap defining concrete milestones and associated effort required. Today, ATLAS is proceeding further with a set of "demonstrators" with focussed R\u0026D in specific topics described in the roadmap. The demonstrators cover areas such as optimised tape writing and access, data recreation on-demand and the use of commercial clouds. This paper presents an overview of the demonstrators, detailing the plans, timelines and expected impact of the work.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 193,
            id: 'c16846',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11502/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'South, David'],
                emailHash: 'c3aa620021fa578a90d03b76601c59b5',
                familyName: 'South',
                firstName: 'David',
                name: 'David South',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12104,
            sessionSlotId: 2675,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'R\u0026D in ATLAS Distributed Computing towards HL-LHC',
            uniqueId: 'c16846',
            url: '/event/459/contributions/11502/',
          },
          c16847: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11465/attachments/9387/13613/CHEP2023%20Computing%20Model%20Evolution%20May2023.pdf',
                  id: 13613,
                  title: 'CHEP2023 Computing Model Evolution May2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11465,
            description:
              'In this talk, we discuss the evolution of the computing model of the ATLAS experiment at the LHC. After LHC Run 1, it became obvious that the available computing resources at the WLCG were fully used. The processing queue could reach millions of jobs during peak loads, for example before major scientific conferences and during large scale data processing. The unprecedented performance of the LHC during Run 2 and subsequent large data volumes required more computing power than the WLCG consortium pledged. In addition to unpledged/opportunistic resources available through the grid, the integration of resources such as supercomputers and cloud computing with the ATLAS distributed computing model has led to significant changes in both the workload management system and the data management system, thereby changing the computing model as a whole. The implementation of the data carousel model and data on-demand, cloud and HPC integration, and other innovations expanded the physics capabilities of experiments in the field of high energy physics and made it possible to implement bursty data simulation and processing. In the past few years ATLAS, and many other High Energy (HEP) or Nuclear Physics (NP) and Astroparticle experiments, evaluated commercial clouds as an additional part of their computing resources. In this talk, we will briefly describe the ATLAS-Google and ATLAS-Amazon projects and how they were fully integrated with the ATLAS computing model. We will try to answer a fundamental question about the future computing model for experiments with large data volumes and distributed computing resources by considering three possible options:\r\n - HEP/NP experiments will continue to own and use pledged resources\r\n - HEP/NP experiments will buy resources from commercial providers\r\n - HEP/NP experiments will own core resources and buy additional resources from commercial providers.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 415,
            id: 'c16847',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11465/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [2, 'Klimentov, Alexei'],
                emailHash: '6fef2a09accf1a707bc4c992c15a8438',
                familyName: 'Klimentov',
                firstName: 'Alexei',
                name: 'Alexei Klimentov',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12104,
            sessionSlotId: 2675,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Future Data-Intensive Experiment Computing Models: Lessons learned from the recent evolution of the ATLAS Computing Model',
            uniqueId: 'c16847',
            url: '/event/459/contributions/11465/',
          },
        },
        entryType: 'Session',
        friendlyId: 8,
        id: 's12104',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2038/session-timetable.pdf',
        room: 'Marriott Ballroom II-III',
        sessionCode: '',
        sessionId: 2038,
        sessionSlotId: 2675,
        slotTitle: 'Computing Strategies and Evolution',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 4 - Distributed Computing',
        uniqueId: 's12104',
        url: '/event/459/sessions/2038/',
      },
      s12106: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#feffbf',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Hageboeck, Stephan'],
            emailHash: '64d6d832b57aa9e7597d4011228b0da0',
            familyName: 'Hageboeck',
            firstName: 'Stephan',
            name: 'Stephan Hageboeck',
          },
          {
            affiliation: 'CNU',
            displayOrderKey: [0, 'Heddle, Dave'],
            emailHash: 'a9a8498d1e59570298c4ee6af05530e7',
            familyName: 'Heddle',
            firstName: 'Dave',
            name: 'Dave Heddle',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16917: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11595/attachments/9292/13484/CHEP2023_HDF5_Experience_In_DUNE.pdf',
                  id: 13484,
                  title: 'CHEP2023_HDF5_Experience_In_DUNE.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11595,
            description:
              'The Deep Underground Neutrino Experiment (DUNE) has historically represented data using a combination of custom data formats and those based on ROOT I/O.  Recently, DUNE has begun using the Hierarchical Data Format (HDF5) for some of its data storage applications.  HDF5 provides high-performance, low-overhead I/O in DUNE\u2019s data acquisition (DAQ) environment.  DUNE will use HDF5 to record raw data from the ProtoDUNE Horizontal Drift (HD), ProtoDUNE Vertical Drift (VD) and ICEBERG detectors, and the HD and VD coldbox test stands. Dedicated I/O modules have been developed to read the HDF5 data from these detectors into the offline framework for reconstruction directly and via XRootD. HDF5 is also very commonly used on High Performance Computers (HPCs) and is well-suited for use in AI/ML applications. The DUNE software stack contains modules that export data from an offline job in HDF5 format, so that they can be processed  by external AI/ML software.  The  collaboration is also developing strategies to incorporate HDF5 in the detector simulation chains.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 133,
            id: 'c16917',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11595/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'Chowdhury, Barnali'],
                emailHash: 'f1abd21806aaa1bb0f3d27e75cf75708',
                familyName: 'Chowdhury',
                firstName: 'Barnali',
                name: 'Dr Barnali Chowdhury',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12106,
            sessionSlotId: 2677,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'DUNE HDF5 Experience',
            uniqueId: 'c16917',
            url: '/event/459/contributions/11595/',
          },
          c16918: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11562/attachments/9296/13490/CHEP_2023_HDTree.pdf',
                  id: 13490,
                  title: 'CHEP_2023_HDTree.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11562/attachments/9296/14096/go',
                  id: 14096,
                  title: 'GitHub: tomeichlersmith/hdtree',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11562,
            description:
              'ROOT\u0027s TTree data structure has been highly successful and useful for HEP; nevertheless, alternative file formats now exist which may offer broader software tool support and more-stable in-memory interfacing. We present a data serialization library that produces a similar data structure within the HDF5 data format; supporting C++ standard collections, user-defined data types, and schema evolution of those types. This HDF5-based serialization shows improved performance compared to a similar ROOT-based serialization library when embedded into an event processing framework for a HEP experiment and opens the door to using other software that struggled to interface with the ROOT format.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 496,
            id: 'c16918',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11562/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Minnesota',
                displayOrderKey: [1, 'Eichlersmith, Tom'],
                emailHash: '0bf9029a7da4f21062e8c9b769f7f9ba',
                familyName: 'Eichlersmith',
                firstName: 'Tom',
                name: 'Tom Eichlersmith',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12106,
            sessionSlotId: 2677,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Schema-Evolution and the TTree within HDF5',
            uniqueId: 'c16918',
            url: '/event/459/contributions/11562/',
          },
          c16919: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11594/attachments/9389/13620/rntuple-chep23.pdf',
                  id: 13620,
                  title: 'rntuple-chep23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11594,
            description:
              'The RNTuple I/O subsystem is ROOT\u0027s future event data file format and access API. It is driven by the expected data volume increase at upcoming HEP experiments, e.g. at the HL-LHC, and recent opportunities in the storage hardware and software landscape such as NVMe drives and distributed object stores. RNTuple is a redesign of the TTree binary format and API and has shown to deliver substantially faster data throughput and better data compression both compared to TTree and to industry standard formats. In order to let HENP computing workflows benefit from RNTuple\u0027s superior performance, however, the I/O stack needs to connect efficiently to the rest of the ecosystem, from grid storage to (distributed) analysis frameworks to (multithreaded) experiment frameworks for reconstruction and ntuple derivation. With the RNTuple binary format arriving at version 1.0, we present RNTuple\u0027s feature set, integration efforts, and its performance impact on the time-to-solution. We show the latest performance figures of RDataFrame analysis code of realistic complexity, comparing RNTuple and TTree as data sources. We discuss RNTuple\u0027s approach to functionality critical to the HENP I/O (such as multithreaded writes, fast data merging, schema evolution) and we provide an outlook on the road to its use in production.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 444,
            id: 'c16919',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11594/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Blomer, Jakob'],
                emailHash: '771122777d10e8f8dc1c3aa7156922e8',
                familyName: 'Blomer',
                firstName: 'Jakob',
                name: 'Jakob Blomer',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12106,
            sessionSlotId: 2677,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'ROOT\u2019s RNTuple I/O Subsystem: The Path to Production',
            uniqueId: 'c16919',
            url: '/event/459/contributions/11594/',
          },
          c16920: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11559/attachments/9294/13486/CHEP23_Integration_of_RNTuple_in_ATLAS.pdf',
                  id: 13486,
                  title: 'CHEP23_Integration_of_RNTuple_in_ATLAS.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11559,
            description:
              'After using ROOT TTree for over two decades and storing more than an exabyte of compressed data, advances in technology have motivated a complete redesign, RNTuple, that breaks backward-compatibility to take better advantage of these storage options. The RNTuple I/O subsystem has been designed to address performance bottlenecks and shortcomings of ROOT\u0027s current state of the art TTree I/O subsystem. Specifically, it comes with an updated, more compact binary data format, that can be stored both in ROOT files and natively in object stores, on performance engineering for modern storage hardware (e.g. high-throughput low-latency NVMe SSDs), and robust and easy to use interfaces.\r\nRNTuple is scheduled to become production grade in 2024; recently it became mature enough to start exploring the integration into experiments\u0027 software. In particular, in this contribution we analyze the challenges and discuss their solutions on the way to supporting the ATLAS Analysis Event Data Model (based on xAOD data format) in Athena, part of the software stack for the ATLAS experiment.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 435,
            id: 'c16920',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11559/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Amsterdam',
                displayOrderKey: [0, 'De Geus, Florine'],
                emailHash: 'af86a4ac45efe6f6cff91ad4eb5006d5',
                familyName: 'De Geus',
                firstName: 'Florine',
                name: 'Mrs Florine De Geus',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12106,
            sessionSlotId: 2677,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Integration of RNTuple in ATLAS Athena',
            uniqueId: 'c16920',
            url: '/event/459/contributions/11559/',
          },
          c16921: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11571/attachments/9442/13691/2023-CHEP-ROOT-IO.pdf',
                  id: 13691,
                  title: '2023-CHEP-ROOT-IO.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11571/attachments/9442/13690/2023-CHEP-ROOT-IO.pptx',
                  id: 13690,
                  title: '2023-CHEP-ROOT-IO.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11571,
            description:
              'Analysis performance has a significant impact on the productivity of physicists. The vast majority of analyses use ROOT (https://root.cern). For a few years now, ROOT has offered an analysis interface called RDataFrame which helps getting the best performance for analyses, ideally making them I/O limited, i.e. with their performance limited by the throughput of reading the input data.\r\n \r\nThe CERN IT department has recently noted (https://doi.org/10.5281/zenodo.6337728) that for the analysis activities (that they heuristically identified as such) there was no apparent performance CPU nor I/O bottleneck as seen from their point of view.   We will report on our investigation in collaboration with USCMS and the CERN IT department to understand better where the inefficiencies that gave rise to this situation come from and the improvements that were made in ROOT to significantly reduce those inefficiencies.   We will also describe additional logging and tagging facilities introduced to help distinguish the type of workload and help correlate the information gathered on the server side with the activities carried out by the users\u2019 analysis.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 134,
            id: 'c16921',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11571/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Canal, Philippe'],
                emailHash: '6d86e8b49397c11a44f4a1d750a5d65a',
                familyName: 'Canal',
                firstName: 'Philippe',
                name: 'Philippe Canal',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12106,
            sessionSlotId: 2677,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Improving ROOT I/O Performance for Analysis',
            uniqueId: 'c16921',
            url: '/event/459/contributions/11571/',
          },
          c16922: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11565/attachments/9397/13628/Bulk%20RDF%20%40%20CHEP%202023-1.pdf',
                  id: 13628,
                  title: 'Bulk RDF @ CHEP 2023-1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11565,
            description:
              'RDataFrame is ROOT\u0027s high-level interface for Python and C++ data analysis. Since it first became available, RDataFrame adoption has grown steadily and it is now poised to be a major component of analysis software pipelines for LHC Run 3 and beyond. Thanks to its design inspired by declarative programming principles, RDataFrame enables the development of high-performance, highly parallel analyses without requiring expert knowledge of multi-threading and I/O: user logic is expressed in terms of self-contained, small computation kernels tied together via a high-level API. This design completely decouples analysis logic from its actual execution, and opens several interesting avenues for workflow optimization. In particular, in this work we explore the benefits of moving internal data processing from an event-by-event to a bulk-by-bulk loop: it dramatically reduces framework\u0027s performance overheads; in collaboration with the I/O layer it improves data access patterns; it exposes information that optimizing compilers might use to auto-vectorize the invocation of user-defined computations; finally, while existing user-facing interfaces remain unaffected, it becomes possible to additionally offer interfaces that explicitly expose bulks of events, useful e.g. for the injection of GPU kernels into the analysis workflow. Design challenges useful to inform future R\u0026D will be presented, as well as an investigation of the relevant time-memory tradeoffs backed by novel performance benchmarks.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 149,
            id: 'c16922',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11565/contribution.pdf',
            presenters: [
              {
                affiliation: 'EP-SFT, CERN',
                displayOrderKey: [1, 'Guiraud, Enrico'],
                emailHash: '4bc18e3ffc402883aac55946584510d6',
                familyName: 'Guiraud',
                firstName: 'Enrico',
                name: 'Enrico Guiraud',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12106,
            sessionSlotId: 2677,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Boosting RDataFrame performance with transparent bulk event processing',
            uniqueId: 'c16922',
            url: '/event/459/contributions/11565/',
          },
        },
        entryType: 'Session',
        friendlyId: 10,
        id: 's12106',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2040/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VIII',
        sessionCode: '',
        sessionId: 2040,
        sessionSlotId: 2677,
        slotTitle: 'I/O and Data Formats',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#1f1f02',
        title: 'Track 6 - Physics Analysis Tools',
        uniqueId: 's12106',
        url: '/event/459/sessions/2040/',
      },
      s12107: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#1a3f14',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Nebraska-Lincoln',
            displayOrderKey: [0, 'Weitzel, Derek'],
            emailHash: 'ca0d239ab6ae0ec500eff8eb24779855',
            familyName: 'Weitzel',
            firstName: 'Derek',
            name: 'Derek Weitzel',
          },
          {
            affiliation: 'KEK',
            displayOrderKey: [0, 'Kishimoto, Tomoe'],
            emailHash: '1c2a9b3c698c4de9040fbe367da6851b',
            familyName: 'Kishimoto',
            firstName: 'Tomoe',
            name: 'Tomoe Kishimoto',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17088: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11642/attachments/9204/13565/LSST_RSP_at_CHEP23_MAINETTI.pdf',
                  id: 13565,
                  title: 'LSST_RSP_at_CHEP23_MAINETTI.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11642,
            description:
              'The Vera C. Rubin observatory is preparing for the execution of the most ambitious astronomical survey ever attempted, the Legacy Survey of Space and Time (LSST). Currently, in its final phase of construction in the Andes mountains in Chile and due to start operations in late 2024 for 10 years, its 8.4-meter telescope will nightly scan the southern sky and collect images of the entire visible sky every 4 nights using a 3.2 Gigapixel camera, the largest imaging device ever built for astronomy. Automated detection and classification of celestial objects will be performed by sophisticated algorithms on high-resolution images to progressively produce an astronomical catalog eventually composed of 20 billion galaxies and 17 billion stars and their associated physical properties.\r\n\r\nIn this contribution, we will briefly present the infrastructure deployed at the French Rubin data facility (operated by the IN2P3 computing center) to deploy the Rubin Science Platform, a set of web-based services to provide effective and convenient access to LSST data for scientific analysis.\r\n\r\nWe will describe the main services of the platform, the components that provide those services and our deployment model as well as some feedback collected by end users. We will also present the Kubernetes-based infrastructure we are experimenting with for hosting LSST astronomical catalog, a multi-petabyte relational database developed for the specific needs of the project.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 567,
            id: 'c17088',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11642/contribution.pdf',
            presenters: [
              {
                affiliation: 'CC-IN2P3/CNRS',
                displayOrderKey: [0, 'Mainetti, Gabriele'],
                emailHash: '1b15c6e591d20ca6e19b37946d6876f3',
                familyName: 'Mainetti',
                firstName: 'Gabriele',
                name: 'Gabriele Mainetti',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12107,
            sessionSlotId: 2678,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Experience deploying an analysis facility for the Rubin Observatory\u2019s Legacy Survey of Space and Time (LSST) data',
            uniqueId: 'c17088',
            url: '/event/459/contributions/11642/',
          },
          c17089: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11627/attachments/9205/13364/CMS_Spanish_AF_CHEP_2023.pdf',
                  id: 13364,
                  title: 'CMS_Spanish_AF_CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11627,
            description:
              'The increasingly larger data volumes that the LHC experiments will accumulate in the coming years, especially in the High-Luminosity LHC era, call for a paradigm shift in the way experimental datasets are accessed and analyzed. The current model, based on data reduction on the Grid infrastructure, followed by interactive data analysis of manageable size samples on the physicists\u2019 individual computers, will be superseded by the adoption of Analysis Facilities. This rapidly evolving concept is converging to include dedicated hardware infrastructures and computing services optimized for the effective analysis of large HEP data samples. This contribution will describe the actual implementation of this new analysis facility model at the CIEMAT institute, in Spain, to support the local CMS experiment community. Our presentation will report on the deployment of dedicated highly-performant hardware, the operation of data staging and caching services, that ensure prompt and efficient access to CMS physics analysis datasets, and the integration and optimization of a custom analysis framework, based on ROOT\u0027s RDataFrame and CMS NanoAOD format. Finally, performance results obtained by benchmarking the deployed infrastructure and software against a full CMS reference analysis workflow will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 157,
            id: 'c17089',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11627/contribution.pdf',
            presenters: [
              {
                affiliation: 'CIEMAT - PIC',
                displayOrderKey: [7, 'P\u00e9rez-Calero Yzquierdo, Antonio'],
                emailHash: '01a84001ee4bc4352b98e880033d1fe7',
                familyName: 'P\u00e9rez-Calero Yzquierdo',
                firstName: 'Antonio',
                name: 'Dr Antonio P\u00e9rez-Calero Yzquierdo',
              },
              {
                affiliation: 'CIEMAT - PIC',
                displayOrderKey: [6, 'Delgado Peris, Antonio'],
                emailHash: '2bba65732ab3dedc873b55c5dc858b51',
                familyName: 'Delgado Peris',
                firstName: 'Antonio',
                name: 'Dr Antonio Delgado Peris',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12107,
            sessionSlotId: 2678,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'The Spanish CMS Analysis Facility at CIEMAT',
            uniqueId: 'c17089',
            url: '/event/459/contributions/11627/',
          },
          c17090: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11616/attachments/9447/13697/US%20ATLAS%20Analysis%20Facilities%20-%20CHEP%202023.pdf',
                  id: 13697,
                  title: 'US ATLAS Analysis Facilities - CHEP 2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11616,
            description:
              'Prior to the start of the LHC Run 3, the US ATLAS Software and Computing operations program established three shared Tier 3 Analysis Facilities (AFs). The newest AF was established at the University of Chicago in the past year, joining the existing AFs at Brookhaven National Lab and SLAC National Accelerator Lab. In this paper, we will describe both the common and unique aspects of these three AFs, and the resulting distributed facility from the user\u2019s perspective, including how we monitor and measure the AFs. The common elements include enabling easy access via Federated ID, file sharing via EOS, provisioning of similar Jupyter environments using common Jupyter kernels and containerization, and efforts to centralize documentation and user support channels. The unique components we will cover are driven in turn by the requirements, expertise and resources at each individual site. Finally, we will highlight how the US AFs are collaborating with other ATLAS and LHC wide (IRIS-HEP and HSF) user analysis support activities, evaluating tools like ServiceX and new file formats such as DAOD PHYSLITE.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 354,
            id: 'c17090',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11616/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [3, 'Rind, Ofer'],
                emailHash: '56bd8f0cd3d30362f6ce23aeaea280f2',
                familyName: 'Rind',
                firstName: 'Ofer',
                name: 'Ofer Rind',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12107,
            sessionSlotId: 2678,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'The Creation and Evolution of the US ATLAS Shared Analysis Facilities',
            uniqueId: 'c17090',
            url: '/event/459/contributions/11616/',
          },
          c17091: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11608/attachments/9267/13439/CHEP%202023%20-%20Towards%20Ten%20Minute%20Analysis.pdf',
                  id: 13439,
                  title: 'CHEP 2023 - Towards Ten Minute Analysis.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11608,
            description:
              'Effective analysis computing requires rapid turnaround times in order to enable frequent iteration, adjustment, and exploration, leading to discovery.  An informal goal of reducing 10TB of experimental data in about ten minutes using campus-scale computing infrastructure is an achievable goal, just considering raw hardware capability.  However, compared to production computing, which seeks to maximize throughput at a massive scale over the timescale of weeks and months, analysis computing requires different optimizations in terms of startup latency, data locality, scalability limits, and long-tail behavior.  At Notre Dame, we have developed substantial experience with running scalable analysis codes on campus infrastructure on a daily basis.  Using the TopEFT application, based on the Coffea data analysis framework and the Work Queue distributed executor, we reliably process 2TB of data, 375 CPU-hours analysis codes to completion in about one hour on hundreds of nodes, albeit with a high variability due to competing system loads.  The python environment needed in the compute nodes is setup and cached on the fly if needed (300MB as tarball sent to worker nodes, 1GB unpacked).  In this talk, we present our analysis of the performance limits of the current system, taking into account software dependencies, data access, result generation, and fault tolerance.  We present our plans for attacking the ten minute goal through a combination of hardware evolution, improved storage management, and application scheduling.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 87,
            id: 'c17091',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11608/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Notre Dame',
                displayOrderKey: [4, 'Lawrence, John'],
                emailHash: 'e83218839c5f386e791b6d02e1cdf40d',
                familyName: 'Lawrence',
                firstName: 'John',
                name: 'John Lawrence',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12107,
            sessionSlotId: 2678,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Toward Ten-Minute Turnaround in CMS Data Analysis: The View from Notre Dame',
            uniqueId: 'c17091',
            url: '/event/459/contributions/11608/',
          },
          c17092: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11613/attachments/9378/13602/IO%20perf%20studies%20CHEP2023.pdf',
                  id: 13602,
                  title: 'IO perf studies CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11613,
            description:
              'The recent evolutions of the analysis frameworks and physics data formats of the LHC experiments provide the opportunity of using central analysis facilities with a strong focus on interactivity and short turnaround times, to complement the more common distributed analysis on the Grid. In order to plan for such facilities, it is essential to know in detail the performance of the combination of a given analysis framework, of a specific analysis and of the installed computing and storage resources. This contribution describes performance studies performed at CERN, using the EOS disk-based storage, either directly or through an XCache instance, from both batch resources and high-performance compute nodes which could be used to build an analysis facility. A variety of benchmarks, both synthetic and based on real-world physics analyses and their corresponding input datasets, are utilized. In particular, the RNTuple format from the ROOT project is put to the test and compared to the latest version of the TTree format, and the impact of caches is assessed. In addition, we assessed the difference in performance between the use of storage system specific protocols, like XRootd, and FUSE. The results of this study are intended to be a valuable input in the design of analysis facilities, at CERN and elsewhere.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 118,
            id: 'c17092',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11613/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Sciab\u00e0, Andrea'],
                emailHash: 'cbc73225e092c0fa43663ce9496fd7cc',
                familyName: 'Sciab\u00e0',
                firstName: 'Andrea',
                name: 'Andrea Sciab\u00e0',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12107,
            sessionSlotId: 2678,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'I/O performance studies of analysis workloads on production and dedicated resources at CERN',
            uniqueId: 'c17092',
            url: '/event/459/contributions/11613/',
          },
          c17253: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11663/attachments/9201/13360/CHEP2023.pdf',
                  id: 13360,
                  title: 'CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11663,
            description:
              'The INFN Cloud project was launched at the beginning of 2020, aiming to build a distributed Cloud infrastructure and provide advanced services for the INFN scientific communities. A Platform as a Service (PaaS) was created inside INFN Cloud that allows the experiments to develop and access resources as a Software as a Service (SaaS), and CYGNO is the beta-tester of this system. The aim of the CYGNO experiment is to realize a large gaseous Time Projection Chamber based on the optical readout of the photons produced in the avalanche multiplication of ionization electrons in a GEM stack. To this extent, CYGNO exploits the progress in commercial scientific Active Pixel Sensors based on Scientific CMOS for Dark Matter search and Solar Neutrino studies. CYGNO, like many other astroparticle experiments, requires a computing model to acquire, store, simulate and analyze data typically far from High Energy Physics (HEP) experiments. Indeed, astroparticle experiments are typically characterized by the fact to be less demanding from computing resources with respect to HEP one but have to deal with unique and unrepeatable data, sometimes collected in extreme conditions, with extensive use of templates and Montecarlo, and are often re-calibrated and reconstructed many times for a given data sets. Moreover, the varieties and the scale of computing models and requirements are extremely large. In this scenario, the Cloud infrastructure with standardized and optimized services offered to the scientific community could be a useful solution able to match the requirements of many small/medium size experiments. In this work, we will present the CYGNO computing model based on the INFN cloud infrastructure where the experiment software, easily extendible to similar experiments to similar applications on other similar experiments, provides tools as a service to store, archive, analyze, and simulate data.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 534,
            id: 'c17253',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11663/contribution.pdf',
            presenters: [
              {
                affiliation: 'Laboratori Nazionali di Frascati',
                displayOrderKey: [2, 'Mazzitelli, Giovanni'],
                emailHash: '53dfbcb87232c6238f703da50b6e758e',
                familyName: 'Mazzitelli',
                firstName: 'Giovanni',
                name: 'Dr Giovanni Mazzitelli',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12107,
            sessionSlotId: 2678,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Data handling of CYGNO experiment using INFN-Cloud solution',
            uniqueId: 'c17253',
            url: '/event/459/contributions/11663/',
          },
        },
        entryType: 'Session',
        friendlyId: 11,
        id: 's12107',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2041/session-timetable.pdf',
        room: 'Marriott Ballroom IV',
        sessionCode: '',
        sessionId: 2041,
        sessionSlotId: 2678,
        slotTitle: 'Analysis Facilities',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#f1ffef',
        title: 'Track 7 - Facilities and Virtualization',
        uniqueId: 's12107',
        url: '/event/459/sessions/2041/',
      },
      s12108: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#e3f2d3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'DESY',
            displayOrderKey: [0, 'Hernandez Villanueva, Michel'],
            emailHash: '19113800d40b0ae945d3d902953e2b1c',
            familyName: 'Hernandez Villanueva',
            firstName: 'Michel',
            name: 'Michel Hernandez Villanueva',
          },
          {
            affiliation: 'Paul Scherrer Institut',
            displayOrderKey: [0, 'Lange, Clemens'],
            emailHash: '429cdad04fa5f47ba2f6993b0b32ec69',
            familyName: 'Lange',
            firstName: 'Clemens',
            name: 'Clemens Lange',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17129: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11692/attachments/9446/13696/ekauffma_chep2023_pdfversion.pdf',
                  id: 13696,
                  title: 'ekauffma_chep2023_pdfversion.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11692/attachments/9446/13695/ekauffma_chep2023.pptx',
                  id: 13695,
                  title: 'ekauffma_chep2023.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11692,
            description:
              'Machine learning (ML) has become an integral component of high energy physics data analyses and is likely to continue to grow in prevalence. Physicists are incorporating ML into many aspects of analysis, from using boosted decision trees to classify particle jets to using unsupervised learning to search for physics beyond the Standard Model. Since ML methods have become so widespread in analysis and these analyses need to be scaled up for HL-LHC data, neatly integrating ML training and inference into scalable analysis workflows will improve the user experience of analysis in the HL-LHC era.\r\n\r\nWe present the integration of ML training and inference into the IRIS-HEP Analysis Grand Challenge (AGC) pipeline to provide an example of how this integration can look like in a realistic analysis environment. We also utilize Open Data to ensure the project\u2019s reach to the broader community. Different approaches for performing ML inference at analysis facilities are investigated and compared, including performing inference through external servers. Since ML techniques are applied for many different types of tasks in physics analyses, we showcase options for ML integration that can be applied to various inference needs.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 571,
            id: 'c17129',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11692/contribution.pdf',
            presenters: [
              {
                affiliation: 'Princeton University',
                displayOrderKey: [1, 'Kauffman, Elliott'],
                emailHash: '15d6d2ded160f7e5873b88203424f91b',
                familyName: 'Kauffman',
                firstName: 'Elliott',
                name: 'Elliott Kauffman',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12108,
            sessionSlotId: 2679,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Machine Learning for Columnar High Energy Physics Analysis',
            uniqueId: 'c17129',
            url: '/event/459/contributions/11692/',
          },
          c17130: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11670/attachments/9262/13434/The%20ML_INFN%20Initiative.pdf',
                  id: 13434,
                  title: 'The ML_INFN Initiative.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11670,
            description:
              'The ML_INFN initiative (\u201cMachine Learning at INFN\u201d) is an effort to foster Machine Learning activities at the Italian National Institute for Nuclear Physics (INFN).\r\n\r\nIn recent years, AI inspired activities have flourished bottom-up in many efforts in Physics, both at the experimental and theoretical level.\r\n\r\nMany researchers have procured desktop-level devices, with consumer oriented GPUs, and have trained themselves in a variety of ways, from webinars, books, tutorials.\r\n\r\nML_INFN aims to help and systematize such effort, in multiple ways: by offering state-of-the art hardware for Machine Learning, leveraging on the INFN-Cloud provisioning solutions and thus sharing more efficiently GPU-like resources and leveling the access to such resources to all INFN researchers, and by organizing and curating Knowledge Bases with production grade examples from successful activities already in production.\r\n\r\nMoreover, training events have been organized for beginners, based on existing INFN ML research and focussed on flattening the learning curve.\r\n\r\nIn this contribution we will update the status of the project reporting in particular on the development of tools to take advantage of High-Performance computing resources provisioned by CINECA for interactive and batch support to machine learning activities and on the organization of the first in-person advanced-level training event, with a GPU-equipped cloud-based envioronment provided to each participant.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 104,
            id: 'c17130',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11670/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Bologna',
                displayOrderKey: [5, 'Giommi, Luca'],
                emailHash: 'ff1dc0210e633470aca38fb0110d8e24',
                familyName: 'Giommi',
                firstName: 'Luca',
                name: 'Luca Giommi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12108,
            sessionSlotId: 2679,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'ML_INFN project: status report and future perspectives',
            uniqueId: 'c17130',
            url: '/event/459/contributions/11670/',
          },
          c17131: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11676/attachments/9470/13728/sciencemesh-cs3-2023-hugo.pdf',
                  id: 13728,
                  title: 'sciencemesh-cs3-2023-hugo.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11676,
            description:
              'Over the last few years, Cloud Sync\u0026Share platforms have become go-to services for collaboration in scientific, academic and research environments, providing users with coherent and simple ways to access their data assets. Collaboration within those platforms, between local users on local applications, has been demonstrated in various settings, with visible improvements in the research production process. However, extending this paradigm beyond the borders of local and regional services has shown to be a challenge.\r\n\r\nThe EU-funded CS3MESH4EOSC Project aims to address that problem, by providing a bridge between local data and applications, and remote users, in what can be described as a "mesh" of interconnected services. The ScienceMesh, a pan-European federated network of interoperable services, is the main outcome of this Project.\r\n\r\nWe will present the architecture of the ScienceMesh and how it can be leveraged to extend local functionalities to remote users in other institutions, extending HEP beyond national frontiers and boosting cross-disciplinary research. We will then explain its technical foundations, from the APIs and protocols used in its design to the workflows which underlie its operations. We will also discuss the challenges faced during the implementation of the project, especially the integration with the two major open-source Sync\u0026Share products on the market (ownCloud and Nextcloud). We will finish by explaining how we plan to extend the ScienceMesh to other areas and geographies.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 210,
            id: 'c17131',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11676/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Gonzalez, Hugo'],
                emailHash: '6de52bf63fb628b76ec546b7b6d0df21',
                familyName: 'Gonzalez',
                firstName: 'Hugo',
                name: 'Hugo Gonzalez',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12108,
            sessionSlotId: 2679,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'ScienceMesh: pan-European collaboration fabric for Science',
            uniqueId: 'c17131',
            url: '/event/459/contributions/11676/',
          },
          c17132: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11673/attachments/9464/13720/2023-05-08-chep2023-zenodo-10years.pdf',
                  id: 13720,
                  title: '2023-05-08-chep2023-zenodo-10years.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11673,
            description:
              'Zenodo has over the past 10 years grown from a proof of concept to being the world\u0027s largest general-purpose research repository, cementing CERN\u2019s image as a pioneer and leader in Open Science. We will review key challenges faced over the past 10 years and how we overcame them, from getting off the ground, over building trust to securing funding. \r\n\r\nGrowing Zenodo was an enriching and learning experience on how CERN technology can be put at the service of everyone across all research disciplines. We will show how Zenodo helped shape Open Science as we know it and became an essential component of the future European and global Open Science infrastructure.\r\n\r\nAfter 10 successful years, we are getting ready for the next decade. Zenodo is going through a transformation that will make CERN technology even more reachable and useful to the world-wide research community. We will show a glimpse of what\u2019s coming and look ahead to key challenges such as governance, data publishing ethics and how Zenodo can help continue to grow and support the adoption of open science practices, not only within physics but also across the globe\u2019s research communities.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 213,
            id: 'c17132',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11673/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Nielsen, Lars Holm'],
                emailHash: 'bbc951080061fc48cae0279d27f3c015',
                familyName: 'Nielsen',
                firstName: 'Lars Holm',
                name: 'Lars Holm Nielsen',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12108,
            sessionSlotId: 2679,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title:
              '10 years of Zenodo \u2013 growing the world\u2019s largest general-purpose scholarly repository',
            uniqueId: 'c17132',
            url: '/event/459/contributions/11673/',
          },
          c17133: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11687/attachments/9279/13453/The%20ALICE%20Service%20Work%20System.pdf',
                  id: 13453,
                  title: 'The ALICE Service Work System.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11687,
            description:
              'The "A Large Ion Collider Experiment" (ALICE), one of the four large experiments at the European Organization for Nuclear Research (CERN), is responsible for studying the physics of strongly interacting matter and the quark-gluon plasma.\r\nIn order to ensure the full success of ALICE operation and data taking during the Large Hadron Collider Runs 3 and 4, a list of tasks identified as Service Work is established and maintained, which concerns detector maintenance, operation, calibration, quality control, data processing and outreach, as well as coordination and managerial roles in ALICE.\r\nThe ALICE Glance Service Work system is a tool developed in a cooperation between the Federal University of Rio de Janeiro and the ALICE Collaboration that serves as the link between the user interaction and thousands of database entries. In this poster we describe the development process of this system and its functionalities that ranges from planning the entire year of work for hundred of tasks to individually assigning these tasks to members of the collaboration.\r\nThe system is divided into two distinct environments that communicates to generate a service. The first is a REST API written in modern PHP with its source code composed of bounded contexts following the Domain Driven Design paradigm, making the code very adaptable to different interfaces, be it a HTTP controller, in our case the Slim framework or a CLI command. The second is a responsive and clean web page made with the Vue.js framework, responsible to communicate with the user and to provide them with the means to make the requests to the API.\r\nOur database versioning is managed by the PHP Doctrine library, making it easy to semi-automatic deploy our development changes to production.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 492,
            id: 'c17133',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11687/contribution.pdf',
            presenters: [
              {
                affiliation: 'Universidade Federal do Rio De Janeiro, COPPE/EE/IF, Brazil',
                displayOrderKey: [1, 'De Souza Pereira, Jomar Junior'],
                emailHash: '00bd711de97d1c58c40626694af2e4ae',
                familyName: 'De Souza Pereira',
                firstName: 'Jomar Junior',
                name: 'Jomar Junior De Souza Pereira',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12108,
            sessionSlotId: 2679,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'The ALICE Glance Service Work system',
            uniqueId: 'c17133',
            url: '/event/459/contributions/11687/',
          },
          c17134: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11688/attachments/9304/13675/feickert_software-citation_2023-05-07.pdf',
                  id: 13675,
                  title: 'feickert_software-citation_2023-05-07.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11688/attachments/9304/13500/go',
                  id: 13500,
                  title:
                    'talk: Software Citation in HEP: Current State and Recommendations for the Future',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11688/attachments/9304/14196/go',
                  id: 14196,
                  title: 'Zenodo archive',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11688,
            description:
              'In November 2022, the HEP Software Foundation (HSF) and the Institute for Research and Innovation for Software in High-Energy Physics (IRIS-HEP) organized a workshop on the topic of \u201cSoftware Citation and Recognition in HEP\u201d. The goal of the workshop was to bring together different types of stakeholders whose roles relate to software citation and the associated credit it provides, in order to engage the community in a discussion of: 1) the ways in which HEP experiments handle citation of software; 2) recognition for software efforts that enable physics results disseminated to the public; and 3) how the scholarly publishing ecosystem supports these activities. We heard from the publication board leadership of ATLAS, CMS, and LHCb in order to understand the current practice of these experiments; various open source community organizations (ROOT, Scikit-HEP, MCnet) discussed how they prefer their software to be cited; talks from publishers (Elsevier, JOSS) recognized the issue and showed an openness to following the wishes of the community; and discussions with tool providers (INSPIRE, Zenodo) covered new standards and tools for citation. The workshop made a number of tensions clear, for example between citations being used for credit and for reproducibility, and between supporting the immediate (and possibly contradictory) desires of software producers that lead to credit in today\u0027s culture and actions that might positively change the culture to better recognize the work of these developers. This talk will present highlights from the workshop as well as findings and recommendations documented in the workshop report.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 543,
            id: 'c17134',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11688/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Wisconsin-Madison',
                displayOrderKey: [0, 'Feickert, Matthew'],
                emailHash: 'fef688a25ce02bfe5af6a307d21f90e9',
                familyName: 'Feickert',
                firstName: 'Matthew',
                name: 'Matthew Feickert',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12108,
            sessionSlotId: 2679,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Software Citation in HEP: Current State and Recommendations for the Future',
            uniqueId: 'c17134',
            url: '/event/459/contributions/11688/',
          },
        },
        entryType: 'Session',
        friendlyId: 12,
        id: 's12108',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2042/session-timetable.pdf',
        room: 'Marriott Ballroom I',
        sessionCode: '',
        sessionId: 2042,
        sessionSlotId: 2679,
        slotTitle: 'Open Science Project Reports',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#253f08',
        title: 'Track 8 - Collaboration, Reinterpretation, Outreach and Education',
        uniqueId: 's12108',
        url: '/event/459/sessions/2042/',
      },
      s12109: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#92b6db',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'INFN-CNAF',
            displayOrderKey: [0, 'Dal Pra, Stefano'],
            emailHash: '4bb37fa7bb6103869e0d7cdb7c104ea6',
            familyName: 'Dal Pra',
            firstName: 'Stefano',
            name: 'Stefano Dal Pra',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Wenzel, Sandro'],
            emailHash: '7e8bcc2855b51c4e93b19e095a8b2ec9',
            familyName: 'Wenzel',
            firstName: 'Sandro',
            name: 'Sandro Wenzel',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17164: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11724/attachments/9419/13661/chep_pypwa_2023_phelps_v0.pdf',
                  id: 13661,
                  title: 'chep_pypwa_2023_phelps_v0.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11724,
            description:
              'Abstract\r\n\r\nWe present results on Deep Learning applied to Amplitude and Partial Wave Analysis (PWA) for spectroscopic analyses. Experiments in spectroscopy often aim to observe strongly-interacting, short-lived particles that decay to multi-particle final states. These particle decays have angular distributions that our deep learning model has been trained to identify. Working with TensorFlow and Keras libraries we have developed several neural network architectures that will be presented. One architecture that will be highlighted is our \u201cHybrid\u201d Autoencoder (AE) architecture that has the best performance by far as it is able to resolve ambiguities. This AE is an unsupervised regressor that constrains the latent space variables to represent physically relevant quantities such as production amplitudes. As the training needs to be performed in a large amount of simulated data, a novel on-the-fly generation techniques is also used. Results of performed mass-independent and mass-dependent amplitude analyses using this technique will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 251,
            id: 'c17164',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11724/contribution.pdf',
            presenters: [
              {
                affiliation: 'Christopher Newport University and Jefferson Lab',
                displayOrderKey: [1, 'Phelps, William'],
                emailHash: '4205b726aa85ea214a3cd0f6befdf7cf',
                familyName: 'Phelps',
                firstName: 'William',
                name: 'Dr William Phelps',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12109,
            sessionSlotId: 2680,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Deep Learning for Amplitude Analysis in Spectroscopy',
            uniqueId: 'c17164',
            url: '/event/459/contributions/11724/',
          },
          c17166: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11723/attachments/9295/13647/Baler_v2.pdf',
                  id: 13647,
                  title: 'Baler_v2.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11723/attachments/9295/13648/Baler_v2.pptx',
                  id: 13648,
                  title: 'Baler_v2.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11723,
            description:
              'One common issue in vastly different fields of research and industry is the ever-increasing need for more data storage. With experiments taking more complex data at higher rates, the data recorded is quickly outgrowing the storage capabilities. This issue is very prominent in LHC experiments such as ATLAS where in five years the resources needed are expected to be many times larger than the storage available (assuming a flat budget model and current technology trends) [1]. Since the data formats used are already highly compressed, storage constraints could require more drastic measures such as lossy compression, where some data accuracy is lost during the compression process.\r\n\r\nIn our work, following from a number of undergraduate projects [2,3,4,5,6,7], we have developed an interdisciplinary open-source tool for machine learning-based lossy compression. The tool utilizes an autoencoder neural network, which is trained to compress and decompress data based on correlations between the different variables in the dataset. The process is lossy, meaning that the original data values and distributions cannot be reconstructed precisely. However, for certain variables and observables where the precision loss is tolerable, the high compression ratio allows for more data to be stored yielding greater statistical power.\r\n\r\n[1] - https://cerncourier.com/a/time-to-adapt-for-big-data/\r\n[2] - http://lup.lub.lu.se/student-papers/record/9049610 \r\n[3] - http://lup.lub.lu.se/student-papers/record/9012882\r\n[4] - http://lup.lub.lu.se/student-papers/record/9004751 \r\n[5]\u00a0-  http://lup.lub.lu.se/student-papers/record/9075881 \r\n[6] - https://zenodo.org/record/5482611#.Y3Yysy2l3Jz\r\n[7] - https://zenodo.org/record/4012511#.Y3Yyny2l3Jz',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 376,
            id: 'c17166',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11723/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lund University (SE)',
                displayOrderKey: [1, 'Gall\u00e9n, Axel'],
                emailHash: 'ae51faf3fe7b8dd231d5c022599a4585',
                familyName: 'Gall\u00e9n',
                firstName: 'Axel',
                name: 'Axel Gall\u00e9n',
              },
              {
                affiliation: 'Lund University (SE)',
                displayOrderKey: [2, 'Ekman, Alexander'],
                emailHash: 'af13448ed394f88c063c30233087c3a3',
                familyName: 'Ekman',
                firstName: 'Alexander',
                name: 'Alexander Ekman',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12109,
            sessionSlotId: 2680,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Machine learning based compression for scientific data',
            uniqueId: 'c17166',
            url: '/event/459/contributions/11723/',
          },
          c17167: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11728/attachments/9452/13841/QCNN-v2.pdf',
                  id: 13841,
                  title: 'QCNN-v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11728,
            description:
              'The Super Tau Charm Facility (STCF) proposed in China is a new-generation electron\u2013positron collider with center-of-mass energies covering 2-7 GeV. In STCF, the discrimination of high momentum hadrons is a challenging and critical task for various physics studies. In recent years, machine learning methods have gradually become one of the mainstream methods in the PID field of high energy physics experiments, with the advantage of big data processing.  \r\nIn this work, targeting at the pion/kaon identification problem at STCF, we have developed a convolutional neural network (CNN) in the endcap PID system, which is a time-of-flight detector based on detection of internally reflected Cherenkov light (DTOF). By combining the hit position and arrival time of each Cherenkov photon at multi-anode microchannel plate photomultipliers, a two dimensional pixel map is constructed as the CNN input. The preliminary results show that the CNN model has a promising performance against the pion/kaon identification problem. In addition, based on the traditional CNN, a quantum convolution neural network (QCNN) is developed as well, as a proof-of-concept work exploring possible quantum advantages provided by quantum machine learning methods.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 439,
            id: 'c17167',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11728/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Li, Teng'],
                emailHash: 'c6c302428ff4c73ceb3723fea91da1c6',
                familyName: 'Li',
                firstName: 'Teng',
                name: 'Teng Li',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12109,
            sessionSlotId: 2680,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Pion/Kaon Identification at STCF DTOF Based on Classical/Quantum Convolutional Neural Network',
            uniqueId: 'c17167',
            url: '/event/459/contributions/11728/',
          },
          c17168: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11734/attachments/9290/13481/Kabus%20CHEP%20May%202023.pdf',
                  id: 13481,
                  title: 'Kabus CHEP May 2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11734,
            description:
              'The main focus of the ALICE experiment, quark-gluon plasma measurements, requires\r\naccurate particle identification (PID). The ALICE detectors allow identifying particles over a broad momentum interval ranging from about 100 MeV/c up to 20 GeV/c.\r\n\r\nHowever, hand-crafted selections and the Bayesian method do not perform well in the\r\nregions where the particle signals overlap. Moreover, an ML model can explore more\r\ndetector information. During LHC Run 2, preliminary studies with Random Forests obtained much higher efficiencies and purities for selected particles than standard techniques.\r\n\r\nFor Run 3, we investigate Domain Adaptation Neural Networks that account for the\r\ndiscrepancies between the Monte Carlo simulations and the experimental data. Preliminary studies show that domain adaptation improves particle classification. Moreover, the solution is extended with Feature Set Embedding to give the network more flexibility to train on data with various sets of detector signals.\r\n\r\nPID ML is already integrated with the ALICE Run 3 Analysis Framework. Preliminary results for the PID of selected particle species, including real-world analyzes, will be discussed as well as the possible optimizations.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 113,
            id: 'c17168',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11734/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN / Warsaw University of Technology',
                displayOrderKey: [1, 'Kabus, Maja'],
                emailHash: '0bf656c62f08b7ab5df24c69a1cba5a4',
                familyName: 'Kabus',
                firstName: 'Maja',
                name: 'Maja Kabus',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12109,
            sessionSlotId: 2680,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Particle identification with machine learning in ALICE Run 3',
            uniqueId: 'c17168',
            url: '/event/459/contributions/11734/',
          },
          c17169: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11718/attachments/9544/13848/flashsim_chep.pdf',
                  id: 13848,
                  title: 'flashsim_chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11718,
            description:
              'Analyses in HEP experiments often rely on large MC simulated datasets. These datasets are usually produced with full-simulation approaches based on Geant4 or exploiting parametric \u201cfast\u201d simulations introducing approximations and reducing the computational cost. With our work we created a prototype version of a new \u201cfast\u201d simulation that we named \u201cflashsim\u201d targeting analysis level data tiers (such as CMS NanoAOD). Such a simulation software is based on Machine Learning, in particular exploiting the Normalizing Flows generative model. We will present the physics results achieved with this prototype, currently simulating only a few physics objects collections, in terms of: 1) accuracy of object properties, 2) correlations among paris of observables, 3) comparisons of analysis level derived quantities and discriminators between full-simulation and flash-simulation of the very same events. The speed up obtained with such an approach is of several orders of magnitude, so that when using flashsim the simulation bottleneck is represented by the \u201cgenerator\u201d (e.g. Pythia) step. We further investigated upsampling techniques, reusing the same \u201cgenerated event\u201d passing it multiple times through the detector simulation, in order to understand the increase in statistical precision that could be ultimately achieved. The results achieved with the current prototype show a higher physics accuracy and a lower computing cost compared to other fast simulation approaches such as CMS standard fastsim and Delphes based simulations.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 476,
            id: 'c17169',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11718/contribution.pdf',
            presenters: [
              {
                affiliation: 'Scuola Normale Superiore \u0026 INFN Pisa',
                displayOrderKey: [1, 'Vaselli, Francesco'],
                emailHash: 'f9711fb2602b81ba0d031767ea5bb30b',
                familyName: 'Vaselli',
                firstName: 'Francesco',
                name: 'Francesco Vaselli',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12109,
            sessionSlotId: 2680,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Flashsim: a ML based simulation for analysis datatiers',
            uniqueId: 'c17169',
            url: '/event/459/contributions/11718/',
          },
          c17477: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11762/attachments/9520/13842/FastCaloGANV2%20for%20Run3.pptx',
                  id: 13842,
                  title: 'FastCaloGANV2 for Run3.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11762,
            description:
              'AtlFast3 is the new ATLAS fast simulation that exploits a wide range of ML techniques to achieve high-precision fast simulation. The latest version of the AtlFast3 used in Run3 deploys FastCaloGANV2 which consists of 500 Generative Adversarial Networks used to simulate the showers of all particles in the ATLAS calorimeter system. The Muon Punch Through tool has also been completely rewritten using deep NN for the classification of events. An additional deep network is used to predict and correct the longitudinal position of the hits in the calorimeter layer based on the energy deposited by each shower in the calorimeter layers. These tools have been instrumental in improving the performance of AtlFast3.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 424,
            id: 'c17477',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11762/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Roma Tor Vergata and Universita\u0027 di Roma Tor Vergata',
                displayOrderKey: [0, 'Giannelli, Michele Faucci'],
                emailHash: 'b599f4a9469b5ebe0ca1932a087a8722',
                familyName: 'Giannelli',
                firstName: 'Michele Faucci',
                name: 'Michele Faucci Giannelli',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12109,
            sessionSlotId: 2680,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'FastCaloGAN: a fast simulation of the ATLAS Calorimeter with GANs',
            uniqueId: 'c17477',
            url: '/event/459/contributions/11762/',
          },
        },
        entryType: 'Session',
        friendlyId: 13,
        id: 's12109',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2043/session-timetable.pdf',
        room: 'Hampton Roads VII',
        sessionCode: '',
        sessionId: 2043,
        sessionSlotId: 2680,
        slotTitle: 'Applications 2',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#03070f',
        title: 'Track 9 - Artificial Intelligence and Machine Learning',
        uniqueId: 's12109',
        url: '/event/459/sessions/2043/',
      },
      s12111: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#4f144e',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Nikhef National institute  for subatomic physics (NL)',
            displayOrderKey: [0, 'Aaij, Roel'],
            emailHash: '6fecef0aff50325ef5ea685927dec4a5',
            familyName: 'Aaij',
            firstName: 'Roel',
            name: 'Roel Aaij',
          },
          {
            affiliation: 'Fermi National Accelerator Laboratory',
            displayOrderKey: [0, 'Timm, Steven'],
            emailHash: '40848b6b99ba5ca40be682891f2cc812',
            familyName: 'Timm',
            firstName: 'Steven',
            name: 'Steven Timm',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17219: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11814/attachments/9457/13711/CaTS_chep2023_v10.pdf',
                  id: 13711,
                  title: 'CaTS_chep2023_v10.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11814/attachments/9457/13712/CaTS_chep2023_v10.pptx',
                  id: 13712,
                  title: 'CaTS_chep2023_v10.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11814,
            description:
              'CaTS is a Geant4 advanced example that is part of Geant4[1] since version 11.0. It demonstrates the use of Opticks[2] to offload the simulation of optical photons to GPUs. Opticks interfaces with the Geant4 toolkit to collect all the necessary information to generate and trace optical photons, re-implements the optical physics processes to be run on the GPU, and automatically translates the Geant4 geometry into a GPU appropriate format. To trace the photons Opticks\r\nuses NVIDIA\u00ae OptiX7\u2122 [3]. In this presentation we shall describe CaTS and the work performed to integrate Opticks with Geant4 using the tasking mechanism and the work to include CaTS in the software framework used by liquid Argon TPC neutrino experiments. We shall demonstrate that the generation and tracing of optical photons represents an ideal application to be offloaded to GPUs, fully utilizing the high degree of available parallelism. In a typical liquid Argon TPC simulation, a speedup of several hundred times compared to single threaded Geant4 is observed.\r\n\r\n[1]https://geant4.web.cern.ch/\r\n[2]https://doi.org/10.1051/epjconf/202125103009\r\n[3]https://developer.nvidia.com/rtx/ray-tracing/optix\r\n\r\nKeywords: Simulation of optical photons, GPUs.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 345,
            id: 'c17219',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11814/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Wenzel, Hans'],
                emailHash: '8f9be3b3afde50b61bf19e4a1e0ab965',
                familyName: 'Wenzel',
                firstName: 'Hans',
                name: 'Hans Wenzel',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12111,
            sessionSlotId: 2682,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'CaTS: Integration of Geant4 and Opticks',
            uniqueId: 'c17219',
            url: '/event/459/contributions/11814/',
          },
          c17220: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11829/attachments/9445/13694/23.05.-Madgraph-CHEP-SH.pdf',
                  id: 13694,
                  title: '23.05.-Madgraph-CHEP-SH.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11829,
            description:
              'Madgraph5_aMC@NLO is one of the workhorses for Monte Carlo event generation in the LHC experiments and an important consumer of compute resources. The software has been reengineered to maintain the overall look-and-feel of the user interface while achieving very large overall speedups. The computationally intensive part (the calculation of "matrix elements") is offloaded to new implementations optimized for GPUs and for vector CPUs, using event-level data parallelism. In this contribution, we will report on the first experience with the alpha release of the software supporting leading-order QCD processes. The achieved performance speedups and the potential for further improvements will be discussed in detail.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 356,
            id: 'c17220',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11829/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [2, 'Hageboeck, Stephan'],
                emailHash: '64d6d832b57aa9e7597d4011228b0da0',
                familyName: 'Hageboeck',
                firstName: 'Stephan',
                name: 'Stephan Hageboeck',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12111,
            sessionSlotId: 2682,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'Madgraph5_aMC@NLO on GPUs and vector CPUs: experience with the first alpha release',
            uniqueId: 'c17220',
            url: '/event/459/contributions/11829/',
          },
          c17221: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11830/attachments/9276/13450/Meagher_2023May_CHEP_Parallel_Air_Shower.pdf',
                  id: 13450,
                  title: 'Meagher_2023May_CHEP_Parallel_Air_Shower.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11830,
            description:
              'The IceCube Neutrino Observatory is a cubic kilometer neutrino telescope \r\nlocated at the Geographic South Pole. For every observed neutrino event, \r\nthere are over 10^6 background events caused by cosmic-ray air shower \r\nmuons. In order to properly separate signal from background, it is \r\nnecessary to produce Monte Carlo simulations of these air showers. \r\nAlthough to-date, IceCube has produced large quantities of background \r\nsimulation, these studies remain statistics limited. The most \r\nsignificant impediment to producing more simulation is complicated \r\ncomputing requirements: the first stage of the simulation, air shower \r\nand muon propagation, needs to be run on CPUs while the second stage, \r\nphoton propagation, can only be performed efficiently on GPUs. \r\nProcessing both of these stages on the same node will result in an \r\nunderutilized GPU but using different nodes will encounter bandwidth \r\nbottlenecks. Furthermore, due to the power-law energy spectrum of cosmic \r\nrays, the memory footprint of the detector response often exceeded the \r\nlimit in unpredictable ways. In this talk, I will present new \r\nclient/server code which parallelizes the first stage onto multiple CPUs \r\non the same node and then passes it on to the GPU for photon \r\npropagation. This results in GPU utilization of greater than 90% as well \r\nas more predictable memory usage and an overall factor of 20 improvement \r\nin speed over previous techniques.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 93,
            id: 'c17221',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11830/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Wisconsin\u2013Madison',
                displayOrderKey: [1, 'Meagher, Kevin'],
                emailHash: '0d034d9e8141e111515ab351d428d981',
                familyName: 'Meagher',
                firstName: 'Kevin',
                name: 'Dr Kevin Meagher',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12111,
            sessionSlotId: 2682,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Parallelization of Air Shower Simulation with IceCube',
            uniqueId: 'c17221',
            url: '/event/459/contributions/11830/',
          },
          c17222: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11818/attachments/9324/13745/srj-chep.pdf',
                  id: 13745,
                  title: 'srj-chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11818,
            description:
              'Celeritas is a new Monte Carlo detector simulation code designed for computationally intensive applications (specifically, HL-LHC simulation) on high-performance heterogeneous architectures. In the past two years Celeritas has advanced from prototyping a simple, GPU-based, single-physics-model infinite medium to implementing a full set of electromagnetic physics processes in complex geometries. The latest release of Celeritas has incorporated full VecGeom device-based navigation, an event loop in the presence of magnetic fields, and detector hit scoring.  The new Acceleritas library provides a scheduler to offload electromagnetic physics to the GPU within a Geant4 driven simulation, enabling integration of Celeritas into HEP experimental frameworks such as CMSSW.  Finally, we are collaborating with the AdePT team to design a suite of benchmarks to verify the correctness and evaluate the performance of GPU-enabled detector simulations.  We present an overview of these new capabilities in Celeritas and show the performance results on both standalone and Geant4-integrated detector simulation benchmarks on both Nvidia and AMD GPU-based architectures.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 310,
            id: 'c17222',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11818/contribution.pdf',
            presenters: [
              {
                affiliation: 'Oak Ridge National Laboratory',
                displayOrderKey: [1, 'Johnson, Seth'],
                emailHash: '372044a7595de8b4494b5d5b2a1b0902',
                familyName: 'Johnson',
                firstName: 'Seth',
                name: 'Seth Johnson',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12111,
            sessionSlotId: 2682,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'Celeritas: EM physics on GPUs and a path to full-featured accelerated detector simulation',
            uniqueId: 'c17222',
            url: '/event/459/contributions/11818/',
          },
          c17294: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11850/attachments/9398/13630/20230508_CHEP23_Zenny_Wetterstan_Acceleration_beyond_lowest_order_event_generation.pdf',
                  id: 13630,
                  title:
                    '20230508_CHEP23_Zenny_Wetterstan_Acceleration_beyond_lowest_order_event_generation.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11850,
            description:
              'An important area of HEP studies at the LHC currently concerns the need for more extensive and precise comparison data. Important tools in this realm are event reweighting and the evaluation of more precise next-to-leading order (NLO) physics processes via Monte Carlo (MC) event generators, especially in the context of the upcoming High Luminosity LHC phase. Current event generators need to improve their throughput for these studies. MadGraph5_aMC@NLO (MGaMC) is an event generator being used heavily by LHC experiments which has been accelerated considerably with a GPU and vector CPU port, but as of yet only for leading order processes. In this contribution, a prototype for event reweighting using the accelerated MGaMC software package, as well as plans for the implementation of NLO calculations, will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 189,
            id: 'c17294',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11850/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Wettersten, Zenny'],
                emailHash: '002b5001b77c1c8e912f45d6d34118f2',
                familyName: 'Wettersten',
                firstName: 'Zenny',
                name: 'Zenny Wettersten',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12111,
            sessionSlotId: 2682,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Acceleration beyond lowest order event generation',
            uniqueId: 'c17294',
            url: '/event/459/contributions/11850/',
          },
        },
        entryType: 'Session',
        friendlyId: 15,
        id: 's12111',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2045/session-timetable.pdf',
        room: 'Marriott Ballroom VII',
        sessionCode: '',
        sessionId: 2045,
        sessionSlotId: 2682,
        slotTitle: 'Simulaton on Heterogeneous Architectures',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffefff',
        title:
          'Track X - Exascale Science, Heterogeneous Computing and Accelerators, and Quantum Computing',
        uniqueId: 's12111',
        url: '/event/459/sessions/2045/',
      },
      s17206: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#b9cbca',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Pittsburgh',
            displayOrderKey: [0, 'Bandieramonte, Marilena'],
            emailHash: 'f4b45dbfcfef5c90c1b484b28e777c71',
            familyName: 'Bandieramonte',
            firstName: 'Marilena',
            name: 'Marilena Bandieramonte',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'VALLECORSA, SOFIA'],
            emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
            familyName: 'VALLECORSA',
            firstName: 'SOFIA',
            name: 'SOFIA VALLECORSA',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-08',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17075: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: null,
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11415,
            description:
              'The Large Field Low-energy X-ray Polarization Detector (LPD) is a gas photoelectric effect polarization detector designed for the detailed study of X-ray temporary sources in high-energy astrophysics. Previous studies have shown that the polarization degree of gamma ray bursts (GRBs) is generally low or unpolarized. Considering the spatial background and other interferences, We need high modulation algorithms to observe low polarization GRB. For this purpose, moment analysis, graph theory, neural network algorithms are studied for the reconstruction of photoelectron emission angle. Combined with experimental and simulation data, the reconstruction performance of different algorithms at various energy and incident angles is evaluated. \r\n\r\nMoment analysis algorithm finds out the large angle scattering point of photoelectron and remove the zone. Photoelectron track after cutting   can be reconstructed. However, on the one hand, when track length is large, the performance of moment analysis algorithm becomes worse. On the other hand, for short cases, the track information loss caused by cutting is more serious, and the performance of moment analysis algorithm will also be degraded. In order to address these problems, graph theory algorithm and neural network are studied. Graph theory algorithm improves the reconstruction performance by precisely positioning the photoelectric action point through the trunk endpoint, which is more effective for longer tracks. Training samples of neural network algorithm are from the simulation platform built based on Geant4 in which photoelectric interaction, ionization diffusion, signal digitization and other processes on the detector are simulated as real as possible. Two typical neural networks, CNN and GNN, are studied. The results show that both neural networks predict high modulation and stability in designed energy range. In order to carefully evaluate the performance of the algorithm, the simulation should be as close to the real situation as possible.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 483,
            id: 'c17075',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11415/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Chinese Academy of Sciences',
                displayOrderKey: [1, 'Yi, Difan'],
                emailHash: '0ae9fa3249a0c46d06fcecb32b473738',
                familyName: 'Yi',
                firstName: 'Difan',
                name: 'Dr Difan Yi',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17206,
            sessionSlotId: 3655,
            startDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title: 'Simulation and Reconstruction of Photoelectric X-ray Polarimetry on LPD',
            uniqueId: 'c17075',
            url: '/event/459/contributions/11415/',
          },
          c17208: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11449/attachments/9374/13595/Farinelli_PARSIFAL_CHEP2023.pdf',
                  id: 13595,
                  title: 'Farinelli_PARSIFAL_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11449,
            description:
              'PARSIFAL (PARametrized SImulation) is a software tool that can reproduce the complete response of both triple-GEM and micro-RWELL based trackers. It takes into account the involved physical processes by their simple parametrization and thus in a very fast way. Existing software as GARFIELD++ are robust and reliable, but very CPU time consuming. The implementation of PARSIFAL was driven by the necessity to reduce the processing time, without losing the precision of a full simulation. A series of parameters, that can be extracted from the GARFIELD++ simulation, are set as input to PARSIFAL, which then runs independently from GARFIELD++. PARSIFAL can simulate samples with high statistics much faster, taking into account the various steps (ionization, diffusion, multiplication, signal induction and electronics) from the simple sampling from parameterized distributions. In the case of the micro-RWELL MPGD, the effect of the high resistivity layer on the charge spread on the anode was introduced, following M.S. Dixit and A. Rankin treatment.\r\nPARSIFAL was used to simulate triple-GEM chambers and the results were tuned to match experimental data from testbeams. In this case the adopted electronics was APV-25 readout by SRS system, which has been simulated in the code. The same procedure was later applied to micro-RWELL chambers, readout this time by the TIGER ASIC and the GEMROC system. This new electronics was added to PARSIFAL code and a tuning of the simulated-to-real data was performed. A presentation of the full code will be given in this contribution, setting the focus on the latest implementations and on a first comparison with experimental data from micro-RWELL.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 204,
            id: 'c17208',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11449/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Ferrara',
                displayOrderKey: [1, 'Farinelli, Riccardo'],
                emailHash: '8673859ba6b9c76e6be1bf20a265cfab',
                familyName: 'Farinelli',
                firstName: 'Riccardo',
                name: 'Riccardo Farinelli',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17206,
            sessionSlotId: 3655,
            startDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title:
              'PARSIFAL: parametrized simulation of triple-GEM and micro-RWELL response to a charged particle',
            uniqueId: 'c17208',
            url: '/event/459/contributions/11449/',
          },
          c17209: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11426/attachments/9314/13511/AtlFast3_MicheleFaucciGiannelli.pptx',
                  id: 13511,
                  title: 'AtlFast3_MicheleFaucciGiannelli.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11426,
            description:
              'AtlFast3 is the new, high-precision fast simulation in ATLAS that was deployed by the collaboration to replace AtlFastII, the fast simulation tool that was successfully used for most of Run2. AtlFast3 combines a parametrization-based Fast Calorimeter Simulation and a new machine-learning-based Fast Calorimeter Simulation based on Generative Adversarial Networks (GANs). The new fast simulation can reproduce the Geant4 inputs with higher accuracy than the old AtlFast2. In particular, the simulation of jets of particles reconstructed with large radii and the detailed description of their substructure are significantly improved in AtlFast3. Results will be presented on the performance of the new AtlFast3 that will be deployed for the simulation of the majority of events in Run3; these changes are crucial for achieving the precision needed by analyses that will need to rely mainly on fast simulation.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 420,
            id: 'c17209',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11426/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Roma Tor Vergata',
                displayOrderKey: [1, 'Faucci Giannelli, Michele'],
                emailHash: 'b599f4a9469b5ebe0ca1932a087a8722',
                familyName: 'Faucci Giannelli',
                firstName: 'Michele',
                name: 'Dr Michele Faucci Giannelli',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17206,
            sessionSlotId: 3655,
            startDate: {
              date: '2023-05-08',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title: 'The AtlFast3, the ATLAS fast simulation tool in Run3',
            uniqueId: 'c17209',
            url: '/event/459/contributions/11426/',
          },
          c17211: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11454/attachments/9426/14314/Lamarr_mbarbetti_CHEP2023.pdf',
                  id: 14314,
                  title: 'Lamarr_mbarbetti_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11454,
            description:
              'Detailed detector simulation is the major consumer of CPU resources at LHCb, having used more than 80% of the total computing budget during Run 2 of the Large Hadron Collider at CERN. As data is collected by the upgraded LHCb detector during Run 3 of the LHC, larger requests for simulated data samples are necessary, and will far exceed the pledged resources of the experiment, even with existing fast simulation options. An evolution of technologies and techniques to produce simulated samples is mandatory to meet the upcoming needs of analysis to interpret signal versus background and measure efficiencies. In this context, we propose Lamarr, a Gaudi-based framework designed to offer to LHCb the fastest solution for simulations.\r\nLamarr consists of a pipeline of modules parametrizing both the detector response and the reconstruction algorithms of the LHCb experiment. Most of the parameterizations are made of Deep Generative Models and Gradient Boosted Decision Trees trained on simulated samples or alternatively, where possible, on real data. Embedding Lamarr in the general LHCb Gauss Simulation framework allows combining its execution with any of the available generators in a seamless way.\r\nLamarr has been validated by comparing key reconstructed quantities with Detailed Simulation. Good agreement of the simulated distributions is obtained with two-order-of-magnitude speed-up of the simulation phase.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 629,
            id: 'c17211',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11454/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Firenze',
                displayOrderKey: [0, 'Barbetti, Matteo'],
                emailHash: '2691ed985eae0fa652f790a2b89ca5b9',
                familyName: 'Barbetti',
                firstName: 'Matteo',
                name: 'Matteo Barbetti',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17206,
            sessionSlotId: 3655,
            startDate: {
              date: '2023-05-08',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title: 'The LHCb ultra-fast simulation option, Lamarr: design and validation',
            uniqueId: 'c17211',
            url: '/event/459/contributions/11454/',
          },
          c17300: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11798/attachments/9413/13654/CHEP2023_ADOnofrio.pdf',
                  id: 13654,
                  title: 'CHEP2023_ADOnofrio.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11798,
            description:
              'IDEA (Innovative Detector for an Electron-positron Accelerator) is an innovative general-purpose detector concept, designed to study electron-positron collisions at future e$^+$e$^-$ circular colliders (FCC-ee and CEPC). \r\nThe detector will be equipped with a dual read-out calorimeter able to measure separately the hadronic component and the electromagnetic component of the showers initiated by the impinging hadrons.\r\n\r\nParticle flow algorithms (PFAs) have become the paradigm of detector design for the high energy frontier and this talk discusses a project to build a Particle Flow algorithm for the IDEA detector using Machine Learning (ML) techniques. Machine Learning is used for particle reconstruction and identification profiting of the high granularity of the fiber-based dual-readout calorimeter. Neural Networks (NN) are built for electron, pions, neutral kaons, muons reconstruction and identification inside the calorimeter and for the jet reconstruction. The performances of the algorithm using several  NN architectures will be shown, with particular attention to the layer setup and the activation function choices. The performances will be evaluated on the resolution function of the reconstructed particles and of the reconstructed jet. The algorithm will be trained using both parallel CPUs and GPU, and the time performances and the memory usage of the two approaches will be systematically compared.\r\nFinally, the aim of the project is to develop the NN algorithm inside the Pandora PFA framework.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 416,
            id: 'c17300',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11798/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN-Roma Tre',
                displayOrderKey: [1, 'DOnofrio, Adelina'],
                emailHash: '4f1bfc883583879d1ccfafc33228a0ca',
                familyName: 'DOnofrio',
                firstName: 'Adelina',
                name: 'Adelina DOnofrio',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17206,
            sessionSlotId: 3655,
            startDate: {
              date: '2023-05-08',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title:
              'Development of particle flow algorithms based on Neural Network techniques for the IDEA calorimeter at future colliders',
            uniqueId: 'c17300',
            url: '/event/459/contributions/11798/',
          },
          c17318: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11777/attachments/9359/13597/gaede_chep23_caloml_v01.pdf',
                  id: 13597,
                  title: 'gaede_chep23_caloml_v01.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11777,
            description:
              'Modern high energy physics experiments fundamentally rely on accurate simulation- both to characterise detectors and to connect observed signals to underlying theory. Traditional simulation tools are reliant upon Monte Carlo methods which, while powerful, consume significant computational resources. These computing pressures are projected to become a major bottleneck at the high luminosity stage of the LHC and for future colliders. Deep generative models hold promise to potentially offer significant reductions in compute times, while maintaining a high degree of physical fidelity.\r\n\r\nThis contribution provides an overview of a growing body of work focused on simulating showers in highly granular calorimeters, which is making significant strides towards realising fast simulation tools based on deep generative models. Progress on the simulation of both electromagnetic and hadronic showers, as well further steps to address challenges faced when broadening the scope of these simulators, will be reported. A particular focus will be placed on the high degree of physical fidelity achieved, as well as the performance after interfacing with reconstruction algorithms.',
            duration: 15.0,
            endDate: {
              date: '2023-05-08',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 538,
            id: 'c17318',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11777/contribution.pdf',
            presenters: [
              {
                affiliation: 'DESY',
                displayOrderKey: [0, 'Gaede, Frank'],
                emailHash: '438141d829121ab01b748f2ed2b32d39',
                familyName: 'Gaede',
                firstName: 'Frank',
                name: 'Mr Frank Gaede',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17206,
            sessionSlotId: 3655,
            startDate: {
              date: '2023-05-08',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title:
              'Generative Models for Fast Simulation of Electromagnetic and Hadronic Showers in Highly Granular Calorimeters',
            uniqueId: 'c17318',
            url: '/event/459/contributions/11777/',
          },
        },
        entryType: 'Session',
        friendlyId: 17,
        id: 's17206',
        inheritLoc: false,
        inheritRoom: false,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2561/session-timetable.pdf',
        room: 'Chesapeake Meeting Room',
        sessionCode: '',
        sessionId: 2561,
        sessionSlotId: 3655,
        slotTitle: 'Simulation',
        startDate: {
          date: '2023-05-08',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f0202',
        title: 'Track 3+9 Crossover',
        uniqueId: 's17206',
        url: '/event/459/sessions/2561/',
      },
    },
    '20230509': {
      b12116: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 30.0,
        endDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12116',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom (3rd floor)',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-09',
          time: '10:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'AM Break',
        uniqueId: 'b12116',
      },
      b12120: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12120',
        inheritLoc: false,
        inheritRoom: false,
        location: '',
        room: '',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'Lunch on Own',
        uniqueId: 'b12120',
      },
      b16369: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#8ec473',
        conferenceId: 459,
        description:
          'Join like-minded attendees to hang out and celebrate diversity at the Grain, a rooftop beer garden located atop the Hilton Norfolk The Main hotel.  This will be a casual evening of camaraderie.\r\n\r\nSponsorship provided by IDEAS4HPC.',
        duration: 120.0,
        endDate: {
          date: '2023-05-09',
          time: '20:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b16369',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Grain',
        room: '',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#021f03',
        title: '"Diversity Drink" at Grain',
        uniqueId: 'b16369',
      },
      s12076: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#5f171a',
        conferenceId: 459,
        contribDuration: 60.0,
        conveners: [
          {
            affiliation: 'Argonne National Laboratory',
            displayOrderKey: [0, 'Childers, Taylor'],
            emailHash: 'f7343705e75f1b3d105ea219714bcad9',
            familyName: 'Childers',
            firstName: 'Taylor',
            name: 'Taylor Childers',
          },
        ],
        description: '',
        duration: 60.0,
        endDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        entries: {},
        entryType: 'Session',
        friendlyId: 3,
        id: 's12076',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2033/session-timetable.pdf',
        room: 'Hampton Roads Ballroom and Foyer Area',
        sessionCode: '',
        sessionId: 2033,
        sessionSlotId: 2654,
        slotTitle: 'Poster Session Tuesday',
        startDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffffff',
        title: 'Poster Session',
        uniqueId: 's12076',
        url: '/event/459/sessions/2033/',
      },
      s12113: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfdfdf',
        conferenceId: 459,
        contribDuration: 600.0,
        conveners: [],
        description: '',
        duration: 60.0,
        endDate: {
          date: '2023-05-09',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        entries: {},
        entryType: 'Session',
        friendlyId: 4,
        id: 's12113',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2034/session-timetable.pdf',
        room: '',
        sessionCode: '',
        sessionId: 2034,
        sessionSlotId: 2684,
        slotTitle: 'Registration',
        startDate: {
          date: '2023-05-09',
          time: '08:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#151515',
        title: 'Registration',
        uniqueId: 's12113',
        url: '/event/459/sessions/2034/',
      },
      s12115: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'Brookhaven Science Associates',
            displayOrderKey: [0, 'LAURET, Jerome'],
            emailHash: '60e98fb28af2edf9a8a6c39567ee11c7',
            familyName: 'LAURET',
            firstName: 'Jerome',
            name: 'Jerome LAURET',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '10:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17271: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11671/attachments/9585/13973/CHEP2023_VRE_Gazzarrini.pdf',
                  id: 13973,
                  title: 'CHEP2023_VRE_Gazzarrini.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 11671,
            description:
              'One of the objectives of the EOSC (European Open Science Cloud) Future Project is to integrate diverse analysis workflows from Cosmology, Astrophysics and High Energy Physics in a common framework. The project\u2019s development relies on the implementation of the Virtual Research Environment (VRE), a prototype platform supporting the goals of Dark Matter and Extreme Universe Science Projects in the respect of FAIR data policies. It makes use of a common AAI system, and exposes the experiments\u2019 data (ATLAS, Fermi-LAT, CTA, Darkside, Km3Net, Virgo, LOFAR)  in a reliable distributed storage infrastructure for multi-science: the Data Lake. The entry point of such a platform is a jupyterhub instance deployed on top of a scalable Kubernetes infrastructure, providing an interactive graphical interface for researchers to access, analyse and share data. The data access and browsability is enabled through API calls to the high level data management and storage orchestration software (Rucio). In this contribution we will provide an overview of the VRE and present our recent work to improve its usability and functionality. The improvements include a software repository plug-in enabling researchers to directly select computational environments from Docker images and the integration of a re-analysis platform (Reana) supporting various distributed computing backends. The final goal of the VRE project, bringing together data and software access, workflow reproducibility and enhanced user interface, is to facilitate scientific collaboration, ultimately accelerating research in various fields.',
            duration: 30.0,
            endDate: {
              date: '2023-05-09',
              time: '09:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 101,
            id: 'c17271',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11671/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Gazzarrini, Elena'],
                emailHash: '39921bb8f8e49b6f44e880111530f493',
                familyName: 'Gazzarrini',
                firstName: 'Elena',
                name: 'Elena Gazzarrini',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12115,
            sessionSlotId: 2686,
            startDate: {
              date: '2023-05-09',
              time: '09:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'The Virtual Research Environment: towards a comprehensive analysis platform',
            uniqueId: 'c17271',
            url: '/event/459/contributions/11671/',
          },
          c17272: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12493/attachments/9310/13507/ESnet%20Evolution%20-%20CHEP23.pdf',
                  id: 13507,
                  title: 'ESnet Evolution - CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12493,
            description:
              'The Energy Sciences Network (ESnet) is the high performance network of the US Department of Energy Office of Science.  Over its 36-year span, ESnet has evolved to meet the requirements of ever changing scientific workflows.  This presentation will provide a brief history of ESnet\u0027s generational changes and highlight the capabilities of its current generation network ESnet6.  This presentation will also provide a glimpse into a future ESnet(7) and the requirements driving its design.',
            duration: 30.0,
            endDate: {
              date: '2023-05-09',
              time: '10:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 647,
            id: 'c17272',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12493/contribution.pdf',
            presenters: [
              {
                affiliation: 'ESnet',
                displayOrderKey: [0, 'Guok, Chin'],
                emailHash: 'dde57bec332b3e9cf364f7ff1bd8af78',
                familyName: 'Guok',
                firstName: 'Chin',
                name: 'Chin Guok',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12115,
            sessionSlotId: 2686,
            startDate: {
              date: '2023-05-09',
              time: '09:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Evolution of ESnet - A Changing Landscape in Scientific Networking',
            uniqueId: 'c17272',
            url: '/event/459/contributions/12493/',
          },
          c17273: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11610/attachments/9586/13938/CHEP%202023_coffea_casa__.pdf',
                  id: 13938,
                  title: 'CHEP 2023_coffea_casa__.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 11610,
            description:
              'The large data volumes expected from the High Luminosity LHC (HL-LHC) present challenges to existing paradigms and facilities for end-user data analysis. Modern cyberinfrastructure tools provide a diverse set of services that can be composed into a system that provides physicists with powerful tools that give them straightforward access to large computing resources, with low barriers to entry. The coffea-casa analysis facility provides an environment for end users enabling the execution of increasingly complex analyses such as those demonstrated by the Analysis Grand Challenge (AGC) and capturing the features that physicists will need for the HL-LHC.\r\nWe describe the development progress of the coffea-casa facility featuring its modularity while demonstrating the ability to port and customize the facility software stack to other locations. The facility also facilitates the support of different backends to other batch systems while staying Kubernetes-native.\r\nWe present evolved architecture of the facility, such as the integration of advanced data delivery services (e.g. ServiceX) and making data caching services (e.g. XCache) available to end users of the facility.\r\nWe also highlight the composability of modern cyberinfrastructure tools. To enable machine learning pipelines at coffee-casa analysis facilities, a set of industry ML solutions adopted for HEP columnar analysis were integrated on top of existing facility services. These services also feature transparent access for user workflows to GPUs available at a facility via inference servers while using Kubernetes as enabling technology.',
            duration: 30.0,
            endDate: {
              date: '2023-05-09',
              time: '10:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 379,
            id: 'c17273',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11610/contribution.pdf',
            presenters: [
              {
                affiliation: 'University Nebraska-Lincoln (US)',
                displayOrderKey: [1, 'Shadura, Oksana'],
                emailHash: 'f5e7378e16e9fee96ad835deb6454086',
                familyName: 'Shadura',
                firstName: 'Oksana',
                name: 'Oksana Shadura',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12115,
            sessionSlotId: 2686,
            startDate: {
              date: '2023-05-09',
              time: '10:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Coffea-Casa: Building composable analysis facilities for the HL-LHC',
            uniqueId: 'c17273',
            url: '/event/459/contributions/11610/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's12115',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2686,
        slotTitle: 'Computing Infrastructure \u0026 Future Directions',
        startDate: {
          date: '2023-05-09',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's12115',
        url: '/event/459/sessions/2024/',
      },
      s12117: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#6f390d',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Barisits, Martin'],
            emailHash: '46207806844c107a8fefe566f669b206',
            familyName: 'Barisits',
            firstName: 'Martin',
            name: 'Martin Barisits',
          },
          {
            affiliation: 'FNAL',
            displayOrderKey: [0, 'Kirby, Michael'],
            emailHash: '384f25285a1799bb8e2b4ea136bfd945',
            familyName: 'Kirby',
            firstName: 'Michael',
            name: 'Michael Kirby',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16964: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11301/attachments/9638/14036/S3-Gateway-CHEP23-v03.pdf',
                  id: 14036,
                  title: 'S3-Gateway-CHEP23-v03.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11301/attachments/9638/14037/S3-Gateway-CHEP23-v03.pptx',
                  id: 14037,
                  title: 'S3-Gateway-CHEP23-v03.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11301,
            description:
              'The Xrootd S3 Gateway is a universal high performance proxy service that can be used to access S3 portals using existing HEP credentials (e.g. JSON Web Tokens and x509). This eliminates one of the biggest roadblocks to using public cloud storage resources. This paper describes how the S3 Gateway leverages existing HEP software (e.g. Davix and XRootD) to provide a familiar scalable service that works with public (i.e. AWS, GCS, etc) and private (i.e. CEPH, MinIO, etc.) S3 portal available to the HEP community, and to seamlessly integrate them into the WLCG storage and data transfer ecosystem. The test was conducted at an ATLAS site and ATLAS installation in GCP and AWS.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 181,
            id: 'c16964',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11301/contribution.pdf',
            presenters: [
              {
                affiliation: 'SLAC National Accelerator Laboratory',
                displayOrderKey: [0, 'Yang, Wei'],
                emailHash: '454586d94ec6a7ebad3c68ceb1774c96',
                familyName: 'Yang',
                firstName: 'Wei',
                name: 'Wei Yang',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12117,
            sessionSlotId: 2687,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Xrootd S3 Gateway for WLCG Storage',
            uniqueId: 'c16964',
            url: '/event/459/contributions/11301/',
          },
          c16965: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11318/attachments/9187/13344/2305-XCache-CHEP23.pdf',
                  id: 13344,
                  title: '2305-XCache-CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11318,
            description:
              'There has been a significant increase in data volume from various large scientific projects, including the Large Hadron Collider (LHC) experiment. The High Energy Physics (HEP) community requires increased data volume on the network, as the community expects to produce almost thirty times annual data volume between 2018 and 2028 [1]. To mitigate the repetitive data access issue and network overloading, regional data caching mechanism [2], [3], or in-network cache has been deployed in Southern California for the US CMS, and its effectiveness has been studied [4], [5]. With the number of redundant data transfers over the wide-area network decreasing, the caching approach improves overall application performance as well as network traffic savings.\r\n\r\nIn this work, we examined the trends in data volume and data throughput performance from the Southern California Petabyte Scale Cache (SoCal Repo) [6], which includes 24 federated caching nodes with approximately 2.5PB of total storage. From the trends, we also determined how much a machine learning model can predict the network access patterns for the regional data cache. The fluctuation in the daily cache utilization, as shown in Figure 1, is high, and it is challenging to build a learning model to follow the trends.\r\n\u003cimg src="https://sdm.lbl.gov/students/chep23/size_ratio_whole_chep23.png" width="350" height="150"\u003e\r\n\u003ca href="https://sdm.lbl.gov/students/chep23/size_ratio_whole_chep23.png"\u003eFigure 1\u003c/a\u003e: Daily proportion of cache hits volume and cache misses volume from July 2021 to June 2022, with 8.02 million data access records for 8.2PB of traffic volume for cache misses and 4.5PB of traffic volume for cache hits. 35.4% of the total traffic has been saved from the cache.\r\n\r\nThe daily and hourly study also modeled the cache utilization and data throughput performance, with 80% of the training data and 20% of the testing data. Figure 2 shows the samples of our hourly study results. The root-mean-square error (RMSE) is measured and compared to the standard deviation of the input data values to provide a reference to determine how large the errors of predictions are. The relative error, ratio of testing RMSE and standard deviation, is less than 0.5, indicating the predictions are pretty accurate.\r\n\r\n\u003cimg src="https://sdm.lbl.gov/students/chep23/hourly_cache_miss_size_chep23.png" width="350" height="150"\u003e\r\n\u003ca href="https://sdm.lbl.gov/students/chep23/hourly_cache_miss_size_chep23.png"\u003eFigure 2 (a)\u003c/a\u003e: Hourly volume of cache misses; training set RMSE=0.16, testing set RMSE=0.40, std.dev=1.42\r\n\u003cimg src="https://sdm.lbl.gov/students/chep23/hourly_cache_miss_average_throughput_chep23.png" width="350" height="150"\u003e\r\n\u003ca href="https://sdm.lbl.gov/students/chep23/hourly_cache_miss_average_throughput_chep23.png"\u003eFigure 2 (b)\u003c/a\u003e: Hourly throughput of cache misses; training set RMSE=25.90, testing set RMSE=18.93, std.dev=121.36\r\n\r\nThe study results can be used to optimize the cache utilization, network resources, and application workflow performance, and become the base for exploring characteristics of other data lakes as well as examining longer term network requirements for the data caches.\r\n\r\n\u003ca href="https://sdm.lbl.gov/students/chep23/CHEP23_xcache_ext_abstract.pdf"\u003eFull PDF of the extended abstract is available online: https://sdm.lbl.gov/students/chep23/CHEP23_xcache_ext_abstract.pdf\u003c/a\u003e\r\n\r\nReferences\r\n[1] B. Brown, E. Dart, G. Rai, L. Rotman, and J. Zurawski, \u201cNuclear physics network requirements review report,\u201d Energy Sciences Network, University of California, Publication Management System Report LBNL- 2001281, 2020. [Online]. Available: https://www.es.net/assets/Uploads/ 20200505- NP.pdf\r\n[2] X. Espinal, S. Jezequel, M. Schulz, A. Sciaba`, I. Vukotic, and F. Wuerthwein, \u201cThe quest to solve the hl-lhc data access puzzle,\u201d EPJ Web of Conferences, vol. 245, p. 04027, 2020. [Online]. Available: https://doi.org/10.1051/epjconf/202024504027\r\n[3] E. Fajardo, D. Weitzel, M. Rynge, M. Zvada, J. Hicks, M. Selmeci, B. Lin, P. Paschos, B. Bockelman, A. Hanushevsky, F. Wu \u0308rthwein, and I. Sfiligoi, \u201cCreating a content delivery network for general science on the internet backbone using XCaches,\u201d EPJ Web of Conferences, vol. 245, p. 04041, 2020. [Online]. Available: https://doi.org/10.1051/epjconf/202024504041\r\n[4] E. Copps, H. Zhang, A. Sim, K. Wu, I. Monga, C. Guok, F. Wurthwein, D. Davila, and E. Fajardo, \u201cAnalyzing scientific data sharing patterns with in-network data caching,\u201d in 4th ACM International Workshop on System and Network Telemetry and Analysis (SNTA 2021), ACM. ACM, 2021.\r\n[5] R. Han, A. Sim, K. Wu, I. Monga, C. Guok, F. Wurthwein, D. Davila, J. Balcas, and H. Newman, \u201cAccess trends of in-network cache for scientific data,\u201d in 5th ACM International Workshop on System and Network Telemetry and Analysis (SNTA 2022), ACM. ACM, 2022.\r\n[6] E. Fajardo, A. Tadel, M. Tadel, B. Steer, T. Martin, and F. Wu \u0308rthwein, \u201cA federated xrootd cache,\u201d Journal of Physics: Conference Series, vol. 1085, p. 032025, 2018.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 32,
            id: 'c16965',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11318/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lawrence Berkeley National Laboratory',
                displayOrderKey: [1, 'Sim, Alex'],
                emailHash: '38f38c492f6ca658cfb28aabcf9a502c',
                familyName: 'Sim',
                firstName: 'Alex',
                name: 'Alex Sim',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12117,
            sessionSlotId: 2687,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Predicting Resource Usage Trends with Southern California Petabyte Scale Cache',
            uniqueId: 'c16965',
            url: '/event/459/contributions/11318/',
          },
          c16966: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11329/attachments/9637/14299/CHEP_2023_AmazonS3_IntelDAOS-4.pdf',
                  id: 14299,
                  title: 'CHEP_2023_AmazonS3_IntelDAOS-4.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11329,
            description:
              'Current and future distributed HENP data analysis infrastructures rely increasingly on object stores in addition to regular remote file systems. Such file-less storage systems are popular as a means to escape the inherent scalability limits of the POSIX file system API. Cloud storage is already dominated by S3-like object stores, and HPC sites are starting to take advantage of object stores for the next generation of supercomputers. In light of this, ROOT\u0027s new I/O subsystem RNTuple has been engineered to support object stores alongside (distributed) file systems as first class citizens, while also addressing performance bottlenecks and interface shortcomings of its predecessor, TTree I/O.\r\nIn this contribution, we describe the improvements around RNTuple\u2019s support for object stores, expounding on the challenges and insights toward efficient storage and high-throughput data transfers. Specifically, we introduce RNTuple\u2019s native backend for the Amazon S3 cloud storage and present the latest developments in our Intel DAOS backend, demonstrating RNTuple\u2019s integration with next-generation HPC sites.\r\nThrough experimental evaluations, we compare the two backends in single node and distributed end-to-end analyses using ROOT\u2019s RDataFrame, proving Amazon S3 and Intel DAOS as viable HENP storage providers.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 436,
            id: 'c16966',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11329/contribution.pdf',
            presenters: [
              {
                affiliation: 'UFRGS (BR)',
                displayOrderKey: [1, 'Lazzari Miotto, Giovanna'],
                emailHash: '595eb9841bc216749bb1bb4bc14efb51',
                familyName: 'Lazzari Miotto',
                firstName: 'Giovanna',
                name: 'Ms Giovanna Lazzari Miotto',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12117,
            sessionSlotId: 2687,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Storing LHC Data in Amazon S3 and Intel DAOS through RNTuple',
            uniqueId: 'c16966',
            url: '/event/459/contributions/11329/',
          },
          c16967: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11314/attachments/9188/14066/2305-dCache-CHEP23.pdf',
                  id: 14066,
                  title: '2305-dCache-CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11314,
            description:
              'At Brookhaven National Lab, the dCache storage management system is used as a disk cache for large high-energy physics (HEP) datasets primarily from the ATLAS experiment[1]. Storage space on dCache is considerably smaller than the full ATLAS data collection. Therefore, a policy is needed to determine what data files to keep in the cache and what files to evict. A good policy is to keep frequently needed files in the future. In this work, we use the current and past file access information to predict the number of file accesses in the next day. The analysis tasks from the ATLAS experiment often access a predefined dataset as a group. Therefore, this study predicts how many times a dataset will be accessed in the future rather than each individual file.\r\nHEP collaborations like ATLAS generate files in groups known as datasets and each of these groups (datasets) is produced by a task (such as an physical exepriement and a simulation) with a Task ID, or TID. The dCache system operators are considering policies specified in TIDs rather than individual files. For example, if a dataset (with a specific TID) is expected to be very popular in the next few days, it might make sense to ping all files of the dataset in disk.\r\nTo investigate how datasets tend to be accessed, we first performed K-means clustering on 9 months\u2019 worth of dCache operational logs. Figure 1 shows the results of clustering the datasets according to their present and next day access counts. The cluster corresponding to datasets with less than 10^4 accesses is extremely large, whereas the clusters corresponding to higher numbers of accesses are small. This indicates that the majority of datasets are accessed relatively few times, and that there is also a small number of highly popular datasets. Pinging the small group of very popular datasets in dCache would achieve our goal of a popularity-based cache policy.\r\n\u003cimg src="https://sdm.lbl.gov/students/chep23/kmeans_cluster_transparent.png"\u003e\r\n\u003ca href="https://sdm.lbl.gov/students/chep23/kmeans_cluster_transparent.png"\u003eFigure 1\u003c/a\u003e: K-means clustering with k=4. A small number of datasets are accessed much more frequently than others and their access counts might be predictable.\r\n\r\nThe neural network was trained using a dataset containing information for 9 months\u2019 worth of dCache transactions. We process the raw dCache logs into daily access statistics with the next day\u2019s access count as the target variable for learning. The neural network was built using PyTorch; it uses 2 dense layers, the Tanh activation function, and the ADAM optimizer.\r\n\u003cimg src="https://sdm.lbl.gov/students/chep23/top100_final_fit.png"\u003e\r\n\u003ca href="https://sdm.lbl.gov/students/chep23/top100_final_fit.png"\u003eFigure 2\u003c/a\u003e: Predicted vs. actual access counts in the next day. The 100 most popular datasets sorted according to their actual access counts.\r\n\r\nFigure 2 shows the predicted access values vs the actual access values for the 100 most popular datasets. The most popular dataset is accessed much more than the second most popular dataset, while the access counts of the next ten most popular datasets follow a power law with the exponent of -0.57, and an RMSE of 5.7 \u00d7 10^5. The access counts of many commonly accessed datasets follow the same power law show in Figure 2 for the majority of the top 100 popular datasets. This power law has an exponent of -0.47, and an RMSE of 2.0 \u00d7 10^6. This corroborates the pattern shown in Figure 1, where there is a small group of highly popular datasets, and their accesses are more predictable. Since the most popular few datasets are accessed much more frequently than others, pinging them in the disk cache could simplify the cache replacement decisions without sacrificing the overall disk cache effectiveness.\r\nIn summary, our results show that the popularity of the most popular datasets are predictable. It is therefore possible to ping these datasets in dCache, yielding a more effective cache policy. Future work will develop, simulate, and benchmark cache policies based off of the method presented here.\r\n\r\nThe PDF version of the extended abstract is available on \u003ca href="https://sdm.lbl.gov/students/chep23/CHEP23_dCache_ext_abstract.pdf"\u003ehttps://sdm.lbl.gov/students/chep23/CHEP23_dCache_ext_abstract.pdf\u003c/a\u003e',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 50,
            id: 'c16967',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11314/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lawrence Berkeley National Laboratory',
                displayOrderKey: [3, 'Wu, Kesheng'],
                emailHash: 'a042d6b43b8bfd624ccd2323e1c96aca',
                familyName: 'Wu',
                firstName: 'Kesheng',
                name: 'Kesheng Wu',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12117,
            sessionSlotId: 2687,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Understanding Data Access Patterns for dCache System',
            uniqueId: 'c16967',
            url: '/event/459/contributions/11314/',
          },
          c16968: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11308/attachments/9565/13884/ncsmith-chep2023-objectstores.pdf',
                  id: 13884,
                  title: 'ncsmith-chep2023-objectstores.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11308,
            description:
              'In this talk, we present a novel data format design that obviates the need for data tiers by storing individual event data products in column objects. The objects are stored and retrieved through Ceph S3 technology, and a companion metadata system handles tracking of the object lifecycle. Performance benchmarks of data storage and retrieval will be presented, along with scaling tests of the data and metadata system.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 611,
            id: 'c16968',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11308/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Smith, Nick'],
                emailHash: '0f0194268cd54afc83a39b62386f535a',
                familyName: 'Smith',
                firstName: 'Nick',
                name: 'Nick Smith',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12117,
            sessionSlotId: 2687,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'A Ceph S3 Object Data Store for HEP',
            uniqueId: 'c16968',
            url: '/event/459/contributions/11308/',
          },
          c16969: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11296/attachments/9634/14027/ATL-COM-SOFT-2023-016.pdf',
                  id: 14027,
                  title: 'ATL-COM-SOFT-2023-016.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11296,
            description:
              'Rucio is a software framework that provides scientific collaborations with the ability to organise, manage and access large volumes of data using customisable policies. The data can be spread across globally distributed locations and across heterogeneous data centres, uniting different storage and network technologies as a single federated entity. Rucio offers advanced features such as distributed data recovery or adaptive replication, and is highly scalable, modular, and extensible. Rucio has been originally developed to meet the requirements of the high-energy physics experiment ATLAS, and is being continuously extended to support LHC experiments and other diverse scientific communities. In recent years several R\u0026D projects in these communities have started to evaluate the integration of both private and commercially-provided cloud storage systems. As they are using Rucio, new functionality has been developed to make the integration as seamless as possible. In addition the underlying systems, FTS and GFAL/Davix, have been extended for these use cases. In this contribution we detail the technical aspects of this work. In particular the challenges when building a generic interface to self-hosted cloud storage such as MinIO or CEPH S3 Gateway, to established providers such as Google Cloud Storage and Amazon Simple Storage Service, as well as upcoming decentralised clouds such as SEAL. We will highlight aspects such as authentication and authorisation, direct and remote access, throughput and cost estimation, and give experiences on daily operations.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 192,
            id: 'c16969',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11296/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Lassnig, Mario'],
                emailHash: 'ef2db26b320ecf7a9b75abbaef349287',
                familyName: 'Lassnig',
                firstName: 'Mario',
                name: 'Mario Lassnig',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12117,
            sessionSlotId: 2687,
            startDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title:
              'Extending Rucio with modern cloud storage support: Experiences from ATLAS, SKA and ESCAPE',
            uniqueId: 'c16969',
            url: '/event/459/contributions/11296/',
          },
        },
        entryType: 'Session',
        friendlyId: 5,
        id: 's12117',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2035/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2035,
        sessionSlotId: 2687,
        slotTitle: 'Clouds \u0026 Caches',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffeddf',
        title: 'Track 1 - Data and Metadata Organization, Management and Access',
        uniqueId: 's12117',
        url: '/event/459/sessions/2035/',
      },
      s12118: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#6f390d',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Lassnig, Mario'],
            emailHash: 'ef2db26b320ecf7a9b75abbaef349287',
            familyName: 'Lassnig',
            firstName: 'Mario',
            name: 'Mario Lassnig',
          },
          {
            affiliation: 'University of California, San Diego',
            displayOrderKey: [0, 'Davila, Diego'],
            emailHash: 'be09e73b4230a27dffdb0d8305b8a1d0',
            familyName: 'Davila',
            firstName: 'Diego',
            name: 'Diego Davila',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16970: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11310/attachments/9209/13837/HTTP%20Rest%20API%20for%20tape%20file%20recalls%20CHEP.pdf',
                  id: 13837,
                  title: 'CHEP_2023_HTTP_Tape_REST_API.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11310,
            description:
              'The goal of the \u201cHTTP REST API for Tape\u201d project is to provide a simple, minimalistic and uniform interface to manage data transfers between Storage Endpoints (SEs) where the source file is on tape. The project is a collaboration between the developers of WLCG storage systems (EOS+CTA, dCache, StoRM) and data transfer clients (gfal2, FTS). For some years, HTTP has been growing in popularity as the preferred data transfer protocol between many WLCG SEs. However \u2014 unlike other protocols such as XRootD and SRM \u2014 HTTP does not include a method to stage files from tape to disk prior to transfer, forcing the use of workarounds such as hybrid protocols (different protocols used for the \u201cstage\u201d and \u201ctransfer\u201d parts of the operation). The HTTP REST API offers a simple and consistent solution, by extending the HTTP protocol to include staging operations. It provides clients with a common and consistent API across different storage systems to manage and monitor the disk and tape residency of stored files. In this contribution, we present the history and motivation of the HTTP REST API project, the specification of version 1 of the API and implementation details in the various storage and data transfer systems. We also describe our experiences of its deployment and use for LHC Run-3 operations. We conclude with a discussion of possible future work.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 201,
            id: 'c16970',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11310/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Davis, Michael'],
                emailHash: 'eacccc5e987e51543524c340e0572b0e',
                familyName: 'Davis',
                firstName: 'Michael',
                name: 'Michael Davis',
              },
              {
                affiliation: 'CERN',
                displayOrderKey: [2, 'Afonso, Joao'],
                emailHash: '53ab4e718e95ca4188c680fde4d4c321',
                familyName: 'Afonso',
                firstName: 'Joao',
                name: 'Mr Joao Afonso',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12118,
            sessionSlotId: 2688,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'An HTTP REST API for Tape-backed Storage',
            uniqueId: 'c16970',
            url: '/event/459/contributions/11310/',
          },
          c16972: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11291/attachments/9361/13798/chep2023_oral_sahn_v3.pdf',
                  id: 13798,
                  title: 'chep2023_oral_sahn_v3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11291,
            description:
              'CDS (Custodial Disk Storage), a disk-based custodial storage powered by CERN EOS storage system, has been operating for the ALICE experiment at the KISTI Tier-1 Centre since November 2021. The CDS replaced existing tape storage operated for almost a decade, after its stable demonstration in the WLCG Tape Challenges in October 2021. We tried to challenge the economy of tape storage in the aspects of data protection and storage capacity by purchasing cheap off-the-shelf disk enclosures and applying four-parity mode of EOS RAIN (Redundant Array of Independent Nodes) layout. In order to get a brief picture of the TCO (Total Cost of Ownership) of the CDS comparing with tape storage, we tried to conduct an estimation of acquisition, operation and other costs on both tape and disk-based custodial storages. A detailed discussion will be presented including capital and operating expenses over time for the custodial storages such as initial purchases and their expansions (with some predictions on the CDS), spaces, installation, software and hardware licenses, maintenance and power consumption.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 448,
            id: 'c16972',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11291/contribution.pdf',
            presenters: [
              {
                affiliation: 'KISTI',
                displayOrderKey: [1, 'Ahn, Sang Un'],
                emailHash: 'a2b664078ca7c28e3fcf55eef33c24fa',
                familyName: 'Ahn',
                firstName: 'Sang Un',
                name: 'Sang Un Ahn',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12118,
            sessionSlotId: 2688,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Challenging the economy of tape storage with the disk-based one',
            uniqueId: 'c16972',
            url: '/event/459/contributions/11291/',
          },
          c16973: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11307/attachments/9259/13428/ATLAS-Data-Carousel-CHEP2023.pdf',
                  id: 13428,
                  title: 'ATLAS-Data-Carousel-CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11307,
            description:
              'The High Luminosity upgrade to the LHC (HL-LHC) is expected to deliver scientific data at the multi-exabyte scale. In order to address this unprecedented data storage challenge, the ATLAS experiment launched the Data Carousel project in 2018. Data Carousel is a tape-driven workflow whereby bulk production campaigns with input data resident on tape are executed by staging and promptly processing a sliding window to disk buffer such that only a small fraction of inputs are pinned on disk at any one time. Data Carousel is now in production for ATLAS in Run3. In this paper, we will provide updates on recent Data Carousel R\u0026D projects, including data-on-demand and tape smart writing. Data-on-demand removes from disk data that has not been accessed for a predefined period, when users request them, they will be either staged from tape or recreated by following the original production steps. Tape smart writing employs intelligent algorithms for file placement on tape in order to retrieve data back more efficiently, which is our long term strategy to achieve optimal tape usage in Data Carousel.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 178,
            id: 'c16973',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11307/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory (US)',
                displayOrderKey: [13, 'Zhao, Xin'],
                emailHash: '29b5108512a99ebccd93ea64eca983ea',
                familyName: 'Zhao',
                firstName: 'Xin',
                name: 'Xin Zhao',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12118,
            sessionSlotId: 2688,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Updates to the ATLAS Data Carousel Project',
            uniqueId: 'c16973',
            url: '/event/459/contributions/11307/',
          },
          c16974: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11294/attachments/9644/14046/cback-chep2023.pptx',
                  id: 14046,
                  title: 'cback-chep2023.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11294,
            description:
              'The CERN IT Department is responsible for ensuring the integrity and security of data stored in the IT Storage Services. General storage backends such as EOSHOME/PROJECT/MEDIA and CEPHFS are used to store data for a wide range of use cases for all stakeholders at CERN, including experiment project spaces and user home directories.\r\n\r\nIn recent years a backup system, CBACK, was developed based on the open source backup program Restic. CBACK is currently used to backup all CERNBox data (about 18PB) stored on disks in the CERN Computing Centre to a disk based S3 service instance in the Prevessin Nethub.\r\n\r\nTo significantly increase the reliability and security of the backups and reduce the storage costs, by limiting the amount of data on disk, we have added a tape storage backend to CBACK. Tape backup also offers a better protection against potential ransomware attacks. To achieve this we have created a transparent interface to the CERN Tape Archive (CTA) service.\r\n\r\nWith this addition CBACK can reliably be extended to new use cases. CBACK has already been extended to backup CephFS Manila shares and in principle can be used to back up any local mountable file system, such as EOS, CephFS, NFS or DFS. Furthermore, CBACK can potentially serve as a replacement for certain use cases currently covered by IBM Spectrum Protect, including the backup of PCs belonging to individual users.\r\n\r\nIn this presentation we will describe the architecture and implementation of CBACK with the new tape storage backend and a number of developments planned for the near future.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 203,
            id: 'c16974',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11294/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Rademakers, Fons'],
                emailHash: '428c01fbcb7520555ed00f2b06246d16',
                familyName: 'Rademakers',
                firstName: 'Fons',
                name: 'Dr Fons Rademakers',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12118,
            sessionSlotId: 2688,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title:
              'Evolution of the CERN Backup system based on RESTIC and the CERN Tape Archive (CTA)',
            uniqueId: 'c16974',
            url: '/event/459/contributions/11294/',
          },
          c16975: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11316/attachments/9500/13923/CTA_Beyond_CERN.pdf',
                  id: 13923,
                  title: 'CTA_Beyond_CERN.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11316,
            description:
              'The CERN Tape Archive (CTA) was conceived as the successor to CASTOR and as the tape back-end to EOS, designed for the archival storage of data from LHC Run-3 and other experimental programmes at CERN. In the wider WLCG, the tape software landscape is quite heterogenous, but we are now entering a period of consolidation. This has led to a number of sites in WLCG (and beyond) reevaluating their options and choosing CTA for their future tape archival storage needs. However, the original mandate for CTA created a number of design constraints which are not necessarily optimal for other sites beyond CERN Tier-0. In this contribution, we discuss how the CTA team has engaged with the wider HEP community and collaborated on changes to the software to allow it to be adopted more widely. In particular, we describe community contributions to CTA to allow it to be used as the tape backend for dCache, and to allow migrations from other tape systems such as Enstore. In addition, we discuss improvements to the building and packaging of CTA to remove CERN-specific dependencies and to facilitate easy distribition to other sites, and describe the schema upgrade procedure for the CTA Catalogue database. Finally, we present a roadmap for the community edition of CTA.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 199,
            id: 'c16975',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11316/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Davis, Michael'],
                emailHash: 'eacccc5e987e51543524c340e0572b0e',
                familyName: 'Davis',
                firstName: 'Michael',
                name: 'Michael Davis',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12118,
            sessionSlotId: 2688,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title:
              'The CERN Tape Archive Beyond CERN \u2014 an Open Source Data Archival System for HEP',
            uniqueId: 'c16975',
            url: '/event/459/contributions/11316/',
          },
        },
        entryType: 'Session',
        friendlyId: 5,
        id: 's12118',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2035/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2035,
        sessionSlotId: 2688,
        slotTitle: 'Tapes',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffeddf',
        title: 'Track 1 - Data and Metadata Organization, Management and Access',
        uniqueId: 's12118',
        url: '/event/459/sessions/2035/',
      },
      s12119: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#6f390d',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Barisits, Martin'],
            emailHash: '46207806844c107a8fefe566f669b206',
            familyName: 'Barisits',
            firstName: 'Martin',
            name: 'Martin Barisits',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Lassnig, Mario'],
            emailHash: 'ef2db26b320ecf7a9b75abbaef349287',
            familyName: 'Lassnig',
            firstName: 'Mario',
            name: 'Mario Lassnig',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16976: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11297/attachments/9384/13608/CHEP23_Caching_Alloc.pdf',
                  id: 13608,
                  title: 'CHEP23_Fischer_Transferable_Caching_Alloc.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11297,
            description:
              'The development of an LHC physics analysis involves numerous investigations that require the repeated processing of terabytes of measured and simulated data. Thus, a rapid processing turnaround is beneficial to the scientific process. We identified two bottlenecks in analysis independent algorithms and developed the following solutions.\r\nFirst, inputs are now cached on individual SSD caches of each worker node. Here, cache efficiency and longevity is increased by a cache aware workload scheduling algorithm. Additionally, the algorithm is resilient against changes in workload composition and worker node allocation.\r\nSecond, the overall throughput is increased through tailored resource allocation, thus maximizing utilization. For this, the result aggregation, in particular of histograms, and the DNN evaluation are transparently offloaded to dedicated resources satisfying their unique demands. Consequently, the resource needs are homogenized for the primary workload.\r\nUsing these measures, a full-fledged LHC Run 2 analysis can be reprocessed from scratch within a few days on a small institute cluster of about 200 logical cores. The individual analysis parts, which are often repeated during development and debugging, have their runtime reduced from hours to minutes, with measured speed ups of up to 1490%. Finally, all these improvements readily carry over to other analyses within the same environment.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 377,
            id: 'c16976',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11297/contribution.pdf',
            presenters: [
              {
                affiliation: 'RWTH Aachen University (DE)',
                displayOrderKey: [0, 'Fischer, Benjamin'],
                emailHash: '48a39651315b6a1c1d0a8f69601dbe3b',
                familyName: 'Fischer',
                firstName: 'Benjamin',
                name: 'Benjamin Fischer',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12119,
            sessionSlotId: 2689,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title:
              'Transferable Improved Analyses Turnaround through Intelligent Caching and Optimized Resource Allocation',
            uniqueId: 'c16976',
            url: '/event/459/contributions/11297/',
          },
          c16977: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11320/attachments/9481/13743/chep_2023_metadata_BelleII.pdf',
                  id: 13743,
                  title: 'chep_2023_metadata_BelleII.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11320,
            description:
              'Rucio is a Data Management software that has become a de-facto standard in the HEP community and beyond. It allows the management of large volumes of data over their full lifecycle. The Belle II experiment located at KEK (Japan) recently moved to Rucio to manage its data over the coming decade (O(10) PB/year). In addition to its Data Management functionalities, Rucio also provides support for storing generic metadata. Rucio metadata already provides accurate accounting of the data stored all over the sites serving Belle II. Annotating files with generic metadata opens up possibilities for finer-grained metadata query support.\r\nWe will first introduce some of the new developments aimed at providing good performance that were done to cover Belle II use-cases like bulk insert methods, metadata inheritance, etc. We will then describe the various tests performed to validate Rucio generic metadata at Belle II scale (O(100M) files), detailing the import and performance tests that were made.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 241,
            id: 'c16977',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11320/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Mississippi',
                displayOrderKey: [1, 'Panta, Anil'],
                emailHash: 'c8c5a88fc8f62d8afa2848a0cc50daf3',
                familyName: 'Panta',
                firstName: 'Anil',
                name: 'Anil Panta',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12119,
            sessionSlotId: 2689,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Evaluation of Rucio as a Metadata Service for the Belle II experiment',
            uniqueId: 'c16977',
            url: '/event/459/contributions/11320/',
          },
          c16978: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11293/attachments/9353/13559/ATL-COM-SOFT-2023-043.pdf',
                  id: 13559,
                  title: 'ATL-COM-SOFT-2023-043.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11293,
            description:
              'The ATLAS experiment is preparing a major change in the conditions data infrastructure in view of Run4 In this presentation we will expose the main motivations for the new design (called CREST for Conditions-REST), the ongoing changes in the DB architecture and present the developments for the deployment of the new system. The main goal is to setup a parallel infrastructure for full scale testing before the end of Run3.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 185,
            id: 'c16978',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11293/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Sheffield (UK)',
                displayOrderKey: [5, 'Costanzo, Davide'],
                emailHash: '80c58a112072cae5dde268a613c0a09b',
                familyName: 'Costanzo',
                firstName: 'Davide',
                name: 'Davide Costanzo',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12119,
            sessionSlotId: 2689,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Conditions Evolution in ATLAS',
            uniqueId: 'c16978',
            url: '/event/459/contributions/11293/',
          },
          c16979: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11317/attachments/9334/13921/CCDB%20-%20CHEP%202023.pdf',
                  id: 13921,
                  title: 'CCDB - CHEP 2023.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11317/attachments/9334/13922/go',
                  id: 13922,
                  title: 'Google slides',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11317,
            description:
              'The ALICE experiment at CERN has undergone a substantial detector, readout and software upgrade for the LHC Run3. A signature part of the upgrade is the triggerless detector readout, which necessitates a real time lossy data compression from 1.1TB/s to 100GB/s performed on a GPU/CPU cluster of 250 nodes. To perform this compression, a significant part of the software, which traditionally is considered off-line, was moved to the front-end of the experiment data acquisition system, for example the detector tracking. This is the case also for the various configuration and conditions databases of the experiment, which are now replaced with a single homogeneous service, serving both the real-time compression, online data quality checks and the subsequent secondary data passes, Monte-Carlo simulation and data analysis.\r\n\r\nThe new service is called CCDB (for Calibration and Conditions Database). It receives, stores and distributes objects created from online detector calibration tasks and control systems, as well as objects created by offline workflows, using a homogeneous and universal metadata annotation schema, distributes them in real time to the Online cluster and replicates all content on Grid storage elements for Grid jobs or by collaboration members. The access to the metadata and objects is done via a REST API and a ROOT-based C++ client interface which streamlines the interaction with this service from compiled code while plain *curl* command line calls are a simple access alternative.\r\n\r\nIn this paper we will present the architecture and implementation details of the components that manage frequent updates of objects with millisecond-resolution intervals of validity and how we have achieved an independent operation of the Online cluster while also making all objects available to Grid computing nodes.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 27,
            id: 'c16979',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11317/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Grigoras, Costin'],
                emailHash: 'a03bb0b3b47a554f9eb69c2fca89dcb7',
                familyName: 'Grigoras',
                firstName: 'Costin',
                name: 'Costin Grigoras',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12119,
            sessionSlotId: 2689,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Calibration and Conditions Database of the ALICE experiment in Run 3',
            uniqueId: 'c16979',
            url: '/event/459/contributions/11317/',
          },
          c16980: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11326/attachments/9582/13907/conditions_db_reference_implementation_chep_2023.pptx',
                  id: 13907,
                  title: 'conditions_db_reference_implementation_chep_2023.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11326/attachments/9582/14015/conditions_db_reference_implementation_chep_2023.pdf',
                  id: 14015,
                  title: 'conditions_db_reference_implementation_chep_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11326,
            description:
              'The HSF Conditions Databases activity is a forum for cross-experiment discussions hoping for as broad a participation as possible.  It grew out of the HSF Community White Paper work to study conditions data access, where experts from ATLAS, Belle II, and CMS converged on a common language and proposed a schema that represents best practice.  The focus of the HSF work is the most difficult use case, specifically the subset of non-event data that are needed for distributed computing resources to process event data with access rates of up to 10k Hz.  Following discussions with a broader community, including NP as well as HEP experiments, a core set of use cases, functionality and behaviour was defined with the aim to describe a core Conditions Database API.  This contribution will describe the reference implementation of both the conditions database service and the client which together encapsulate HSF best practice conditions data handling.\r\n\r\nDjango was chosen for the service implementation, which uses an ORM instead of the direct use of SQL.  The simple relational database schema to organise conditions data is implemented in PostgreSQL.  The task of storing conditions data payloads themselves is outsourced to any POSIX-compliant filesystem, allowing for transparent relocation and redundancy.  Crucially this design provides a clear separation between retrieving the metadata describing which conditions data are needed for a data processing job, and retrieving the actual payloads from storage.  The deployment using helm on OKD will be described together with scaling tests and operations experience from the sPHENIX experiment running many 10k cores at BNL.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 526,
            id: 'c16980',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11326/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [0, 'Gerlach, Lino'],
                emailHash: 'e5aa3fa7fa5318ae6e8ee06d447fc10e',
                familyName: 'Gerlach',
                firstName: 'Lino',
                name: 'Lino Gerlach',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12119,
            sessionSlotId: 2689,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'The HSF Conditions Database reference implementation',
            uniqueId: 'c16980',
            url: '/event/459/contributions/11326/',
          },
          c16981: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11315/attachments/9543/13847/230518_CHEP23_ATLAS_EIMonit.pdf',
                  id: 13847,
                  title: '230518_CHEP23_ATLAS_EIMonit.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11315,
            description:
              'The ATLAS EventIndex is a global catalogue of the events collected, processed or generated by the ATLAS experiment. The system was upgraded in advance of LHC Run 3, with a migration of the Run 1 and Run 2 data from HDFS MapFiles to HBase tables with a Phoenix interface. The frameworks for testing functionality and performance of the new system have been developed. There are two types of tests running. First, the functional test that must check the correct functioning of the import chain. These tests run event picking over a random set of recently imported data to see if the data have been imported correctly, and can be accessed by both the CLI and the PanDA client. The second, the performance test, generates event lookup queries on sets of the EventIndex data and measures the response times. These tests enable studies of the response time dependence on the amount of requested data, and data sample type and size. Both types of tests run regularly on the existing system. The results of the regular tests as well as the statuses of the main EventIndex sub-systems (services health, loaders status, filesystem usage, etc.) are sent to InfluxDB in JSON format via HTTP requests and are displayed on Grafana monitoring dashboards. In case (part of) the system misbehaves or becomes unresponsive, alarms are raised by the monitoring system.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 261,
            id: 'c16981',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11315/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Oxford',
                displayOrderKey: [3, 'Gallas, Elizabeth'],
                emailHash: '98034d8e8a1509e7ffd1a61c9a276354',
                familyName: 'Gallas',
                firstName: 'Elizabeth',
                name: 'Dr Elizabeth Gallas',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12119,
            sessionSlotId: 2689,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Testing framework and monitoring system for the ATLAS EventIndex',
            uniqueId: 'c16981',
            url: '/event/459/contributions/11315/',
          },
        },
        entryType: 'Session',
        friendlyId: 5,
        id: 's12119',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2035/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2035,
        sessionSlotId: 2689,
        slotTitle: 'Databases \u0026 Metadata',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffeddf',
        title: 'Track 1 - Data and Metadata Organization, Management and Access',
        uniqueId: 's12119',
        url: '/event/459/sessions/2035/',
      },
      s12121: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d0c296',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Shahoyan, Ruben'],
            emailHash: '1db06f137fc13354d671d2e50bcdf999',
            familyName: 'Shahoyan',
            firstName: 'Ruben',
            name: 'Ruben Shahoyan',
          },
          {
            affiliation: 'Universite de Geneve (CH)',
            displayOrderKey: [0, 'Antel, Claire'],
            emailHash: '0700a8e2f8e6dd930897159bde503e7d',
            familyName: 'Antel',
            firstName: 'Claire',
            name: 'Claire Antel',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17006: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11370/attachments/9535/13827/LHCb%20Online%20Monitoring.pdf',
                  id: 13827,
                  title: 'LHCb Online Monitoring.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11370,
            description:
              'The LHCb experiment started taking data with an upgraded detector in the Run 3 of the LHC, reading at 30 MHz full detector data with a software-only trigger system. In this context, live data monitoring is crucial to ensure that the quality of the recorded data is optimal. Data from the experiment control system, as well as raw detector data and the output of the software trigger, is used as input to the monitoring system. Plots of important quantities are delivered to the so called data manager shifter, present permanently in the control room. In addition, the same system is used to evaluate the quality of the data for future physics analyses. The presentation will describe the tools involved in the online monitoring: the infrastructure, the data processing, the plot visualisation applications and the various reporting tools available for experts to diagnose and follow problems arising during data taking.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 219,
            id: 'c17006',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11370/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [2, 'Matev, Rosen'],
                emailHash: 'e7515c1322c1eef189feb2943dc4e65c',
                familyName: 'Matev',
                firstName: 'Rosen',
                name: 'Rosen Matev',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12121,
            sessionSlotId: 2690,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'LHCb Online Monitoring',
            uniqueId: 'c17006',
            url: '/event/459/contributions/11370/',
          },
          c17007: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11372/attachments/9548/13852/OMS-AP-CHEP2023-2023-05-09.pdf',
                  id: 13852,
                  title: 'OMS-AP-CHEP2023-2023-05-09.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11372,
            description:
              'The CMS Online Monitoring System (OMS) aggregates and integrates different sources of information into a central place and allows users to view, compare and correlate information. It displays real-time and historical information.\r\nThe tool is heavily used by run coordinators, trigger experts and shift crews, to achieve optimal trigger and efficient data taking. It provides aggregated information for many use cases including data certification.\r\nOMS is the successor of WBM, the Web Based Monitoring which was in use during Run 1 and Run 2 of the LHC. \r\nWBM started as a small tool and grew substantially over the years so that maintenance became challenging. OMS was developed from scratch following several design ideas: to strictly separate the presentation layer from the data aggregation layer; to use a well-defined standard for the communication between presentation layer and aggregation layer; and to employ widely used frameworks from outside the HEP community.\r\n\r\nA report on our experience of the operation of OMS for the first year of data taking of Run 3 in 2022 is presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 501,
            id: 'c17007',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11372/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of California, San Diego, San Diego, California, USA',
                displayOrderKey: [1, 'Petrucci, Andrea'],
                emailHash: '1951a3ffe6d5cf1393f669f572be4751',
                familyName: 'Petrucci',
                firstName: 'Andrea',
                name: 'Mr Andrea Petrucci',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12121,
            sessionSlotId: 2690,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'First year of experience with the new operational monitoring tool for data taking in CMS during Run 3',
            uniqueId: 'c17007',
            url: '/event/459/contributions/11372/',
          },
          c17008: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11393/attachments/9337/13538/CHEP_2023_Jeske.pptx',
                  id: 13538,
                  title: 'CHEP_2023_Jeske.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11393,
            description:
              'Hydra is a system which utilizes computer vision to monitor data quality in near real time.  Currently, it is deployed in all of Jefferson Lab\u2019s experimental halls and lightens the load on shift takers by autonomously monitoring diagnostic plots in near real time.  Hydra is constructed from \u201coff-the-shelf\u201d technologies and is backed up by a full MySQL database.  To aid with both labeling and monitoring of Hydra\u2019s inference, web apps have been developed, lowering the barrier to entry.  Hydra connects with the EPICS alarm system and includes full history recording. This has enabled it to spot issues that were missed by shift takers. When issues are spotted a natural first question is \u201cWhy does Hydra think there is a problem?\u201d.  To answer this question Hydra uses gradCAM to highlight regions of an image which are most important in inference classification.  This provides a level of interpretability/trustworthiness; essential for any operational system in science. This talk will describe the Hydra system, the technologies employed as well as in situ results.  The sociological hurdles in deploying such a system will also be discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 495,
            id: 'c17008',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11393/contribution.pdf',
            presenters: [
              {
                affiliation: 'JLAB',
                displayOrderKey: [2, 'Jeske, Torri'],
                emailHash: '79b6b0afab0c3b392720824035597726',
                familyName: 'Jeske',
                firstName: 'Torri',
                name: 'Torri Jeske',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12121,
            sessionSlotId: 2690,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Computer Vision for Data Quality Monitoring with Hydra',
            uniqueId: 'c17008',
            url: '/event/459/contributions/11393/',
          },
          c17009: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11385/attachments/9486/13869/2023_05_13_qc_chep.pdf',
                  id: 13869,
                  title: '2023_05_13_qc_chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11385,
            description:
              'ALICE (A Large Ion Collider Experiment) has undertaken a major upgrade during the Long Shutdown 2. The increase in the detector data rates, and in particular the continuous readout of the TPC, led to a hundredfold increase in the input raw data, up to 3.5 TB/s. To cope with it, a new common Online and Offline computing system, called O2, has been developed and put in production.\r\n\r\nThe online Data Quality Monitoring (DQM) and the offline Quality Assurance (QA) are critical aspects of the data acquisition and reconstruction software chains. The former intends to provide shifters with precise and complete information to quickly identify and overcome problems while the latter aims at selecting good quality data for physics analyses. Both DQM and QA typically involve the gathering of data, its distributed analysis by user-defined algorithms, the merging of the resulting objects and their visualization.\r\n\r\nThis paper discusses the final architecture and design of the QC, which runs synchronously to the data taking and asynchronously on the Worldwide LHC Computing Grid. Following the successful first year of data taking with beam, we will present our experience and the lessons we learned, before and after the LHC restart, when monitoring the data quality in a real-world and challenging environment. We will finally illustrate the wide range of usages people make of this system by presenting a few, carefully picked, use cases.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 73,
            id: 'c17009',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11385/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Konopka, Piotr'],
                emailHash: '21c992073e7b15c7384475e18c1c9fb4',
                familyName: 'Konopka',
                firstName: 'Piotr',
                name: 'Piotr Konopka',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12121,
            sessionSlotId: 2690,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'The ALICE Data Quality Control',
            uniqueId: 'c17009',
            url: '/event/459/contributions/11385/',
          },
          c17010: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11374/attachments/9497/13764/AIEC_CHEP_2023.pdf',
                  id: 13764,
                  title: 'AIEC_CHEP_2023.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11374/attachments/9497/13765/AIEC_CHEP_2023.pptx',
                  id: 13765,
                  title: 'AIEC_CHEP_2023.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11374,
            description:
              'One critical step on the path from data taking to physics analysis is calibration.  For many experiments this step is both time consuming and computationally expensive. The AI Experimental Calibration and Control project seeks to address these issues, starting first with the GlueX Central Drift Chamber (CDC). We demonstrate the ability of a Gaussian Process to estimate the gain correction factor (GCF) of the GlueX CDC accurately, and also the uncertainty of this estimate.  Using the estimated GCF, the developed system infers a new high voltage (HV) setting that stabilizes the GCF in the face of changing environmental conditions. This happens in near real time during data taking and produces data which are already approximately gain-calibrated, eliminating the cost of performing those calibrations which vary by up to 30% with fixed HV.  We also demonstrate an implementation of an uncertainty aware system which exploits a key feature of a Gaussian process.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 346,
            id: 'c17010',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11374/contribution.pdf',
            presenters: [
              {
                affiliation: 'JLab',
                displayOrderKey: [1, 'Britton, Thomas'],
                emailHash: '821d660cde7c04e24d91f9173ee4f516',
                familyName: 'Britton',
                firstName: 'Thomas',
                name: 'Thomas Britton',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12121,
            sessionSlotId: 2690,
            startDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'AI Driven Experiment Calibration and Control',
            uniqueId: 'c17010',
            url: '/event/459/contributions/11374/',
          },
          c17312: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11409/attachments/9379/13601/CHEP2023Slides_MAmerl_TheATLASJetTriggerInRun3.pdf',
                  id: 13601,
                  title: 'CHEP2023Slides_MAmerl_TheATLASJetTriggerInRun3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11409,
            description:
              'The ATLAS jet trigger is instrumental in selecting events both for Standard Model measurements and Beyond the Standard Model physics searches. Non-standard triggering strategies, such as saving only a small fraction of trigger objects for each event, avoids bandwidth limitations and increases sensitivity to low-mass and low-momentum objects. These events are used by Trigger Level Analyses, which can reach regions of parameter space that would otherwise be inaccessible. To this end, the calibration of trigger-level jets is imperative both to ensure good trigger performance across the ATLAS physics programme and to provide well-measured jets for Trigger Level Analysis. This contribution presents an introduction to the ATLAS jet trigger for Run-3 of the LHC and discusses the performance of the trigger jet calibration. These studies will allow us to commission a Run-3 trigger jet calibration that provides excellent performance across a broad jet transverse momentum range as low as 25 GeV.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 295,
            id: 'c17312',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11409/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'Amerl, Maximilian'],
                emailHash: 'ed6644d18353cd1c3bdae60ebaf7e54b',
                familyName: 'Amerl',
                firstName: 'Maximilian',
                name: 'Maximilian Amerl',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12121,
            sessionSlotId: 2690,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'The Run-3 ATLAS jet trigger',
            uniqueId: 'c17312',
            url: '/event/459/contributions/11409/',
          },
        },
        entryType: 'Session',
        friendlyId: 6,
        id: 's12121',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2036/session-timetable.pdf',
        room: 'Marriott Ballroom V-VI',
        sessionCode: '',
        sessionId: 2036,
        sessionSlotId: 2690,
        slotTitle: 'DQ, Monitoring \u0026 Control',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#000000',
        title: 'Track 2 - Online Computing',
        uniqueId: 's12121',
        url: '/event/459/sessions/2036/',
      },
      s12123: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d0c296',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'KEK',
            displayOrderKey: [0, 'Yamada, Satoru'],
            emailHash: 'f2ae8a15fd4a3fafb55ff71831b9c00c',
            familyName: 'Yamada',
            firstName: 'Satoru',
            name: 'Satoru Yamada',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Pantaleo, Felice'],
            emailHash: '27ffa3f8059859baeebd18c023b215a8',
            familyName: 'Pantaleo',
            firstName: 'Felice',
            name: 'Felice Pantaleo',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17011: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11399/attachments/9632/14023/PhilipChang20230508_LST_CHEP2023Draft_v4.pdf',
                  id: 14023,
                  title: 'PhilipChang20230508_LST_CHEP2023Draft_v4.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11399,
            description:
              'The Large Hadron Collider (LHC) will be upgraded to High-luminosity LHC, increasing the number of simultaneous proton-proton collisions (pile-up, PU) by several-folds. The harsher PU conditions lead to exponentially increasing combinatorics in charged-particle tracking, placing a large demand on the computing resources. The projection on required computing resources exceeds the computing budget with the current algorithms running on single-thread CPUs. Motivated by the rise of heterogeneous computing in high-performance computing centers, we present Line Segment Tracking (LST), a highly parallelizeable algorithm that can run efficiently on GPUs and has been integrated to the CMS experiment central software. The usage of Alpaka framework for the algorithm implementation allows better portability of the code to run on different types of commercial parallel processors allowing flexibility on which processors to purchase for the experiment in the future. To verify a similar computational performance with a native solution, the alpaka implementation is compared with a cuda one on a NVIDIA Tesla V100 GPU. The algorithm creates short track segments in parallel, and progressively form higher level objects by linking segments that are consistent with genuine physics track hypothesis. The computing and physics performance are on par with the latest, multi-CPU versions of existing CMS tracking algorithms.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 289,
            id: 'c17011',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11399/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Florida',
                displayOrderKey: [11, 'Chang, Philip'],
                emailHash: '8351743a546e3a2fd839a750b4fa13da',
                familyName: 'Chang',
                firstName: 'Philip',
                name: 'Philip Chang',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12123,
            sessionSlotId: 2692,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Line Segment Tracking in the High-luminosity LHC',
            uniqueId: 'c17011',
            url: '/event/459/contributions/11399/',
          },
          c17012: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11377/attachments/9485/13750/CHEP2023_Lazzari_V2.pdf',
                  id: 13750,
                  title: 'CHEP2023_Lazzari_V2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11377,
            description:
              'The LHCb experiment is currently taking data with a completely renewed DAQ system, capable for the first time of performing a full real-time reconstruction of all collision events occurring at LHC point 8.\r\nThe Collaboration is now pursuing a further upgrade (LHCb "Upgrade-II"), to enable the experiment to retain the same capability at luminosities an order of magnitude larger than the maximum planned for the current Run 3. To this purpose, a vigorous R\u0026D program is ongoing to boost the real-time processing capability of LHCb, needed to cope both with the luminosity increase and the adoption of correspondingly more granular and complex detectors.\r\nNew heterogeneous computing solutions are being explored, with the aim of moving reconstruction and data reduction to the earliest possible stages of processing. In this talk we describe the results obtained from a realistic demonstrator for a high-throughput reconstruction of tracking detectors, operated parasitically on real LHCb data from Run 3 in a purposedly built testbed facility. This demonstrator is based on a extremely parallel, \u0027artificial retina\u0027 architecture, implemented in commercial, PCIe-hosted FPGA cards interconnected by fast optical links, and encompasses a sizable fraction of the LHCb VELO pixel detector. The implications of the results in view of potential applications in HEP are discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 557,
            id: 'c17012',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11377/contribution.pdf',
            presenters: [
              {
                affiliation: 'Pisa University \u0026 INFN',
                displayOrderKey: [0, 'Lazzari, Federico'],
                emailHash: '407af292b8278f432cc8ff42efee27ec',
                familyName: 'Lazzari',
                firstName: 'Federico',
                name: 'Federico Lazzari',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12123,
            sessionSlotId: 2692,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Demonstration of track reconstruction with FPGAs on live data at LHCb',
            uniqueId: 'c17012',
            url: '/event/459/contributions/11377/',
          },
          c17013: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11375/attachments/9434/13680/CHEP_GNNonFPGAforEFTracking.pdf',
                  id: 13680,
                  title: 'CHEP_GNNonFPGAforEFTracking.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11375,
            description:
              'The High-Luminosity LHC (HL-LHC) will provide an order of magnitude increase in integrated luminosity and enhance the discovery reach for new phenomena. The increased pile-up foreseen during the HL-LHC necessitates major upgrades to the ATLAS detector and trigger. The Phase-II trigger will consist of two levels, a hardware-based Level-0 trigger and an Event Filter (EF) with tracking capabilities. Within the Trigger and Data Acquisition group, a heterogeneous computing farm consisting of CPUs and potentially GPUs and/or FPGAs is under study, together with the use of modern machine learning algorithms such as Graph Neural Networks (GNNs).\r\n\r\nGNNs are a powerful class of geometric deep learning methods for modelling spatial dependencies via message passing over graphs. They are well-suited for track reconstruction tasks by learning on an expressive structured graph representation of hit data and considerable speedup over CPU-based execution is possible on FPGAs.\r\n\r\nThe focus of this talk is a study of track reconstruction for the Phase-II EF system using GNNs on FPGAs. We explore each of the steps in a GNN-based EF tracking pipeline: graph construction, edge classification using an interaction network, and track reconstruction. Several methods and hardware platforms are under evaluation, studying resource utilisation and minimization of model size using quantization aware training, while simultaneously retaining high track reconstruction efficiency and low fake rates required for the EF tracking system.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 296,
            id: 'c17013',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11375/contribution.pdf',
            presenters: [
              {
                affiliation: 'Heidelberg University',
                displayOrderKey: [0, 'Dittmeier, Sebastian'],
                emailHash: '453333ace5bdad7ed0ccc8b107dd7823',
                familyName: 'Dittmeier',
                firstName: 'Sebastian',
                name: 'Sebastian Dittmeier',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12123,
            sessionSlotId: 2692,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Track reconstruction for the ATLAS Phase-II HLT using GNNs on FPGAs',
            uniqueId: 'c17013',
            url: '/event/459/contributions/11375/',
          },
          c17014: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11383/attachments/9503/13777/CHEP2023_v3.pdf',
                  id: 13777,
                  title: 'CHEP2023_v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11383,
            description:
              'During the LHC Run 3, the significant upgrades on many detectors and a brand new reconstruction software allows the ALICE experiment to record Pb-Pb collisions at an interaction rate of 50 kHz in a trigger-less continuous readout mode.\r\n\r\nThe key to process the 1TB/s peak data rate in ALICE is the usage of GPUs. There are two main data processing phases: the synchronous phase, where the TPC reconstruction uses most computing resources, and the asynchronous one, where more GPU resources are available.\r\n\r\nOther detectors are aiming at profiting from this computing potential to offload their reconstructions on graphics cards.\r\n\r\nIn this talk, we illustrate how we successfully ported on GPU both the primary vertex finding and the track reconstruction of the silicon tracker of ALICE, the Inner Tracking System.\r\n\r\nWe implemented, integrated, and natively supported the ITS code using automatic code generation on two different GPU brands with a single code base.\r\n\r\nThe implementation details, performance, and how this technique can be easily used in other applications will be discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 331,
            id: 'c17014',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11383/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Concas, Matteo'],
                emailHash: '47f3908963a2607c6713274892c478b9',
                familyName: 'Concas',
                firstName: 'Matteo',
                name: 'Matteo Concas',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12123,
            sessionSlotId: 2692,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'A vendor-unlocked ITS GPU reconstruction in ALICE',
            uniqueId: 'c17014',
            url: '/event/459/contributions/11383/',
          },
          c17015: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11405/attachments/9298/14040/Allen%20Calo%20Reco%20-%20CHEP23%20(3).pdf',
                  id: 14040,
                  title: 'Allen Calo Reco - CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11405,
            description:
              'The LHCb experiment has recently started a new period of data taking after a major upgrade in both software and hardware. One of the biggest challenges has been the migration of the first part of the trigger system (HLT1) into a parallel GPU architecture framework called Allen, which performs a partial reconstruction of most of the LHCb sub-detectors. In Allen, the reconstruction of the Electromagnetic Calorimeter (ECAL) sub-detector is used in many selection algorithms, but its efficiency is currently 10% lower than the full reconstruction performed in the second stage of the trigger. In this talk, we present a preliminary performance study of an alternative ECAL reconstruction algorithm implemented in Allen that complements the current algorithm to maximise the reconstruction efficiency and also minimise the impact on the throughput rate.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 200,
            id: 'c17015',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11405/contribution.pdf',
            presenters: [
              {
                affiliation: 'La Salle URL',
                displayOrderKey: [1, 'Valls Canudas, N\u00faria'],
                emailHash: '65afd8b0f762a05ee9c9ab3475773099',
                familyName: 'Valls Canudas',
                firstName: 'N\u00faria',
                name: 'N\u00faria Valls Canudas',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12123,
            sessionSlotId: 2692,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'Preliminary Performance Study of an Alternative Calorimeter Clustering Solution for Allen in LHCb',
            uniqueId: 'c17015',
            url: '/event/459/contributions/11405/',
          },
          c17016: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11382/attachments/9444/13693/CHEP2023_triggerlessDaqNPLM.pdf',
                  id: 13693,
                  title: 'CHEP2023_triggerlessDaqNPLM.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11382,
            description:
              'The sensitivity of modern HEP experiments to New Physics (NP) is limited by the hardware-level triggers used to select data online, resulting in a bias in the data collected. The deployment of efficient data acquisition systems integrated with online processing pipelines is instrumental to increase the experiments\u0027 sensitivity to the discovery of any anomaly or possible signal of NP. In designing such systems the combination of heterogeneous processing elements, including FPGAs and GPUs, is foreseen to sustain the large throughput of raw data from the detectors.\r\nIn this work, we present the first implementation of an end-to-end infrastructure that acquires continuously data from an experimental setup and processes it online looking for statistical anomalies using a Machine Learning (ML) technique. The infrastructure is deployed at the INFN Legnaro National Laboratory (LNL) and reads out data from a reduced-sized version of the drift tube muon detector of the CMS experiment at CERN. The data stream is first processed by an FPGA to cluster signals associated with the passage of a muon through the detector and produce candidate stubs.  Candidate events are then reconstructed and all muon hits and the reconstructed muon stubs are analyzed online by an algorithm deployed on a GPU to perform unbiased data exploration and statistical anomaly detection. The New Physics Learning Machine (NPLM) technique is used to evaluate the compatibility between incoming batches of experimental data and a reference sample representing the normal behavior of the data. In the specific case of the LNL test stand, the NPLM algorithm uses as a reference sample a dataset gathered in nominal detector conditions; data deviations from the normal behavior, if detected, are characterized and then mapped to known sources of detector malfunctioning with some degree of confidence. Unexpected behaviors, that might signal the presence of New Physics, can be singled out if the observed discrepancy doesn\u0027t match any of the expected anomalies. The system is currently dealing with the limited throughput originated by the cosmic muon flux; nevertheless, all components of the readout chain are designed to scale up and be eventually employed in experiments at the LHC.\r\nIn this contribution, we describe the technical implementation of the online processing pipeline and assess the performance of its most critical components.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 400,
            id: 'c17016',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11382/contribution.pdf',
            presenters: [
              {
                affiliation: 'Padova University \u0026 INFN',
                displayOrderKey: [2, 'Migliorini, Matteo'],
                emailHash: '34c0b7b99f9e405cb15d11f24b51476d',
                familyName: 'Migliorini',
                firstName: 'Matteo',
                name: 'Matteo Migliorini',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12123,
            sessionSlotId: 2692,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'Triggerless data acquisition pipeline for Machine Learning based statistical anomaly detection',
            uniqueId: 'c17016',
            url: '/event/459/contributions/11382/',
          },
        },
        entryType: 'Session',
        friendlyId: 6,
        id: 's12123',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2036/session-timetable.pdf',
        room: 'Marriott Ballroom V-VI',
        sessionCode: '',
        sessionId: 2036,
        sessionSlotId: 2692,
        slotTitle: 'Accelerated Computing \u0026 Artificial Intelligence',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#000000',
        title: 'Track 2 - Online Computing',
        uniqueId: 's12123',
        url: '/event/459/sessions/2036/',
      },
      s12124: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d9dfc3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Chulalongkorn University',
            displayOrderKey: [0, 'Srpimanobhas, Norraphat'],
            emailHash: '545290bb86b1edf24b84a6682630eadc',
            familyName: 'Srpimanobhas',
            firstName: 'Norraphat',
            name: 'Dr Norraphat Srpimanobhas',
          },
          {
            affiliation: 'University of Pittsburgh',
            displayOrderKey: [0, 'Bandieramonte, Marilena'],
            emailHash: 'f4b45dbfcfef5c90c1b484b28e777c71',
            familyName: 'Bandieramonte',
            firstName: 'Marilena',
            name: 'Marilena Bandieramonte',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17059: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11423/attachments/9492/13758/CMS%20Tier-0%20data%20processing%20during%20the%20detector%20commissioning%20in%20Run-3.pdf',
                  id: 13758,
                  title: 'CMS Tier-0 data processing during the detector commissioning in Run-3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11423,
            description:
              'The CMS Tier-0 service is responsible for the prompt processing and distribution of the data collected by the CMS Experiment. A number of upgrades were implemented during the long shutdown of the Large Hadron Collider, which improved the performance and reliability of the service. We report our experience of the data taking during Run-3 detector commissioning as well as performance of the system with respect to Run-2.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 126,
            id: 'c17059',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11423/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Amado, Jhonatan'],
                emailHash: '5b07c99372d2b02dda156c804cab0fe5',
                familyName: 'Amado',
                firstName: 'Jhonatan',
                name: 'Jhonatan Amado',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12124,
            sessionSlotId: 2693,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'CMS Tier0 data processing during the detector commissioning in Run-3',
            uniqueId: 'c17059',
            url: '/event/459/contributions/11423/',
          },
          c17060: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11441/attachments/9366/13945/JUNOSW_20230509-v1.pdf',
                  id: 13945,
                  title: 'JUNOSW_20230509-v1.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11441/attachments/9366/13946/JUNOSW_20230509-v1.pptx',
                  id: 13946,
                  title: 'JUNOSW_20230509-v1.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11441,
            description:
              '(on behalf of the JUNO Collaboration)\r\n\r\nJiangmen Underground Neutrino Observatory (JUNO), under construction in southern China, is a multi-purpose neutrino experiment designed to determine the neutrino mass hierarchy and precisely measure oscillation parameters. Equipped with a 20-kton liquid scintillator central detector viewed by 17,612 20-inch and 25,6000 3-inch photomultiplier tubes, JUNO could reach the unprecedented energy resolution of 3% at 1 MeV.\r\n\r\nJUNO is expected to start data taking in 2024 and plans to run for more than 20 years with about 2 petabytes of raw data each year. The large volume of data has brought a great challenge to the JUNO offline data processing and analysis.\r\n\r\nThis contribution will comprehensively review the development of JUNO offline software (JUNOSW) which started in 2012 in order to support JUNO\u2019s specific requirements, and will particularly highlight the following topics:\r\n\r\n 1\uff09 Data processing framework which supports buffering and management of multiple events, event splitting and mixing, TBB-based multi-threading, and integration of machine learning etc.\r\n 2\uff09Unified detector geometry management to support multiple applications including simulation, calibration, reconstruction and detector visualization.\r\n 3\uff09ROOT based event data model charactering data representations at different processing stages and complicated relationships between them.\r\n 4\uff09Event index based correlation analysis to support selection of sparse physics events from the large volume of data.\r\n\r\nThe JUNO data processing and analysis chain was completed and has been used by several rounds of Monte Carlo data challenge on both local computing clusters and the distributed computing infrastructure.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 176,
            id: 'c17060',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11441/contribution.pdf',
            presenters: [
              {
                affiliation: 'Shandong University',
                displayOrderKey: [0, 'HUANG, Xingtao'],
                emailHash: 'e1e2a24dff8c78d2c078091f397fc033',
                familyName: 'HUANG',
                firstName: 'Xingtao',
                name: 'Xingtao HUANG',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12124,
            sessionSlotId: 2693,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'JUNO Offline Software for Data Processing and Analysis',
            uniqueId: 'c17060',
            url: '/event/459/contributions/11441/',
          },
          c17061: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11413/attachments/9605/13956/20230511%20HLT%20migration%20to%20glideins%20CHEP23.pdf',
                  id: 13956,
                  title: '20230511 HLT migration to glideins CHEP23.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11413/attachments/9605/15240/CHEP_2023_SI_HLT_for_offline.pdf',
                  id: 15240,
                  title: 'CHEP_2023_SI_HLT_for_offline.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11413,
            description:
              'The former CMS Run 2 High Level Trigger (HLT) farm is one of the largest contributors to CMS compute resources, providing about 30k job slots for offline computing. The role of this farm has been evolving, from an opportunistic resource exploited during inter-fill periods in the LHC Run 2, to a nearly transparent extension of the CMS capacity at CERN during LS2 and into the LHC Run 3 started in 2022. This \u201cpermanent cloud\u201d is located on-site at the LHC interaction point 5, where the CMS detector is installed. As a critical example, the execution of Tier 0 tasks, such as prompt detector data reconstruction, has been fully commissioned. This resource can therefore be used in combination with the dedicated Tier 0 capacity at CERN, in order to process and absorb peaks in the stream of data coming from the CMS detector, as well as contributing to the prompt reconstruction of a substantial fraction of the \u201cparked data sample\u201d, dedicated primarily to B physics studies. The initial deployment model for this resource, based on long-lived statically configured VMs, including HTCondor execution node services connected to the CMS Submission Infrastructure (SI), provided the required level of functionality to enable its exploitation for offline computing. However, this configuration presented certain limitations in its flexibility of use in comparison to pilot-based resource acquisition at the WLCG sites. For example, slot defragmentation techniques were required to enable matching of Tier 0 multicore jobs. Additionally, the configuration of fair-share quotas and priorities for the diverse CMS tasks could not be directly managed by the CMS SI team, in charge of enforcing the global CMS resource provisioning and exploitation policies. A new configuration of this permanent cloud has been proposed in order to solve these shortcomings. A vacuum-like model, based on GlideinWMS pilot jobs joining the CMS CERN HTCondor Pool has been prototyped and successfully tested and deployed. This contribution will describe this redeployment work on the permanent cloud for an enhanced support to CMS offline computing, comparing the former and new models\u2019 respective functionalities, along with the commissioning effort for the new setup.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 217,
            id: 'c17061',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11413/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of California San Diego',
                displayOrderKey: [1, 'Mascheroni, Marco'],
                emailHash: '91b6e0458cfc38c6e2b8701f5a7b9a24',
                familyName: 'Mascheroni',
                firstName: 'Marco',
                name: 'Marco Mascheroni',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12124,
            sessionSlotId: 2693,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Repurposing of the Run 2 CMS High Level Trigger Infrastructure as an Cloud Resource for Offline Computing',
            uniqueId: 'c17061',
            url: '/event/459/contributions/11413/',
          },
          c17062: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11430/attachments/9570/14057/oscar%20chep2023.pdf',
                  id: 14057,
                  title: 'oscar chep2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11430,
            description:
              'The Super Tau Charm Facility (STCF) proposed in China is a new-generation electron\u2013positron collider with center-of-mass energies covering 2-7 GeV and a peak luminosity of 5*10^34 cm^-2s^-1. The offline software of STCF (OSCAR) is developed to support the offline data processing, including detector simulation, reconstruction, calibration as well as physics analysis. To meet STCF\u2019s specific requirements, OSCAR is designed and developed based on the SNiPER framework, a lightweight common software for HEP experiments. Besides the commonly used software such as Geant4 and ROOT, several state-of-art software and tools in the HEP community are incorporated as well, such as the Detector Description Toolkit (DD4hep), the plain-old-data I/O (Podio) and Intel Thread Building Blocks (TBB) etc.\r\n\r\nThis contribution will present the overall design of OSCAR, and in particular, the following topics will be highlighted.\r\n1. The design of the Event Data Model based on Podio, and the implementation of the data management system, via the integration of Podio and SNiPER.\r\n2. The parallelized data processing based on SNiPER and TBB, and in particular, the design of GlobalStore based on the Podio EventStore to support concurrent data access and data I/O.\r\n3. The geometry management system based on DD4hep that provides consistent geometry for detector simulation, calibration, reconstruction and detector visualization.\r\n4. Automated software validation system that supports validation at multiple levels ranging from unit test to physical validation.\r\n\r\nCurrently, OSCAR is fully functioning to facilitate the conceptual design of the STCF detector and the physics potential study. Meanwhile, OSCAR can also provide a potential solution for other lightweight HEP experiments as well.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 265,
            id: 'c17062',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11430/contribution.pdf',
            presenters: [
              {
                affiliation: 'Shandong University',
                displayOrderKey: [1, 'Li, Teng'],
                emailHash: 'c6c302428ff4c73ceb3723fea91da1c6',
                familyName: 'Li',
                firstName: 'Teng',
                name: 'Teng Li',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12124,
            sessionSlotId: 2693,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Offline Data Processing Software for the Super Tau Charm Facility',
            uniqueId: 'c17062',
            url: '/event/459/contributions/11430/',
          },
          c17063: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11445/attachments/9489/13755/Kirby_Status_of_DUNE_Offline_CHEP_2023.pdf',
                  id: 13755,
                  title: 'Kirby_Status_of_DUNE_Offline_CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11445,
            description:
              'We summarize the status of Deep Underground Neutrino Experiment (DUNE) software and computing development. We describe plans for the computing infrastructure needed to acquire, catalog, reconstruct, simulate and analyze the data from the DUNE experiment and its prototypes in pursuit of the experiment\u0027s physics goals of precision measurements of neutrino oscillation parameters, detection of astrophysical neutrinos, measurement of neutrino interaction properties and searches for physics beyond the Standard Model. In contrast to traditional HEP computational problems, DUNE\u0027s Liquid Argon TPC data consist of simple but very large (many GB) data objects which share many characteristics with astrophysical images. We have successfully reconstructed and simulated data from 4% prototype detector runs at CERN. The data volume from the full DUNE detector, when it starts commissioning late in this decade will present memory management challenges in conventional processing but significant opportunities to use advances in machine learning and pattern recognition as a frontier user of High Performance Computing facilities capable of massively parallel processing. Our goal is to develop infrastructure resources that are flexible and accessible enough to support creative software solutions as HEP computing evolves.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 281,
            id: 'c17063',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11445/contribution.pdf',
            presenters: [
              {
                affiliation: 'FNAL',
                displayOrderKey: [1, 'Kirby, Michael'],
                emailHash: '384f25285a1799bb8e2b4ea136bfd945',
                familyName: 'Kirby',
                firstName: 'Michael',
                name: 'Michael Kirby',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12124,
            sessionSlotId: 2693,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Status of DUNE Offline Computing',
            uniqueId: 'c17063',
            url: '/event/459/contributions/11445/',
          },
          c17473: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11869/attachments/9345/13548/CHEP2023_dune_offline_databases.pptx',
                  id: 13548,
                  title: 'CHEP2023_dune_offline_databases.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11869,
            description:
              'The Deep Underground Neutrino Experiment (DUNE) is a long-baseline experiment which aims to study neutrino oscillation and astroparticle physics. It will produce vast amounts of metadata, which describe the data coming from the read-out of the primary DUNE detectors. Various databases will make up the overall DB architecture for this metadata. ProtoDUNE at CERN is the largest existing prototype for DUNE and serves as a testing ground for - among other things - possible database solutions for DUNE.\r\nThe subset of all metadata that is accessed during offline data reconstruction and analysis is referred to as \u2018conditions data\u2019 and it is stored in a dedicated database. As offline data reconstruction and analysis will be deployed on HTC and HPC resources, conditions data is expected to be accessed at very high rates. It is therefore crucial to store it in a granularity that matches the expected access patterns allowing for extensive caching. This requires a good understanding of the sources and use cases of conditions data. This contribution will briefly summarize the database architecture deployed at ProtoDUNE and explain the various sources of conditions data. We will present how the conditions data is retrieved from the run conditions and beam database; and how, together with the conditions data from the Detector Control System (Slow Controls) and those needed for the calibration of a LArTPC, are put in a format to match the expected access patterns.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 264,
            id: 'c17473',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11869/contribution.pdf',
            presenters: [
              {
                affiliation: 'Colorado State University',
                displayOrderKey: [1, 'Vizcaya Hernandez, Ana Paula'],
                emailHash: '55e6cd0a4de5857d1ff436059431d355',
                familyName: 'Vizcaya Hernandez',
                firstName: 'Ana Paula',
                name: 'Dr Ana Paula Vizcaya Hernandez',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12124,
            sessionSlotId: 2693,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'DUNE Database Development',
            uniqueId: 'c17473',
            url: '/event/459/contributions/11869/',
          },
        },
        entryType: 'Session',
        friendlyId: 7,
        id: 's12124',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2037/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VI',
        sessionCode: '',
        sessionId: 2037,
        sessionSlotId: 2693,
        slotTitle: 'Data preparation (part I)',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#272f09',
        title: 'Track 3 - Offline Computing',
        uniqueId: 's12124',
        url: '/event/459/sessions/2037/',
      },
      s12125: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d9dfc3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Chulalongkorn University',
            displayOrderKey: [0, 'Srpimanobhas, Norraphat'],
            emailHash: '545290bb86b1edf24b84a6682630eadc',
            familyName: 'Srpimanobhas',
            firstName: 'Norraphat',
            name: 'Dr Norraphat Srpimanobhas',
          },
          {
            affiliation: 'Fermilab',
            displayOrderKey: [0, 'Yang, Tingjun'],
            emailHash: '521815c4f74228afa6e1c602c0d8d4a7',
            familyName: 'Yang',
            firstName: 'Tingjun',
            name: 'Tingjun Yang',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17053: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11419/attachments/9633/14035/20230509-CHEP2023-algorithms-ePIC.pdf',
                  id: 14035,
                  title: '20230509-CHEP2023-algorithms-ePIC.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11419,
            description:
              'The EPIC collaboration at the Electron-Ion Collider recently laid the groundwork for its software infrastructure. Large parts of the software ecosystem for EPIC mirror the setup from the Key4hep project, for example DD4hep for geometry description, and EDM4hep/PODIO for the data model. However, other parts of the EPIC software ecosystem diverge from Key4hep, for example for the event processing framework (JANA2 for EPIC versus Gaudi for Key4hep). The `algorithms` initiative by the EPIC collaboration tries to foster cross-community collaboration and algorithm-sharing by providing a framework- and experiment-independent approach to digitization and reconstruction algorithms. In this talk we will focus on the design choices behind Algorithms and showcase real-world integration of `algorithms` with both JANA2 and Gaudi.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 558,
            id: 'c17053',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11419/contribution.pdf',
            presenters: [
              {
                affiliation: 'Argonne National Laboratory',
                displayOrderKey: [0, 'Joosten, Sylvester'],
                emailHash: 'c1ec96da61f73f5759282a5bdddd4ab9',
                familyName: 'Joosten',
                firstName: 'Sylvester',
                name: 'Sylvester Joosten',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12125,
            sessionSlotId: 2694,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Algorithms: Framework- and Experiment-independent algorithms at EPIC',
            uniqueId: 'c17053',
            url: '/event/459/contributions/11419/',
          },
          c17054: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11431/attachments/9567/13959/ACTS_for_STCF_20230506.pdf',
                  id: 13959,
                  title: 'ACTS_for_STCF_20230506.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11431,
            description:
              'The reconstruction of charged particles\u2019 trajectories is one of the most complex and CPU-consuming event processing chains in high energy physics (HEP) experiments. Meanwhile, the precision of track reconstruction has direct and significant impact on vertex reconstruction, physics flavour tagging and particle identfication, and eventually on physics precision, in particular for HEP experiments at the precison frontier, such as the Super \u03c4-Charm facility (STCF). \r\n\r\nWith an electron-positron collider operating at center-of-mass-energy 2\u223c7 GeV and a peak luminosity above 0.5 \u00d7 10^35 cm^\u22122 s^\u22121, the STCF physics program will provide an unique platform for in-depth studies of hadron structure and non-perturbative strong interaction, as well as probing physics beyond the Standard Model at the \u03c4-Charm sector suceeding the present Being Electron-Positron Collider II (BEPCII). To fulfill the physics targets and to further maximize the physics potential at the STCF,  the charged particles with momentum down to 50 MeV must be detected and reconstructed, and an excellent momentum and angular resolution of the charged particles must be achieved. Therefore, development of a highly-performant and maintenable tracking software is very curcial for the design, construction and operation of STCF.\r\n\r\nBased on the tracking experience at LHC, the project, A Common Tracking Software (ACTS),  aims to provide an open-source experiment-independent and framework-independent software designed for modern computing architectures. It provides a set of high-level performant track reconstruction tools which are agnostic to the details of the detection technologies and magnetic field configuration, and tested for strict thread-safety to support multi-threaded event processing. ACTS has been used as a tracking toolkit at experiments such as ATLAS, sPHENIX, ALICE etc. and has shown very promising tracking performance in terms of both physics performance and time performance. However, its applications so far are mainly focusing on silicon-based tracking systems and are often less concerned with charged tracks with momentum below a few hundreds of MeV.\r\n\r\nIn this talk, I will report on development of the STCF track reconstrcon software based on the detection information from a Silicon (or uRWELL)-based Inner Tracker and  a Main Drift Chamber using the Kalman Filter based track finding and fitting algorithms of ACTS. This is the first application of ACTS for a drift chamber and shows very promising performance. Therefore, the efforts on tuning its performance, in particular for charged tracks with low momentum down to 50 MeV, will be highlighted.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 244,
            id: 'c17054',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11431/contribution.pdf',
            presenters: [
              {
                affiliation: 'Shandong University',
                displayOrderKey: [0, 'Li, Teng'],
                emailHash: 'c6c302428ff4c73ceb3723fea91da1c6',
                familyName: 'Li',
                firstName: 'Teng',
                name: 'Teng Li',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12125,
            sessionSlotId: 2694,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Performance of track reconstruction at STCF using ACTS',
            uniqueId: 'c17054',
            url: '/event/459/contributions/11431/',
          },
          c17056: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11443/attachments/9534/13825/2023-05-09_chep_v6.pdf',
                  id: 13825,
                  title: '2023-05-09_chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11443,
            description:
              'ACTS is an experiment independent toolkit for track reconstruction, which is designed from the ground up for thread-safety and high performance. It is built to accommodate different experiment deployment scenarios, and also serves as community platform for research and development of new approaches and algorithms.\r\n\r\nThe Event Data Model (EDM) is a critical piece of the tracking library that is visible to clients. Until this point, ACTS was mostly focused on an internal EDM, targeting data interchange betweens various components in the toolkit. \r\nThis contribution reports on a new and improved client EDM for ACTS. For an experiment-agnostic toolkit like ACTS, this requires strong abstractions of potentially experiment-specific details, including event context data like sensor alignments, and tracking inputs like measurements. By applying similar abstraction strategies, the presented EDM can be an expressive, low-overhead abstraction over experiment-specific backends, and seamlessly integrates into an experiment framework and IO model.\r\n\r\nThe presented EDM includes the ACTS track class, the main data type which tracking clients interact with. It is designed to be interfaced with different IO backends, and also flexible enough to support dynamic information required by various track fitters. At the same time, careful design ensures it can seamlessly serve as a key data object in experiment reconstruction data flows.\r\n\r\nIn this contribution, the interaction of this centerpiece of the example workflows in ACTS with the standalone ROOT IO, as well as the integration with the EDM4hep package will be shown, and key performance characteristics discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 510,
            id: 'c17056',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11443/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Gessinger, Paul'],
                emailHash: '78cdc5520c018d3142d9ea78259037e6',
                familyName: 'Gessinger',
                firstName: 'Paul',
                name: 'Dr Paul Gessinger',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12125,
            sessionSlotId: 2694,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Flexible, robust and minimal-overhead Event Data Model for track reconstruction in ACTS',
            uniqueId: 'c17056',
            url: '/event/459/contributions/11443/',
          },
          c17057: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11417/attachments/9418/13659/2023-05-chep.pdf',
                  id: 13659,
                  title: '2023-05-chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11417,
            description:
              'For Run 3, ATLAS redesigned its offline software, Athena, so that the\r\nmain workflows run completely multithreaded. The resulting substantial\r\nreduction in the overall memory requirements allows for better use\r\nof machines with many cores. This talk will discuss the performance\r\nachieved by the multithreaded reconstruction as well as the process\r\nof migrating the large ATLAS code base and tools and techniques\r\nthat were useful in debugging threading-related problems.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 484,
            id: 'c17057',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11417/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [0, 'Snyder, Scott'],
                emailHash: 'b855176f19b0122e23b6c0787bb08a24',
                familyName: 'Snyder',
                firstName: 'Scott',
                name: 'Scott Snyder',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12125,
            sessionSlotId: 2694,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Multithreading ATLAS offline software: a retrospective',
            uniqueId: 'c17057',
            url: '/event/459/contributions/11417/',
          },
          c17058: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11416/attachments/9584/13910/CMS_TRKDQMML_CHEP23_GBenelli_v2.pdf',
                  id: 13910,
                  title: 'CMS_TRKDQMML_CHEP23_GBenelli_v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11416,
            description:
              'During the long shutdown between LHC Run 2 and 3, a reprocessing of 2017 and 2018 CMS data with higher granularity data quality monitoring (DQM) harvesting was done. The time granularity of DQM histograms in this dataset is increased by 3 orders of magnitude. In anticipation of deploying this higher granularity DQM harvesting in the ongoing Run 3 data taking, this dataset is used to study the application of Machine Learning (ML) techniques to data certification with the goal of developing tools for online monitoring and offline certification. In this talk, we will discuss the challenges and present some of the results, illustrating the tools developed for CMS Tracker Data Quality Monitoring and Certification. Studies consider both the use case of anomaly detection in the context of reprocessing campaigns, when all the data is available, and in the context of continuous data taking, when conditions are constantly changing and models need to be trained on data previously collected with similar conditions. Data augmentation is pursued, including information from the CMS Online Monitoring System (luminosity, pile-up, LHC fill, run and trigger), from the CMS Run Registry (sub-detector certification flags), from the CMS conditions database (calibrations). The status of the web application integrating data sources and facilitating development, testing and benchmarking of ML models will be presented using a few test cases.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 554,
            id: 'c17058',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11416/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brown University',
                displayOrderKey: [1, 'Benelli, Gabriele'],
                emailHash: '0245f31f782d3417da2e98d16ff3232c',
                familyName: 'Benelli',
                firstName: 'Gabriele',
                name: 'Gabriele Benelli',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12125,
            sessionSlotId: 2694,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Machine Learning Tools for the CMS Tracker Data Quality Monitoring and Certification',
            uniqueId: 'c17058',
            url: '/event/459/contributions/11416/',
          },
        },
        entryType: 'Session',
        friendlyId: 7,
        id: 's12125',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2037/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VI',
        sessionCode: '',
        sessionId: 2037,
        sessionSlotId: 2694,
        slotTitle: 'Physics performance (part 2)',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#272f09',
        title: 'Track 3 - Offline Computing',
        uniqueId: 's12125',
        url: '/event/459/sessions/2037/',
      },
      s12126: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d0c296',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Universite de Geneve (CH)',
            displayOrderKey: [0, 'Antel, Claire'],
            emailHash: '0700a8e2f8e6dd930897159bde503e7d',
            familyName: 'Antel',
            firstName: 'Claire',
            name: 'Claire Antel',
          },
          {
            affiliation: 'KEK',
            displayOrderKey: [0, 'Yamada, Satoru'],
            emailHash: 'f2ae8a15fd4a3fafb55ff71831b9c00c',
            familyName: 'Yamada',
            firstName: 'Satoru',
            name: 'Satoru Yamada',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17017: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11364/attachments/9411/13652/CHEP23_KRAWCZYK.pdf',
                  id: 13652,
                  title: 'CHEP23_KRAWCZYK.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11364,
            description:
              'The CMS experiment at CERN incorporates one of the highest throughput Data Acquisition (DAQ) systems in High-Energy Physics. Its network throughput will further increase by over an order of magnitude at the High-Luminosity LHC.\r\nThe current Run 3 CMS Event Builder receives all the fragments of Level-1 trigger accepted events from the front-end electronics (740 data streams from the CMS subdetectors) and assembles them into complete 2 MB events at a rate of 100 kHz, with a throughput of 1.6 Tbit/s. Its output is handed over to the High Level Trigger (HLT), which runs in a farm consisting of approximately 200 computers equipped with general-purpose GPUs. The HLT selects interesting events at a rate of several kHz. The CMS DAQ will undergo a full upgrade prior to the start of the HL-LHC operation in 2029. The DAQ readout, network, and compute hardware will be entirely replaced to cope with a maximum input rate of 750 kHz and a nominal event size of 8.4 MB.\r\n\r\nAs with the current system, the Phase-2 event builder will consist of commercial off-the-shelf compute elements interconnected by a high-performance switched network in an all-to-all fashion. The switched network will have to handle an average throughput of about 50 Tb/s. To cope with the increased Level-1 rate, data fragments from individual Level-1 accepts corresponding to one LHC beam revolution (LHC orbit) will be aggregated into "orbit fragments" in the DAQ custom readout electronics. It is these orbit fragments that will then be further aggregated into full orbit data blocks, thus allowing the event builder protocol to work at a constant message rate of about 11 kHz, avoiding excessive message overheads. The final event building will then be delegated to the HLT processes, working each on individual orbit blocks.\r\n\r\nTo accommodate this new concept of "orbit building", the event builder software components will be entirely redesigned. In this work the key new features of the Phase-2 "event" builder are discussed. A study of new software solutions for the Phase-2 Event Builder and preliminary test benchmarks are presented with related performance results.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 493,
            id: 'c17017',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11364/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Krawczyk, Rafal'],
                emailHash: '87047f631e399a13cf20fad3cfca0896',
                familyName: 'Krawczyk',
                firstName: 'Rafal',
                name: 'Dr Rafal Krawczyk',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12126,
            sessionSlotId: 2695,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Event Building studies for CMS Phase-2 at CERN',
            uniqueId: 'c17017',
            url: '/event/459/contributions/11364/',
          },
          c17018: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11367/attachments/9461/13716/HL-LHC%20ATLAS%20DAQ%20network%20benchmarck-1.pdf',
                  id: 13716,
                  title: 'HL-LHC ATLAS DAQ network benchmarck-1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11367,
            description:
              'The ATLAS experiment Data Acquisition (DAQ) system will be extensively upgraded to fully exploit the High-Luminosity LHC (HL-LHC) upgrade, allowing it to record data at unprecedented rates. The detector will be read out at 1 MHz generating over 5 TB/s of data. This design poses significant challenges for the Ethernet-based network as it will be required to transport 20 times more data than during Run 3. The increased data rate, data sizes, and the number of servers will exacerbate the TCP incast effect observed in the past, which makes it impossible to fully exploit the capabilities of the network and limits the performance of the processing farm.\r\nWe present exhaustive and systematic experiments to define buffer requirements in network equipment to minimise the effects of TCP Incast and reduce the impact on the processing applications. Three switch models were stress-tested using DAQ traffic patterns in a test environment at approximately 10% scale of the expected HL-LHC DAQ system size.\r\nAs the HL-LHC system\u0027s desired hardware is not currently available and the lab size is considerably smaller, tests aim to project buffer requirements with different parameters. Different solutions are analysed, comparing software-based and network hardware cost-to-performance ratios to determine the most effective option to mitigate the impact of TCP incast.\r\nThe results of these evaluations will contribute to the decision-making process of acquiring network hardware for the HL-LHC DAQ.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 300,
            id: 'c17018',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11367/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [2, 'Pozo Astigarraga, Eukeni'],
                emailHash: '27ac5e60be871f192160979179a419aa',
                familyName: 'Pozo Astigarraga',
                firstName: 'Eukeni',
                name: 'Eukeni Pozo Astigarraga',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12126,
            sessionSlotId: 2695,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'Benchmarking Data Acquisition event building network performance for the ATLAS HL-LHC upgrade',
            uniqueId: 'c17018',
            url: '/event/459/contributions/11367/',
          },
          c17019: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11402/attachments/9362/13571/2023.05.09%20-%20A.%20Bocci%20-%20Adoption%20of%20the%20alpaka%20performance%20portability%20library%20in%20the%20CMS%20software.pdf',
                  id: 13571,
                  title:
                    'A. Bocci - Adoption of the alpaka performance portability library in the CMS software.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11402,
            description:
              'To achieve better computational efficiency and exploit a wider range of computing resources, the CMS software framework (CMSSW) has been extended to offload part of the physics reconstruction to NVIDIA GPUs, while the support for AMD and Intel GPUs is under development. To avoid the need to write, validate and maintain a separate implementation of the reconstruction algorithms for each back-end, CMS decided to adopt a performance portability framework. After evaluating different alternative, it was decided to adopt Alpaka as the solution for Run-3.\r\n\r\nAlpaka (Abstraction Library for Parallel Kernel Acceleration) is a header-only C++ library that provides performance portability across different back-ends, abstracting the underlying levels of parallelism. It supports serial and parallel execution on CPUs, and extremely parallel execution on GPUs.\r\n\r\nThis contribution will show how Alpaka is used inside CMSSW to write a single code base; to use different toolchains to build the code for each supported back-end, and link them into a single application; and to select the best back-end at runtime. It will highlight how the alpaka-based implementation achieves near-native performance, and will conclude discussing the plans to support additional back-ends.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 553,
            id: 'c17019',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11402/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Bocci, Andrea'],
                emailHash: 'a0edda8c32b7a5c82a1cf049171b0206',
                familyName: 'Bocci',
                firstName: 'Andrea',
                name: 'Andrea Bocci',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12126,
            sessionSlotId: 2695,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Adoption of the alpaka performance portability library in the CMS software',
            uniqueId: 'c17019',
            url: '/event/459/contributions/11402/',
          },
          c17020: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11384/attachments/9346/13549/CHEP-Belle2HLT.pdf',
                  id: 13549,
                  title: 'CHEP-Belle2HLT.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11384,
            description:
              'The original HLT framework used in the Belle II experiment was formaly upgraded replacing the old IPC based ring buffer with the ZeroMQ data transport to overcome the unexpected IPC locking problem. The new framework has been working stably in the beam run so far, but it lacks the capability to recover the processing fault without stopping the on-going data taking. In addition, the compatibility with the offline framework (basf2) was lost which was maintained in the original. \r\n\r\nIn order to solve these, an improved core processing framework is developed based on basf2 running on each of worker servers, while keeping the existing ZeroMQ data transport between the servers unchanged. The new core framework is implemented with a lock-free 1-to-N and N-to-1 data transport using ZeroMQ utilizing the IPC socket so that it keeps a 100% compatibility with the original ring-buffer based offline framework. When a processing fault occurs, the affected currently processing event is salvaged from the input buffer and sent directly to the output using ZeroMQ broadcast. The terminated process is automatically restarted without stopping data taking.\r\n\r\nThis contribution describes the detail of the improved Belle II HLT frameowrk with the result of the performance test in the real Belle II DAQ data flow.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 142,
            id: 'c17020',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11384/contribution.pdf',
            presenters: [
              {
                affiliation: 'KEK',
                displayOrderKey: [1, 'Itoh, Ryosuke'],
                emailHash: '1c1f5d8a5a97ebe1f059686f6c8fc295',
                familyName: 'Itoh',
                firstName: 'Ryosuke',
                name: 'Ryosuke Itoh',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12126,
            sessionSlotId: 2695,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Improved HLT framework for Belle II experiment',
            uniqueId: 'c17020',
            url: '/event/459/contributions/11384/',
          },
          c17021: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11389/attachments/9439/13687/20230509_K8sDUNEDAQ_Lasorak.pdf',
                  id: 13687,
                  title: '20230509_K8sDUNEDAQ_Lasorak.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11389,
            description:
              'The Deep Underground Neutrino Experiment (DUNE) is a next generation long-baseline neutrino experiment based in the USA which is expected to start taking data in 2029. DUNE aims to precisely measure neutrino oscillation parameters by detecting neutrinos from the LBNF beamline (Fermilab) at the Far Detector, 1300 kilometres away, in South Dakota. The Far Detector will consist of four cryogenic Liquid Argon Time Projection Chamber (LArTPC) detectors of 17kT, each producing more than 1 TB/s of data. The main requirements for the data acquisition (DAQ) system are the ability to run continuously for extended periods of time, with a 99% uptime requirement, and the functionality to record beam neutrinos and low energy neutrinos from the explosion of a neighbouring supernova, should one occur during the lifetime of the experiment. The key challenges are the high data rates that the detectors generate and the deep underground environment, which places constraints on power and space. To overcome these challenges, the DUNE experiment plans to use a highly efficient C++ software suite and a server farm of about 110 nodes continuously running about two hundred processes located close to the detector, 1.5 miles underground. Thirty nodes will be at the surface and will run around two hundred processes simultaneously. DUNE is studying the use the Kubernetes framework to distribute containerised workloads and take advantage of its resource definitions and high uptime services to run the DAQ system. Progress in deploying these systems at the CERN neutrino platform on the prototype DUNE experiments (ProtoDUNE) were also made.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 315,
            id: 'c17021',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11389/contribution.pdf',
            presenters: [
              {
                affiliation: 'Imperial College London',
                displayOrderKey: [1, 'Lasorak, Pierre'],
                emailHash: '8c7df3f5fbd3d11ad525f047240190e4',
                familyName: 'Lasorak',
                firstName: 'Pierre',
                name: 'Pierre Lasorak',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12126,
            sessionSlotId: 2695,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Kubernetes for DUNE DAQ',
            uniqueId: 'c17021',
            url: '/event/459/contributions/11389/',
          },
          c17022: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11400/attachments/9523/13812/Scientific%20application%20development%20based%20on%20the%20Daisy.pptx',
                  id: 13812,
                  title: 'Scientific application development based on the Daisy.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11400,
            description:
              'Large-scale research facilities are becoming prevalent in the modern scientific landscape. One of these facilities\u0027 primary responsibilities is to make sure that users can process and analysis measurement data for publication. To allow for barrier-less access to those highly complex experiments, almost all beamlines require fast feedback capable of manipulating and visualizing data online to offer convenience for the decision process of the experimental strategy. And recently, the advent of beamlines at fourth-generation synchrotron sources and high resolution with high sample rate detector has made significant progress that pushes the demand for computing resources to the edge of current workstation capabilities. On top of this, most synchrotron light sources have shifted to prolonged remote operation because of the outbreak of a global pandemic, with the need for remote access to the online instrumental system during the operation. Another issue is the vast data volume produced by specific experiments makes it difficult for users to create local data copies. In this case, on-site data analysis services are necessary both during and after experiments.\r\n      Some state-of-the-art experimental techniques, such as phase-contrast tomography and ptychography approaches, will be deployed. However, it poses a critical problem of integrating this algorithmic development into a novel computing environment used in the experimental workflow. The solution requires collaboration with the user research groups, instrument scientists and computational scientists. A unified software platform that provides an integrated working environment with generic functional modules and services is necessary to meet these requirements. Scientists can work on their ideas, implement the prototype and check the results following some conventions without dealing with the technical details and the migration between different HPC environments. Thus, one of the vital considerations is integrating extensions into the software in a flexible and configurable way. Another challenge resides in the interactions between instrumental sub-systems, such as control system, data acquisition system, computing infrastructures, data management system, data storage system, and so on, which can be quite complicated.\r\n      In this paper, we propose a platform named Daisy for integration and automation across services and tools, which ties together existing computing infrastructure and state-of-the-art algorithms. With modular architecture, it comprises loosely coupled algorithm components that communicate over the heterogeneous in-memory data store and scales horizontally to deliver automation at scale based on Kubernetes. The applications for the different scientific domains of HEPS developed based on the platform will also be introduced.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 195,
            id: 'c17022',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11400/contribution.pdf',
            presenters: [
              {
                affiliation: ' Institute of High Energy Physics, CAS',
                displayOrderKey: [6, 'Fu, Shiyuan'],
                emailHash: '80da714a95d7c08e7a56734aac46b067',
                familyName: 'Fu',
                firstName: 'Shiyuan',
                name: 'Dr Shiyuan Fu',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12126,
            sessionSlotId: 2695,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Scientific application development based on the Daisy',
            uniqueId: 'c17022',
            url: '/event/459/contributions/11400/',
          },
        },
        entryType: 'Session',
        friendlyId: 6,
        id: 's12126',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2036/session-timetable.pdf',
        room: 'Marriott Ballroom V-VI',
        sessionCode: '',
        sessionId: 2036,
        sessionSlotId: 2695,
        slotTitle: 'Network \u0026 Infrastructure',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#000000',
        title: 'Track 2 - Online Computing',
        uniqueId: 's12126',
        url: '/event/459/sessions/2036/',
      },
      s12127: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#efebc2',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'STFC-RAL',
            displayOrderKey: [0, 'Ellis, Katy'],
            emailHash: 'e4c8a78292759108e5511297386cac3a',
            familyName: 'Ellis',
            firstName: 'Katy',
            name: 'Katy Ellis',
          },
          {
            affiliation: 'Unive',
            displayOrderKey: [0, 'Barreiro Megino, Fernando'],
            emailHash: 'f04de6eee7b8dcc20dedefcb9fe659da',
            familyName: 'Barreiro Megino',
            firstName: 'Fernando',
            name: 'Fernando Barreiro Megino',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16848: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11496/attachments/9615/13986/go',
                  id: 13986,
                  title: 'Services and Tools for Collaborations',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11496,
            description:
              'We present a collection of tools and processes that facilitate onboarding a new science collaboration onto the OSG Fabric of Services. Such collaborations typically rely on computational workflows for simulations and analysis that are ideal for executing on OSG\u0027s distributed High Throughput Computing environment (dHTC). The produced output can be accumulated and aggregated at available ephemeral storage for inspection and then distribution to other facilities for long-term storage and further analysis. This work focuses on matching workflow requirements to tools and services that provide job management, automation (Pegasus WMS), and delivery of collaboration-specific software and input data to Execution Points (EP) using the Open Science Data Federation (OSDF) or Rucio. We describe best-supported practices to transfer data products to remote facilities at runtime or after the job completion. We emphasize the importance of early planning that meets the goals of the typically long life cycle of HEP experiments and advocate for an engagement model where the collaboration eventually becomes self-reliant in workload and data management.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 527,
            id: 'c16848',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11496/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Chicago',
                displayOrderKey: [1, 'Paschos, Pascal'],
                emailHash: '0bef7c103c1ea993264b4d70b5211e89',
                familyName: 'Paschos',
                firstName: 'Pascal',
                name: 'Pascal Paschos',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12127,
            sessionSlotId: 2696,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Tools and Services for Collaborations: On-Boarding to the OSG Fabric of Services',
            uniqueId: 'c16848',
            url: '/event/459/contributions/11496/',
          },
          c16849: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11487/attachments/9327/13527/GitOps%20-%20CHEP.pdf',
                  id: 13527,
                  title: 'GitOps - CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11487,
            description:
              'There is no lack of approaches for managing the deployment of distributed services \u2013 in the last 15 years of running distributed infrastructure, the OSG Consortium has seen many of them.  One persistent problem has been each physical site has its style of configuration management and service operations, leading to a partitioning of the staff knowledge and inflexibility in migrating services between sites.\r\n\r\nRecently, the team has been migrating the OSG Fabric of Services to be deployed via Kubernetes which provides a common service orchestration fabric across all sites.  However, this leaves open a question - how does the team interact with Kubernetes?  To coordinate this new style of deployment among geographically distributed clusters and team members, the team has adopted "GitOps", an operational model that uses Git version control repositories to drive service updates. Git-driven operations provides all the benefits of version control such as recording the who, what, when, and why of any given change. But, more powerfully, automated agents synchronize the current state of the Git repository with the current state of the Kubernetes clusters, streamlining the ability to redeploy services from scratch or transfer services between clusters. In this paper, we will describe the setup that enables GitOps deployments of central OSG services and the lessons learned along the way, including rebuilding a suite of services after a critical failure and our experiences with providing high-availability services across multiple Kubernetes clusters.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 545,
            id: 'c16849',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11487/contribution.pdf',
            presenters: [
              {
                affiliation: 'Morgridge Institute for Research',
                displayOrderKey: [1, 'Bockelman, Brian'],
                emailHash: '672e6085d6c2aaaac23af4e09549c9fd',
                familyName: 'Bockelman',
                firstName: 'Brian',
                name: 'Brian Bockelman',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12127,
            sessionSlotId: 2696,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Managing the OSG Fabric of Services the GitOps Way',
            uniqueId: 'c16849',
            url: '/event/459/contributions/11487/',
          },
          c16850: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11483/attachments/9475/13736/presentation.pdf',
                  id: 13736,
                  title: 'presentation.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11483,
            description:
              'The CernVM File System (CVMFS) provides the software distribution backbone for High Energy and Nuclear Physics experiments and many other scientific communities in the form of a globally available shared software area. It has been designed for the software distribution problem of experiment software for LHC Runs 1 and 2. For LHC Run 3 and even more so for HL-LHC (Runs 4-6), the complexity of the experiment software stacks and their build pipelines is substantially larger. For instance, software is being distributed for several CPU architectures, often in the form of containers which includes base and operating system libraries, the number of external packages such as machine learning libraries has multiplied, and there is a shift from C++ to more Python-heavy software stacks that results in more and smaller files needing to be distributed. For CVMFS, the new software landscape means an order of magnitude increase of scale in key metrics such as number of files, number of system calls, and number of concurrent processes accessing the file system client. In this contribution, we report on the performance and reliability engineering on the file system client to sustain current and expected future software access load. Concretely, we show the impact of the newly designed file system cache management, including upstreamed improvements to the fuse kernel module itself, improved utilization of network links and caches (such as line optimization, prefetching, and proxy sharding), and operational improvements on network failure handling, error reporting, and integration with container runtimes. Overall, the new CVMFS client is designed to sustain applications with more than one million file lookups during startup, nodes with hundreds of cores, and thousands of concurrent processes accessing software from the file system client.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 391,
            id: 'c16850',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11483/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Promberger, Laura'],
                emailHash: 'a22d101a488b35c4a90ccf1d7fcc2f3e',
                familyName: 'Promberger',
                firstName: 'Laura',
                name: 'Laura Promberger',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12127,
            sessionSlotId: 2696,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'CernVM-FS at Extreme Scales',
            uniqueId: 'c16850',
            url: '/event/459/contributions/11483/',
          },
          c16851: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11475/attachments/9321/13520/2023_CHEP_AUDITOR.pdf',
                  id: 13520,
                  title: '2023_CHEP_AUDITOR.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11475,
            description:
              'The increasing computational demand in High Energy Physics (HEP) as well as increasing concerns about energy efficiency in high performance/throughput computing are driving forces in the search for more efficient ways to utilize available resources. Since avoiding idle resources is key in achieving high efficiency, an appropriate measure is sharing of idle resources of under-utilized sites with fully occupied sites. The software COBalD/TARDIS can automatically, transparently and dynamically (dis)integrate such resources in an opportunistic manner. \r\nSharing resources however also requires accounting. In this work we introduce AUDITOR (AccoUnting DatahandlIng Toolbox for Opportunistic Resources), a flexible and extensible accounting system that is able to cover a wide range of use cases and infrastructure. AUDITOR gathers accounting data via so-called collectors which are designed to monitor batch systems, COBalD/TARDIS, cloud schedulers or other sources of information. The data is stored in a database and provided to so-called plugins, which take an action based on accounting records. An action could for instance be creating a bill or computing the CO2 footprint, adjusting parameters of a service (for instance priorities in a batch system) or forwarding accounting information to other accounting systems. Depending on the use case, a suitable collector and plugin are selected from a growing ecosystem of collectors and plugins. Libraries for interacting with AUDITOR are provided to facilitate the development of collectors and plugins by the community. \r\nThis contribution gives insights into the design of AUDITOR and how it integrates into a number of different use cases.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 108,
            id: 'c16851',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11475/contribution.pdf',
            presenters: [
              {
                affiliation: 'Freiburg University',
                displayOrderKey: [0, 'Boehler, Michael'],
                emailHash: '7e8376b24550290f6dbbd6b96f5f0b05',
                familyName: 'Boehler',
                firstName: 'Michael',
                name: 'Dr Michael Boehler',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12127,
            sessionSlotId: 2696,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'AUDITOR: Accounting for opportunistic resources',
            uniqueId: 'c16851',
            url: '/event/459/contributions/11475/',
          },
          c16852: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11501/attachments/9380/13603/jiriaf_chep23.pptx',
                  id: 13603,
                  title: 'jiriaf_chep23.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11501,
            description:
              'The JIRIAF project aims to combine geographically diverse computing facilities into an integrated science infrastructure. This project starts by dynamically evaluating temporarily unallocated or idled compute resources from multiple providers. These resources are integrated to handle additional workloads without affecting local running jobs. This paper describes our approach to launch best-effort batch tasks which exploit these underutilized resources. Our system measures the real-time behavior of jobs running on a machine and learns to distinguish typical performance from outliers. Unsupervised ML techniques are used to analyze hardware-level performance measures, followed by a real-time cross-correlation analysis to determine which applications cause performance degradation. We then ameliorate bad behavior by throttling these processes. We demonstrate that problematic performance interference can be detected and acted on, which makes it possible to continue to share resources between applications and simultaneously maintain high utilization levels in a computing cluster. For a case study, we relocate the CLAS12 data processing workflow to a remote data center, preventing file migration and temporal data persistency.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 320,
            id: 'c16852',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11501/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [1, 'Gyurjyan, Vardan'],
                emailHash: 'd96010739664021f1bf2e72848130026',
                familyName: 'Gyurjyan',
                firstName: 'Vardan',
                name: 'Vardan Gyurjyan',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12127,
            sessionSlotId: 2696,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'JIRIAF: JLAB Integrated Research Infrastructure Across Facilities',
            uniqueId: 'c16852',
            url: '/event/459/contributions/11501/',
          },
          c16853: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11508/attachments/9563/13877/Marcon_PaaS_INFN_Cloud_ATL-SOFT-SLIDE-2023-110.pdf',
                  id: 13877,
                  title: 'Marcon_PaaS_INFN_Cloud_ATL-SOFT-SLIDE-2023-110.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11508,
            description:
              'The Worldwide LHC Computing Grid (WLCG) is a large-scale collaboration which gathers the computing resources of around 170 computing centres from more than 40 countries. The grid paradigm, unique to the realm of high energy physics, has successfully supported a broad variety of scientific achievements. To fulfil the requirements of new applications and to improve the long-term sustainability of the grid middleware, more versatile solutions are being investigated. Cloud computing is becoming increasingly popular among open-source and commercial players. The HEP community has also recognized the benefits of integrating cloud technologies into the legacy grid-based workflows. Since March 2021, INFN has entered the field of cloud computing establishing the INFN Cloud infrastructure. Large data centers of the INFN National Computing Center, connected to a nation-wide backbone maintained by the GARR Consortium, are gathered into a redundant and federated infrastructure. This cloud service supports scientific computing, software development and training, and serves as an extension of local computing and storage resources. Among available services, INFN Cloud administrators can create virtual machines, Docker-based deployments or Kubernetes clusters. These options allow the creation of customized environments, both for individual users and for scientific collaborations. This study investigates the feasibility of an automated, cloud-based data analysis workflow for the ATLAS experiment using INFN Cloud resources. The concept is designed as a Platform-as-a-Service (PaaS) solution, based on a Centos 7 Docker image. The customized image is responsible for the provisioning of CERN\u2019s CVMFS and EOS shared filesystems, from which a standardized ATLAS environment can be loaded. The end user\u2019s only responsibility is to provide a working application capable of retrieving and analysing data, and to export the results to a persistent storage. The analysis code can be sourced either from remote git repositories or from a local Docker bind mount. As a final step in the automation workflow, a Kubernetes cluster will be configured within the INFN Cloud infrastructure to allow dynamic resource allocation and the interoperability with batch systems, such as HTCondor, will be investigated.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 198,
            id: 'c16853',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11508/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Milan (IT)',
                displayOrderKey: [1, 'Marcon, Caterina'],
                emailHash: 'f8397c5411a38b255202a584253df615',
                familyName: 'Marcon',
                firstName: 'Caterina',
                name: 'Caterina Marcon',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12127,
            sessionSlotId: 2696,
            startDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'The Platform-as-a-Service paradigm meets ATLAS: developing an automated analysis workflow on the newly established INFN Cloud',
            uniqueId: 'c16853',
            url: '/event/459/contributions/11508/',
          },
        },
        entryType: 'Session',
        friendlyId: 8,
        id: 's12127',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2038/session-timetable.pdf',
        room: 'Marriott Ballroom II-III',
        sessionCode: '',
        sessionId: 2038,
        sessionSlotId: 2696,
        slotTitle: 'Infrastructure and Services',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 4 - Distributed Computing',
        uniqueId: 's12127',
        url: '/event/459/sessions/2038/',
      },
      s12128: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfe555',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Sailer, Andre'],
            emailHash: '84f3f721370dba12b47a611b1a36081e',
            familyName: 'Sailer',
            firstName: 'Andre',
            name: 'Andre Sailer',
          },
          {
            affiliation: 'University of Liverpool',
            displayOrderKey: [0, 'Rodrigues, Eduardo'],
            emailHash: '69164b74ffee995c119954e624bd24ec',
            familyName: 'Rodrigues',
            firstName: 'Eduardo',
            name: 'Eduardo Rodrigues',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16884: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11541/attachments/9344/13882/Large%20scale%20dynamic%20web%20deployment%20for%20CERN%20experience%20and%20outlook.pdf',
                  id: 13882,
                  title: 'Large scale dynamic web deployment for CERN experience and outlook.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11541/attachments/9344/13883/Large%20scale%20dynamic%20web%20deployment%20for%20CERN%20experience%20and%20outlook.pptx',
                  id: 13883,
                  title: 'Large scale dynamic web deployment for CERN experience and outlook.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11541,
            description:
              'CERN hosts more than 1200 websites essential for the mission of the Organization, internal and external collaboration and communicaiton as well as public outreach. The complexity and scale of CERN\u2019s online presence is very diverse with some websites, like https://home.cern/\r\n\r\n, accommodating more than one million unique visitors in a day.\r\n\r\nHowever, regardless of their diversity, all websites are created using the Drupal content management system (CMS), and are self-hosted directly in the CERN Datacenter on a dedicated infrastructure that runs on Kubernetes\r\n\r\n. Workflows like provisioning, deleting, cloning, upgrading, and similar are fully automated and managed by a customised Kubernetes controller. By leveraging the custom controller, the infrastrucutre has proven highly reliant with minimal, manual intervention necessary. In order to further automate deployments and improve goverance, we run a customised version of Drupal called the CERN Drupal Distribution. Supported by end-to-end integration tests and automated browser simulation, our setup enables us to propagate security and feature updates seamlessly to all websites without any downtime.\r\n\r\nIn this paper, we outline the architecture allowing us to build, test, and distribute updates to a large number of websites without any downtime. We further share our experiences and learnings from managing such a service at CERN with a lean team.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 514,
            id: 'c16884',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11541/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Rajula, Vineet Reddy'],
                emailHash: '6123db1b7e95a29f21dcc05af6e00948',
                familyName: 'Rajula',
                firstName: 'Vineet Reddy',
                name: 'Vineet Reddy Rajula',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12128,
            sessionSlotId: 2697,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Large scale dynamic web deployment for CERN, experience and outlook',
            uniqueId: 'c16884',
            url: '/event/459/contributions/11541/',
          },
          c16885: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11537/attachments/9282/13458/lintao-202305-CHEP2023.pdf',
                  id: 13458,
                  title: 'lintao-202305-CHEP2023.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11537/attachments/9282/13459/lintao-202305-CHEP2023.pptx',
                  id: 13459,
                  title: 'lintao-202305-CHEP2023.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11537,
            description:
              'The Jiangmen Underground Neutrino Observatory (JUNO), under construction in South China, primarily aims to determine the neutrino mass hierarchy and the precise measure oscillation parameters. The data-taking is expected to start in 2024 and plans to run for more than 20 years. The development of JUNO offline software (JUNOSW) started in 2012, and it is quite challenging to maintain the JUNOSW for such a long time. In the last ten years, tools such as Subversion, Trac, and CMT had been adopted for software development. However, there are some new requirements, such as how to reduce the building time for the whole project, how to deploy offline algorithms to an online environment, and how to improve the code quality with code review and continuous integration. To meet the further requirements of software development, modern development tools are evaluated for JUNOSW, such as Git, GitLab, CMake, Docker, and Kubernetes. This contribution will present the software development system based on these modern tools for JUNOSW and the functionalities we have achieved: CMake macros are developed to simplify the build instructions for users; CMake generator expressions are used to control the build flags for the online and offline environments; a tool named git-junoenv is developed to help users partially checkout and build the software; a script is used to build and deploy the software on the CVMFS server; a Docker image with CVMFS client installed is created for continuous integration; a GitLab agent is set up to manage GitLab runners in Kubernetes with all the configurations in a GitLab repository. In late 2022, the migration had been done.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 140,
            id: 'c16885',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11537/contribution.pdf',
            presenters: [
              {
                affiliation: 'IHEP',
                displayOrderKey: [1, 'Lin, Tao'],
                emailHash: '2fbe5cd266635003b02a2e156624828b',
                familyName: 'Lin',
                firstName: 'Tao',
                name: 'Dr Tao Lin',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12128,
            sessionSlotId: 2697,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Modern Software Development for JUNO offline software',
            uniqueId: 'c16885',
            url: '/event/459/contributions/11537/',
          },
          c16886: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11554/attachments/9223/13386/ATL-COM-SOFT-2023-013.pdf',
                  id: 13386,
                  title: 'ATL-COM-SOFT-2023-013.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11554,
            description:
              'The ATLAS Continuous Integration (CI) System is the major component of the ATLAS software development infrastructure, synchronizing efforts of several hundred software developers working around the world and around the clock. Powered by 700 fast processors, it is based on the ATLAS GitLab code management service and Jenkins CI server and performs daily up to 100 ATLAS software builds probing the code changes proposed in merge requests. The system uses Operational Intelligence methods to shorten development cycles and lower operating costs. The paper describes these methods, such as removal of redundant operations, compilation and testing parallelization, usage of the directed acyclic graph (DAG) approach in CI pipelines as well as reports achieved improvements.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 333,
            id: 'c16886',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11554/contribution.pdf',
            presenters: [
              {
                affiliation: 'BNL',
                displayOrderKey: [1, 'Undrus, Alexander'],
                emailHash: '44afb07877a33242cf3f06100d4810f6',
                familyName: 'Undrus',
                firstName: 'Alexander',
                name: 'Alexander Undrus',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12128,
            sessionSlotId: 2697,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Operational Intelligence in the ATLAS Continuous Integration System',
            uniqueId: 'c16886',
            url: '/event/459/contributions/11554/',
          },
          c16887: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11543/attachments/9265/13437/2023-05-02-CHEP23%20Nomad.pdf',
                  id: 13437,
                  title: 'Nomad-at-ALICE.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11543,
            description:
              'The ALICE experiment at CERN uses a cluster consisting of virtual and bare-metal machines to build and test proposed changes to the ALICE Online-Offline (O\u003csup\u003e2\u003c/sup\u003e) software in addition to building and publishing regular software releases.\r\n\r\nNomad is a free and open-source job scheduler for containerised and non-containerised applications developed by Hashicorp. It is integrated into an ecosystem of related software, including Consul and Vault, providing a consistent interface to orchestration, monitoring and secret storage. At ALICE, it recently replaced Apache Mesos, Aurora and Marathon as the primary tool for managing our computing resources.\r\n\r\nFirst, we will describe the architecture of the build cluster at the ALICE experiment. After giving an overview of the advantages that Nomad gives us in managing our computing workload, and our reasons for switching away from the Mesos software stack, we will present concrete examples of improvements in monitoring and automatic configuration of web services that we are already benefiting from. Finally, we will discuss where we see opportunities for future work in integrating the ALICE build infrastructure more deeply with Nomad, in order to take advantage of its larger feature set compared to Mesos.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 341,
            id: 'c16887',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11543/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Wilken, Timo'],
                emailHash: 'e8f41872c27e0123bf3b4b25ac2b513e',
                familyName: 'Wilken',
                firstName: 'Timo',
                name: 'Timo Wilken',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12128,
            sessionSlotId: 2697,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Managing software build infrastructure at ALICE using Hashicorp Nomad',
            uniqueId: 'c16887',
            url: '/event/459/contributions/11543/',
          },
          c16889: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11531/attachments/9427/14288/Version-control-and-devops-chep2023.pdf',
                  id: 14288,
                  title: 'Version-control-and-devops-chep2023.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11531/attachments/9427/14287/Version-control-and-devops-chep2023.pptx',
                  id: 14287,
                  title: 'Version-control-and-devops-chep2023.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11531,
            description:
              'GitLab has been running at CERN since 2012. It is a self-service code hosting application based on Git that provides collaboration and code review features, becoming one of the key infrastructures at CERN. It is being widely used at CERN, with more than 17 000 active users, hosting more than 120 000 projects and triggering more than 5 000 jobs per hour.\r\n\r\nOn its initial stage, a custom-made solution has been deployed that, aligned with the exponential increase of projects, workflows and continuous integrations, made the GitLab infrastructure hard and complex to scale and to maintain.\r\n\r\nThe recent migration performed, adopting a new supported Cloud Hybrid infrastructure, has contributed CERN to line up the GitLab infrastructure with both industry standards and best practices, to make the new infrastructure much more robust and performant, obtaining notable benefits in the whole deployment process.\r\n\r\nThis paper will address how this deployment process, on the road to success, has presented a series of challenges and pitfalls that have been faced during this complex migration process.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 227,
            id: 'c16889',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11531/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Posada Trobo, Ismael'],
                emailHash: '142d16b49bb39a74fe8becfe30a6ff8c',
                familyName: 'Posada Trobo',
                firstName: 'Ismael',
                name: 'Ismael Posada Trobo',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12128,
            sessionSlotId: 2697,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Version control and DevOps platform for accelerator and experiments: experience and outlook',
            uniqueId: 'c16889',
            url: '/event/459/contributions/11531/',
          },
        },
        entryType: 'Session',
        friendlyId: 9,
        id: 's12128',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2039/session-timetable.pdf',
        room: 'Chesapeake Meeting Room',
        sessionCode: '',
        sessionId: 2039,
        sessionSlotId: 2697,
        slotTitle: 'Sustainable CI and Build Infrastructure',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 5 - Sustainable and Collaborative Software Engineering',
        uniqueId: 's12128',
        url: '/event/459/sessions/2039/',
      },
      s12129: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#feffbf',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CNU',
            displayOrderKey: [0, 'Heddle, Dave'],
            emailHash: 'a9a8498d1e59570298c4ee6af05530e7',
            familyName: 'Heddle',
            firstName: 'Dave',
            name: 'Dave Heddle',
          },
          {
            affiliation: 'University of Wisconsin\u2013Madison',
            displayOrderKey: [0, 'Held, Alexander'],
            emailHash: 'fed5998fb15c9323f97c094991d6da70',
            familyName: 'Held',
            firstName: 'Alexander',
            name: 'Alexander Held',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16923: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11575/attachments/9531/13822/230509_chep_clustering.pdf',
                  id: 13822,
                  title: '230509_chep_clustering.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11575,
            description:
              'The usage of Deep Neural Networks (DNNs) as multi-classifiers is widespread in modern HEP analyses. In standard categorisation methods, the high-dimensional output of the DNN is often reduced to a one-dimensional distribution by exclusively passing the information about the highest class score to the statistical inference method. Correlations to other classes are hereby omitted.\r\nMoreover, in common statistical inference tools, the classification values need to be binned, which relies on the researcher\u0027s expertise and is often non-trivial. To overcome the challenge of binning multiple dimensions and preserving the correlations of the event-related classification information, we perform K-means clustering on the high-dimensional DNN output to create bins without marginalising any axes.\r\nWe evaluate our method in the context of a simulated cross section measurement at the CMS experiment, showing an increased expected sensitivity over the standard binning approach.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 150,
            id: 'c16923',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11575/contribution.pdf',
            presenters: [
              {
                affiliation: 'RWTH Aachen University',
                displayOrderKey: [1, 'Eich, Niclas'],
                emailHash: '27d3d3b6d4516f8f321a24fb11461a1f',
                familyName: 'Eich',
                firstName: 'Niclas',
                name: 'Niclas Eich',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12129,
            sessionSlotId: 2698,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'Binning high-dimensional classifier output for HEP analyses through a clustering algorithm',
            uniqueId: 'c16923',
            url: '/event/459/contributions/11575/',
          },
          c16924: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11572/attachments/9595/14018/ATL-COM-SOFT-2023-047.pdf',
                  id: 14018,
                  title: 'ATL-COM-SOFT-2023-047.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11572,
            description:
              'The search for the dimuon decay of the Standard Model (SM) Higgs boson looks for a tiny peak on top of a smoothly falling SM background in the dimuon invariant mass spectrum \ud835\udc5a(\ud835\udf07\ud835\udf07). Due to the very small signal-to-background ratio, which is at the level of 0.2% in the region \ud835\udc5a(\ud835\udf07\ud835\udf07) = 120\u2013130 GeV for an inclusive selection, an accurate determination of the background is of paramount importance. The \ud835\udc5a(\ud835\udf07\ud835\udf07) background spectrum is parameterised by analytic functions that can describe this distribution at the per-mill level to avoid a significant bias in the extracted signal yields. The criteria used to select the background functions are based on the spurious signal, which measures the residual signal events obtained from signal-plus-background fits to background-only MC templates. Therefore, these MC templates have to be derived from events with very high statistics in order to reduce possible fluctuations. Computationally, it is extremely expensive, if not impossible, to generate the Drell-Yan \ud835\udc4d/\ud835\udefe\u2217 \u2192 \ud835\udf07\ud835\udf07 background events with detailed simulation. Our study focuses on the use of generative models, trained on the existing fully simulated events of the ATLAS experiment in order to generate billions of events using GPUs for the spurious signal study, and to test the statistical independence of these events. This study presents an interesting alternative procedure in for the generation of events with high statistical power that could be used in the future by default in many analyses at the LHC.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 337,
            id: 'c16924',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11572/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lawrence Berkeley National Laboratory',
                displayOrderKey: [1, 'Ju, Xiangyang'],
                emailHash: 'cdae8bb08ef4238c3dbc49a841ef65f1',
                familyName: 'Ju',
                firstName: 'Xiangyang',
                name: 'Xiangyang Ju',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12129,
            sessionSlotId: 2698,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'Deep generative models for generating Drell-Yan events in the ATLAS collaboration at the LHC',
            uniqueId: 'c16924',
            url: '/event/459/contributions/11572/',
          },
          c16925: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11591/attachments/9546/13850/chep23.pdf',
                  id: 13850,
                  title: 'chep23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11591,
            description:
              'We present New Physics Learning Machine (NPLM), a machine learning-based strategy to detect data departures from a Reference model, with no prior bias on the source of discrepancy. The main idea behind the method is to approximate the optimal log-likelihood-ratio hypothesis test parametrising the data distribution with a universal approximating function, and solving its maximum-likelihood fit as a machine learning problem with a customised loss function [[1][1]]. The method returns a $p$-value that measures the compatibility of the data with the Reference model. The most interesting potential applications are model-independent New Physics searches, validation of new Monte\u00a0Carlo event generators and data quality monitoring. Using efficient large-scale implementations of kernel methods as universal approximators [[2][2]], the NPLM algorithm can be deployed on a GPU-based data acquisition system and be exploited to explore online the readout of an experimental setup. This would allow to spot out detectors malfunctioning or, possibly, unexpected anomalous patters in the data. One crucial advantage of the NPLM algorithm over standard goodness-of-fit tests routinely used in many experiments is its capability of inspecting multiple variables at once, taking care of correlations in the process. It also identifies the most discrepant region of the phase-space and it reconstructs the multidimensional data distribution, allowing for further inspection and interpretation of the results.\r\nFinally, a way for dealing with systematic uncertainties affecting the knowledge of the Reference model has been developed in a neural network framework [[3][3]] and is under construction for kernel methods. \r\n\r\n\r\n  [1]: https://link.aps.org/doi/10.1103/PhysRevD.99.015014\r\n  [2]: https://doi.org/10.1140/epjc/s10052-022-10830-y\r\n  [3]: https://doi.org/10.1140/epjc/s10052-022-10830-y',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 414,
            id: 'c16925',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11591/contribution.pdf',
            presenters: [
              {
                affiliation: 'Universita e INFN, Padova (IT)',
                displayOrderKey: [1, 'Grosso, Gaia'],
                emailHash: 'be53b20fc5091b01566d6bbfa2fcdf0d',
                familyName: 'Grosso',
                firstName: 'Gaia',
                name: 'Gaia Grosso',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12129,
            sessionSlotId: 2698,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Unbiased detection of data departures from expectations with machine learning',
            uniqueId: 'c16925',
            url: '/event/459/contributions/11591/',
          },
          c16926: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11561/attachments/9506/13781/VLohezic_DataDrivenBackgroundGAN.pdf',
                  id: 13781,
                  title: 'VLohezic_DataDrivenBackgroundGAN.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11561,
            description:
              'Data-driven methods are widely used to overcome shortcomings of Monte Carlo (MC) simulations (lack of statistics, mismodeling of processes, etc.) in experimental High Energy Physics. A precise description of background processes is crucial to reach the optimal sensitivity for a measurement. However, the selection of the control region used to describe the background process in a region of interest biases the distribution of some physics observables, rendering the use of such observables impossible in a physics analysis. Rather than discarding these events and/or observables, we propose a novel method to generate physics objects compatible with the region of interest and properly describing the correlations with the rest of the event properties. We use generative adversarial networks (GAN) for this task, as GAN are among the best performing generator models for various machine learning applications. The method is illustrated by generating a new misidentified photon for the $\\mathrm{\\gamma+Jets}$ background of the $\\mathrm{H\\rightarrow\\gamma\\gamma}$ analysis at the CERN LHC, thanks to CMS Open Data simulated samples. We demonstrate that the GAN is able to generate a coherent object within the region of interest and still correlated with the different properties of the rest of the event.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 442,
            id: 'c16926',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11561/contribution.pdf',
            presenters: [
              {
                affiliation: 'Irfu, CEA Saclay - Universit\u00e9 Paris-Saclay',
                displayOrderKey: [1, 'Lohezic, Victor'],
                emailHash: 'a723bb34cb6e7c8461a117ad88885dbf',
                familyName: 'Lohezic',
                firstName: 'Victor',
                name: 'Victor Lohezic',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12129,
            sessionSlotId: 2698,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Data driven background estimation in HEP using Generative Adversarial Networks',
            uniqueId: 'c16926',
            url: '/event/459/contributions/11561/',
          },
          c16927: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11577/attachments/9423/14028/2023_05_08%20-%20CHEP%202023%20-%20ZX%20Active%20Learning%20-%20Draft%207.pptx',
                  id: 14028,
                  title: '2023_05_08 - CHEP 2023 - ZX Active Learning - Draft 7.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11577,
            description:
              'Many theories of Beyond Standard Model (BSM) physics feature multiple BSM particles. Generally, these theories live in higher dimensional phase spaces that are spanned by multiple independent BSM parameters such as BSM particle masses, widths, and coupling constants. Fully probing these phase spaces to extract comprehensive exclusion regions in the high dimensional space is challenging. Constraints on person-power and computational resources can limit analyses to focus only on one- or two-dimensional regions of the relevant parameter spaces. Nonetheless, fully comprehensive exclusion regions, even for complex theory phase spaces, are generally desirable to maximize the utility of such BSM searches.\r\nWe are presenting an advanced analysis workflow composed of an integrated pipeline and active learning that enables such a comprehensive exclusion. The integrated pipeline automatically executes all steps of an analysis from event generation through to limit setting. Active learning is a technique to guide the sampling of the multi-dimensional phase space to find the exclusion contours in an iterative process: the sampled theory phase space points are selected such that the vicinity of the exclusion region is prioritized, reducing the sampling density in the less interesting areas. As a result, it allows searches over a larger space at the same precision, or reduces the resources required for the same search-space. We will present the implementation of the workflow with the Production and Distributed Analysis system (PanDA system) and intelligent Data Delivery Service (iDDS) in ATLAS, and showcase its abilities and utility in an extended search for a dark Z-boson using events with four-lepton final states.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 458,
            id: 'c16927',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11577/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [1, 'Weber, Christian'],
                emailHash: '3ba512a6ea1d473cca46be186e399a8f',
                familyName: 'Weber',
                firstName: 'Christian',
                name: 'Christian Weber',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12129,
            sessionSlotId: 2698,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'An Active Learning application in a dark matter search with ATLAS PanDA and iDDS',
            uniqueId: 'c16927',
            url: '/event/459/contributions/11577/',
          },
          c16928: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11588/attachments/9583/13909/Deep%20Learning%20for%20the%20Matrix%20Element%20Method.pdf',
                  id: 13909,
                  title: 'Deep Learning for the Matrix Element Method.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11588,
            description:
              'The matrix element method (MEM)  is a powerful technique that can be used for the analysis of particle collider data utilizing an *ab initio* calculation of the approximate probability density function for a collision event to be due to a physics process of interest. The most serious difficulty with the ME method, which has limited its applicability to searches for beyond-the-SM physics and precision measurements at colliders, is that it is computationally expensive. Complex final states can take minutes per event or more to calculate the probability densities. ML methods can be used to speed up the numerical evaluation dramatically. In this work, we explore Deep Learning based solutions to approximate MEM calculations and compare their performance with respect to existing computational benchmarks.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 587,
            id: 'c16928',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11588/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Illinois',
                displayOrderKey: [2, 'Neubauer, Mark'],
                emailHash: 'c85b5f720d647bd74e495285464c8375',
                familyName: 'Neubauer',
                firstName: 'Mark',
                name: 'Mark Neubauer',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12129,
            sessionSlotId: 2698,
            startDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Deep Learning for Matrix Element Method',
            uniqueId: 'c16928',
            url: '/event/459/contributions/11588/',
          },
        },
        entryType: 'Session',
        friendlyId: 10,
        id: 's12129',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2040/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VIII',
        sessionCode: '',
        sessionId: 2040,
        sessionSlotId: 2698,
        slotTitle: 'Machine Learning in Analysis',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#1f1f02',
        title: 'Track 6 - Physics Analysis Tools',
        uniqueId: 's12129',
        url: '/event/459/sessions/2040/',
      },
      s12130: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#1a3f14',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Nebraska-Lincoln',
            displayOrderKey: [0, 'Weitzel, Derek'],
            emailHash: 'ca0d239ab6ae0ec500eff8eb24779855',
            familyName: 'Weitzel',
            firstName: 'Derek',
            name: 'Derek Weitzel',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Wiebalck, Arne'],
            emailHash: 'a7f74ac17e363dd0ab1ffe2342ba5c5f',
            familyName: 'Wiebalck',
            firstName: 'Arne',
            name: 'Arne Wiebalck',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17094: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11607/attachments/9490/13756/azaytsev_BNL_SDCC_B725_DC_CHEP2023_v3.pdf',
                  id: 13756,
                  title: 'azaytsev_BNL_SDCC_B725_DC_CHEP2023_v3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11607,
            description:
              'Computational science, data management and analysis have been key factors in the success of Brookhaven National Laboratory\u0027s scientific programs at the Relativistic Heavy Ion Collider (RHIC), the National Synchrotron Light Source (NSLS-II), the Center for Functional Nanomaterials (CFN), and in biological, atmospheric, and energy systems science, Lattice Quantum Chromodynamics (LQCD) and Materials Science, as well as our participation in international research collaborations, such as the ATLAS Experiment at Europe\u0027s Large Hadron Collider (LHC) at CERN (Switzerland) and the Belle II Experiment at KEK (Japan). The construction of a new data center is an acknowledgement of the increasing demand for computing and storage services at BNL in the near term and enable the Lab to address the needs of the future experiments at the High-Luminosity LHC at CERN and the Electron-Ion Collider (EIC) at BNL in the long term. The Computing Facility Revitalization (CFR) project is aimed at repurposing the former National Synchrotron Light Source (NSLS-I) building as the new data center for BNL. The construction of the new data center was finished in 2021Q3, and it was delivered for production in early FY2022 for all collaborations supported by the Scientific Data and Computing Center (SDCC), including: STAR, PHENIX and sPHENIX experiments at RHIC collider at BNL, the Belle II Experiment at KEK (Japan), and the Computational Science Initiative at BNL (CSI). The extension of the central network systems into the new data center and the migration of a significant portion of IT load and services from the existing data center to the new data center has been underway for the duration of the first year of operations. The deployment of four new IBM TS4500 tape libraries was carried out in FY2022 as well for BNL ATLAS Tier-1 Site and sPHENIX experiment at RHIC for which the first period of data taking is expected to be carried out later in FY2023. This presentation will highlight the key mechanical, electrical, and networking components of the new data center in its final configuration as used in production since 2021Q4. Also, we will overview the IT payload deployment operations performed so far in the new data and describe plans to complete the gradual IT equipment replacement and migration from the old data center into the new one to be completed by the end of FY2023 (Sep 30, 2023). We will also show the expected state of occupancy and infrastructure utilization for the new data center up to FY2027 and further upgrade steps for its infrastructure needed in that period.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 556,
            id: 'c17094',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11607/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory (BNL)',
                displayOrderKey: [1, 'Zaytsev, Alexandr'],
                emailHash: '2628a966cb60c5635940b139d5df8cb9',
                familyName: 'Zaytsev',
                firstName: 'Alexandr',
                name: 'Mr Alexandr Zaytsev',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12130,
            sessionSlotId: 2699,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Finalizing Transition to the New Data Center at BNL',
            uniqueId: 'c17094',
            url: '/event/459/contributions/11607/',
          },
          c17095: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11639/attachments/9438/13684/CHEP-2023-05.pdf',
                  id: 13684,
                  title: 'CHEP-2023-05.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11639,
            description:
              'Moving towards Net-Zero requires robust information to enable good decision making at all levels: covering hardware procurement, workload management and operations, as well as higher level aspects encompassing grant funding processes and policy framework development.\r\n\r\nThe IRISCAST project is a proof-of-concept study funded as part of the UKRI Net-Zero Scoping Project. We have performed an audit of carbon costs across a multi-site heterogenous infrastructure by collecting and analysing snapshots of actual usage across different facilities within the IRIS community (https://iris.ac.uk). This combines usage information with an analysis of the embodied costs and careful mapping and consideration of the underlying assumptions to produce an estimate of the overall carbon cost, the key elements that contribute to it, and the important metrics needed to measure it. We present our key findings, lessons learned, and recommendations.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 505,
            id: 'c17095',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11639/contribution.pdf',
            presenters: [
              {
                affiliation: 'Queen Mary University of London',
                displayOrderKey: [0, 'Owen, Alex'],
                emailHash: 'b0f747153321501147cfa4df26d6530b',
                familyName: 'Owen',
                firstName: 'Alex',
                name: 'Alex Owen',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12130,
            sessionSlotId: 2699,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Measuring Carbon: Net-Zero and the IRISCAST Project',
            uniqueId: 'c17095',
            url: '/event/459/contributions/11639/',
          },
          c17096: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11644/attachments/9284/13811/CHEP2023.pdf',
                  id: 13811,
                  title: 'CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11644,
            description:
              'LUX-ZEPLIN (LZ) is a direct detection dark matter experiment currently operating at the Sanford Underground Research Facility (SURF) in Lead, South Dakota. The core component is a liquid xenon time projection chamber with an active mass of 7 tonnes. \r\nTo meet the performance, availability, and security requirements for the LZ DAQ, Online, Slow Control and data transfer systems located at SURF, we have developed and implemented a standalone IT infrastructure. It consists of a fully redundant 10 Gigabit network spanning underground and surface locations with hybrid virtual/physical data centers in both places. We employ virtualization, redundant firewalls, central authentication and user management, a web portal, 2-factor authentication for all remote access, VPN, fine-grained authorization and role management via a central directory, SAML identity and service providers, central configuration management, logging, monitoring, multiple relational databases, and frequent on-site and offsite backups. We exclusively use Open Source tools to provide these services.\r\nData is sent from SURF to the National Energy Sciences Computer Center (NERSC) in Berkeley, California which provides CPU and storage for reconstruction and analysis. \r\nThe system has been running reliably since its installation at SURF in 2019. We currently manage about 100 physical and virtual servers and almost 300 user accounts with 10-20 users online at any time.\r\nThis presentation will give an overview of the system and report on the operational experience.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 411,
            id: 'c17096',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11644/contribution.pdf',
            presenters: [
              {
                affiliation: 'SLAC National Accelerator Laboratory',
                displayOrderKey: [1, 'Luitz, Steffen'],
                emailHash: '708dd9f5f93f23885fb6666c3137b9ae',
                familyName: 'Luitz',
                firstName: 'Steffen',
                name: 'Dr Steffen Luitz',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12130,
            sessionSlotId: 2699,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'The IT infrastructure of the LUX-ZEPLIN experiment at SURF',
            uniqueId: 'c17096',
            url: '/event/459/contributions/11644/',
          },
          c17097: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11612/attachments/9363/13573/CHEP2023%20-%20T2%20Environmental%20Impact.pdf',
                  id: 13573,
                  title: 'CHEP2023 - T2 Environmental Impact.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11612/attachments/9363/15185/publication_right_form_enviro.pdf',
                  id: 15185,
                  title: 'publication_right_form_enviro.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11612,
            description:
              'Recent years have seen an increasing interest in the environmental impact, especially the carbon footprint, generated by the often large scale computing facilities used by the communities represented at CHEP. As this is a fairly new requirement, this information is not always readily available, especially at universities and similar institutions which do not necessarily see large scale computing provision as their core competency. Here we present the results of a survey of a large WLCG Tier 2 with respect to power usage and carbon footprint leveraging all sources of information available to us: Power usage data collected from built-in host level monitoring and external aggregates from the power distribution units at rack level; data sheets provided by the hardware manufacturers and the specifics of data centre that hosts our infrastructure.\r\nWe show that it is possible to estimate the environmental impact without having to invest in dedicated monitoring equipment, but also discuss the limitations of this approach.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 358,
            id: 'c17097',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11612/contribution.pdf',
            presenters: [
              {
                affiliation: 'Imperial College',
                displayOrderKey: [2, 'Whitehouse, Dan'],
                emailHash: '4fe248d8fecd9c8e3b6463989e9959d7',
                familyName: 'Whitehouse',
                firstName: 'Dan',
                name: 'Dan Whitehouse',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12130,
            sessionSlotId: 2699,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Estimating the environmental impact of a large Tier 2',
            uniqueId: 'c17097',
            url: '/event/459/contributions/11612/',
          },
          c17098: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11643/attachments/9369/13587/tecnopolo.pdf',
                  id: 13587,
                  title: 'pellegrino_migrating_cnaf_dc.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11643,
            description:
              'The INFN Tier1 data center is currently located in the premises of the Physics Department of the University of Bologna, where CNAF is also located. During 2023 it will be moved to the \u201cTecnopolo\u201d, the new facility for research, innovation, and technological development in the same city area; the same location is also hosting Leonardo, the pre-exascale supercomputing machine managed by CINECA, co-financed as part of the EuroHPC Joint Undertaking, 4th ranked in the top500 November 2022 list. \r\nThe construction of the new CNAF data center consists of two phases, corresponding to the computing requirements of LHC: Phase 1 involves an IT power of 3 MW, and Phase 2, starting from 2025, involves an IT power up to 10 MW. \r\nThe new datacenter is designed to cope with the computing requirements of the data taking of the HL-LHC experiments, in the time spanning from 2026 to 2040 and will provide, at the same time, computing services for several other INFN experiments and projects, not only belonging to the HEP domain. The co-location with Leonardo opens wider possibilities to integrate HTC and HPC resources and the new CNAF datacenter will be tightly coupled with it, allowing access from a single entrypoint to resources located at CNAF and provided by the supercomputer. Data access from both infrastructures will be transparent to users. \r\nIn this presentation we describe the new data center design, providing a status update on the migration, and we focus on the Leonardo integration showing the results of the preliminary tests to access it from the CNAF access points.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 392,
            id: 'c17098',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11643/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN-CNAF',
                displayOrderKey: [7, 'Pellegrino, Carmelo'],
                emailHash: 'e4ac6dabf304314d7b3e624fd81a29d8',
                familyName: 'Pellegrino',
                firstName: 'Carmelo',
                name: 'Dr Carmelo Pellegrino',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12130,
            sessionSlotId: 2699,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Migrating the INFN-CNAF datacenter to the Bologna Tecnopolo -  a status update',
            uniqueId: 'c17098',
            url: '/event/459/contributions/11643/',
          },
          c17099: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11609/attachments/9396/13894/chep2023.pdf',
                  id: 13894,
                  title: 'chep2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11609,
            description:
              'Queen Mary University of London (QMUL) as part of the refurbishment of one of its\u0027s data centres has installed water to water heat pumps to use the heat produced by the computing servers to provide heat for the university via a district heating system. This will enable us to reduce the use of high carbon intensity natural gas heating boilers, replacing them with electricity which has a lower carbon intensity due to the contribution from wind, solar, hydroelectric, nuclear, biomass sources of power sources. \r\n\r\nThe QMUL GridPP cluster today provides 15PB of storage and over 20K jobs slots mainly devoted to the ATLAS experiment. The data centre that houses the QMUL GridPP cluster, was originally commissioned in 2004. By 2020 it was in significant need of refurbishment. The original design had a maximum power capacity of 200KW, no hot/cold aisle containment, down flow air conditioning units using refrigerant cooling and no raised floor or ceiling plenum.\r\n\r\nThe main requirements of the refurbishment are: To significantly improve the energy efficiency and reduce the carbon usage of the University; Improve the availability and reliability of the power and cooling; Increase the capacity of the facility to provide for future expansion; Provide a long term home for the GridPP cluster to support the computing needs of the LHC and other new large science experiments (SKA/LSST) into the next decade.\r\n\r\nAfter taking into account the future requirements and likely funding allocation, floor space in the datacentre and the space available to house the cooling equipment the following design was chosen: A total power capacity of 390KW with redundant feeds to each rack; 39 racks with an average of 10KW of power per rack (flexable up to 20KW); An enclosed hot aisle design with in row cooling units using water cooling; water to water heat pumps connected to the universities district heating system\r\n\r\nAn overview of the project, it\u0027s status and expected benefits in power and carbon saving are presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 180,
            id: 'c17099',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11609/contribution.pdf',
            presenters: [
              {
                affiliation: 'Queen Mary University of London',
                displayOrderKey: [1, 'Traynor, Daniel'],
                emailHash: 'd784cb31ed0d1334efa12f3200f768f0',
                familyName: 'Traynor',
                firstName: 'Daniel',
                name: 'Daniel Traynor',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12130,
            sessionSlotId: 2699,
            startDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Data Centre Refurbishment with the aim of Energy Saving and Achieving Carbon Net Zero',
            uniqueId: 'c17099',
            url: '/event/459/contributions/11609/',
          },
        },
        entryType: 'Session',
        friendlyId: 11,
        id: 's12130',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2041/session-timetable.pdf',
        room: 'Marriott Ballroom IV',
        sessionCode: '',
        sessionId: 2041,
        sessionSlotId: 2699,
        slotTitle: 'Computing Centre Infrastructure',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#f1ffef',
        title: 'Track 7 - Facilities and Virtualization',
        uniqueId: 's12130',
        url: '/event/459/sessions/2041/',
      },
      s12131: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#e3f2d3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Gazzarrini, Elena'],
            emailHash: '39921bb8f8e49b6f44e880111530f493',
            familyName: 'Gazzarrini',
            firstName: 'Elena',
            name: 'Elena Gazzarrini',
          },
          {
            affiliation: 'DESY',
            displayOrderKey: [0, 'Hernandez Villanueva, Michel'],
            emailHash: '19113800d40b0ae945d3d902953e2b1c',
            familyName: 'Hernandez Villanueva',
            firstName: 'Michel',
            name: 'Michel Hernandez Villanueva',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17135: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11686/attachments/9350/13554/CHEP,%20ScienceBox%202.0%20DEMO.mp4',
                  id: 13554,
                  title: 'CHEP, ScienceBox 2.0 DEMO.mp4',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11686/attachments/9350/13984/CHEP,%20ScienceBox%202.0.pdf',
                  id: 13984,
                  title: 'CHEP, ScienceBox 2.0.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11686,
            description:
              'In this contribution we describe the 2022 reboot of the ScienceBox project, the containerised SWAN/CERNBox/EOS demonstrator package for CERN storage and analysis services. We evolved the original implementation to make use of Helm charts across the entire dependency stack. Charts have become the de-facto standard for application distribution and deployment in managed clusters (e.g., Kubernetes, OpenShift), providing developers and operators with a rich ecosystem of tools to benefit from, as well as the handles to configure applications and rollout changes in a programmatic way.\r\n\r\nAt the same time, we incorporated in ScienceBox the major architectural update to CERNBox, replacing the previous PHP backend with distributed microservices based on Reva. Besides enhancing our existing use cases, the new CERNBox implementation enables and streamlines interoperability with additional applications and sites deployed under the same technology.\r\n\r\nWe present this update as a self-contained and easy-to-use package with minimal dependencies and with the same goals as the original ScienceBox: Provide a sandbox to evaluate the storage, sharing, and analysis services we run at CERN on external premises to non-CERN users. We believe there is not only a great value in releasing and contributing back to the open source projects that sustain these services, but also in describing the configuration and artifacts that make operating such complex software systems at scale possible.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 275,
            id: 'c17135',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11686/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [2, 'Bocchi, Enrico'],
                emailHash: '305e6a33685e9a0fc98912c9f4c52cf0',
                familyName: 'Bocchi',
                firstName: 'Enrico',
                name: 'Enrico Bocchi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12131,
            sessionSlotId: 2700,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title:
              'ScienceBox 2.0: Evolving the demonstrator package for CERN storage and analysis services',
            uniqueId: 'c17135',
            url: '/event/459/contributions/11686/',
          },
          c17136: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11698/attachments/9424/13666/20230509Schneide.pptx',
                  id: 13666,
                  title: '20230509Schneide.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11698,
            description:
              'In the frame of the German NFDI (National Research Data Infrastructure), by now 27 consortia across all domains of science have been setup in order to enhance the FAIR usage and re-usage of scientific data. The consortium PUNCH4NFDI, composed of the German particle, astroparticle, hadron\u0026nuclear, and astrophysics communities, has been approved for initially 5 years of significant funding.\r\n\r\nOn its way towards its still visionary science data platform \u2013 a biotope for the entire lifecycle of digital research products \u2013 PUNCH4NFDI has already made substantial achievements. To provide a federated infrastructure for the involved communities, prototypes of the Computer4PUNCH and Storage4PUNCH servers have been setup that employ established components and middleware from the WLCG community. Existing workflow execution engines are evaluated and adopted for an increasing number of usecases from the participating science disciplines. Work on overarching metadata schemata and related services is ongoing. Furthermore, a set of outreach and education \u0026 training activities is being devised, and the integration of the consortium and the disciplines represented by it into the greater German \u201cNational Research Data Infrastructure\u201d is progressing, respecting European and international boundary conditions and efforts.\r\n\r\nThis contribution lays out the plan of the consortium and presents what it could contribute to the joint effort of the international HEP community.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 89,
            id: 'c17136',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11698/contribution.pdf',
            presenters: [
              {
                affiliation: 'DESY',
                displayOrderKey: [1, 'Schneide, Christiane'],
                emailHash: '24278540a0ed47e1e73eab65e77faa89',
                familyName: 'Schneide',
                firstName: 'Christiane',
                name: 'Christiane Schneide',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12131,
            sessionSlotId: 2700,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'First results from the PUNCH4NFDI Consortium',
            uniqueId: 'c17136',
            url: '/event/459/contributions/11698/',
          },
          c17137: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11679/attachments/9377/13599/chep-eosccz-chudoba.pptx',
                  id: 13599,
                  title: 'chep-eosccz-chudoba.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11679,
            description:
              'Planned EOSC-CZ projects will significantly improve data management in many scientific fields in the Czech Republic. Several calls for projects are under preparation according to the implementation architecture document created in 2021. Emerging National data infrastructure will build basic infrastructure with significant storage capacity for long term archive of scientific data and their accessibility from computing resources. National metadata directory project covers findability and interoperability of data. National repository platform project will operate storage services and related services like authentication and authorization. The system will support already existing data repositories to avoid data duplication, but still to ensure unified access to data. Another project will cover education of researchers and specialists for data curation. About eight additional projects will cover different scientific fields. \r\nThe prepared project for physical sciences will bring together small groups producing data on apparatuses in different laboratories with large projects from high energy physics. Many HEP projects are well advanced in ensuring FAIR principles in data management. Some of their data management tools can be used by small groups if sufficient support is available. We present several examples of differences in requirements on data volumes, their structure and description via metadata together with planned solution how to spread FAIR standards to all participating physics projects.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 485,
            id: 'c17137',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11679/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Chudoba, Jiri'],
                emailHash: '543ad8ae46ecd1be94327a51c4495526',
                familyName: 'Chudoba',
                firstName: 'Jiri',
                name: 'Jiri Chudoba',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12131,
            sessionSlotId: 2700,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'EOSC-CZ plans for physical sciences',
            uniqueId: 'c17137',
            url: '/event/459/contributions/11679/',
          },
          c17138: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11683/attachments/9307/13504/CHEP2023%20-%20Facilitating%20the%20preservation%20of%20LHCb%20analyses.pdf',
                  id: 13504,
                  title: 'CHEP2023 - Facilitating the preservation of LHCb analyses.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11683,
            description:
              'High Energy Physics experiments at the Large Hadron Collider generate petabytes of data that go though multiple transformation before final analysis and paper publication. Recording the provenance of these data is therefore crucial to maintain the quality of the final results. While the tools are in place within LHCb to keep this information for the common experiment-wide transforms, analysts have to implement solutions themselves for the steps dealing with ntuples. The gap between centralised and interactive processing can become problematic. In order to facilitate the task, ntuples extracted by LHCb analysts via so-called \u201cAnalysis Productions\u201d are tracked in the experiment bookkeeping database and can be enriched with extra information about their meaning and intended use. This information can then be used to access the data more easily: a set of Python tools allow locating the files based on their metadata and integrating their processing within analysis workflows. The tools are designed with the intention of ensuring analysis code continues to be functional into the future and are robust against evolutions in how data is accessed. This paper presents the integration of these new tools within the LHCb codebase and demonstrates how they will be used in LHCb data processing and analysis.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 564,
            id: 'c17138',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11683/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Couturier, Benjamin'],
                emailHash: '244c8becc3b603c6e5722f14640d8e65',
                familyName: 'Couturier',
                firstName: 'Benjamin',
                name: 'Benjamin Couturier',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12131,
            sessionSlotId: 2700,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Facilitating the preservation of LHCb analyses',
            uniqueId: 'c17138',
            url: '/event/459/contributions/11683/',
          },
          c17139: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11699/attachments/9315/13512/CHEP2023_Slides.pdf',
                  id: 13512,
                  title: 'CHEP2023_Slides.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11699,
            description:
              'The increasingly pervasive and dominant role of machine learning (ML) and deep learning (DL) techniques in High Energy Physics is posing challenging requirements to effective computing infrastructures on which AI workflows are executed, as well as demanding requests in terms of training and upskilling new users and/or future developers of such technologies.\r\n\r\nIn particular, a growth in the request for training opportunities to become proficient in exploiting programmable hardware capable of delivering low latencies and low energy consumption, like FPGAs, is observed. While training opportunities on generic ML/DL concepts is rich and quite wide in the coverage of sub-topics, a gap is observed in the delivery of hands-on tutorials on ML/DL on FPGAs that can scale to a relatively large number of attendants and that can give access to a relatively diverse set of ad-hoc hardware with different hardware specs. \r\n\r\nA pilot course on ML/DL on FPGAs - born from the  collaboration of INFN-Bologna, the University of Bologna and INFN-CNAF - has been successful in paving the way for the creation of a line of work dedicated to maintaining and expanding an ad-hoc scalable toolkit for similar courses in the future. The practical sessions are based on virtual machines (for code development, no FPGAs), in-house cloud platforms (INFN-cloud infrastructure equipped with AMD/Xilinx Alveo FPGA), Amazon AWS instances for project deployment on FPGAs - all complemented by docker containers with the full environments for the DL frameworks used, as well as Jupyter Notebooks for interactive exercises. The current results and plans of work along the consolidation of such a toolkit will be presented and discussed.\r\n\r\nFinally, a software ecosystem called Bond Machine, capable of dynamically generate computer architectures that can be synthesised in FPGA, is being considered as a suitable alternative to teach FPGA programming without entering into the low-level details, thanks to the hardware abstraction it offers which can simplify the interaction with FPGAs.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 597,
            id: 'c17139',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11699/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Bologna',
                displayOrderKey: [1, 'Lorusso, Marco'],
                emailHash: 'f98ed8cd412de7256d84f55ef5463ec2',
                familyName: 'Lorusso',
                firstName: 'Marco',
                name: 'Marco Lorusso',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12131,
            sessionSlotId: 2700,
            startDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Scalable training on scalable infrastructures for programmable hardware',
            uniqueId: 'c17139',
            url: '/event/459/contributions/11699/',
          },
          c17315: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11711/attachments/9351/14001/SMARTHEPatCHEP2023.pdf',
                  id: 14001,
                  title: 'SMARTHEPatCHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11711,
            description:
              '**S**ynergies between **MA**chine learning, **R**eal-**T**ime analysis and **H**ybrid architectures for efficient **E**vent **P**rocessing and decision making (SMARTHEP) is a European Training Network with the aim of training a new generation of Early Stage Researchers to advance real-time decision-making, effectively leading to data-collection and analysis becoming synonymous.\r\n\r\nSMARTHEP will bring together scientists from the four major LHC collaborations which have been driving the development of real-time analysis (RTA) and key specialists from computer science and industry. By solving concrete problems as a community, SMARTHEP will bring forward a more widespread use of RTA techniques, enabling future HEP discoveries and generating impact in industry.\r\nThe students will contribute to European growth, leveraging their hands-on experience machine learning and accelerators towards concrete commercial deliverables in fields that can most profit from RTA, such as transport, manufacturing, and finance.\r\n\r\nThis contribution presents the training and outreach plan for the network, as well as some of its early results, and is intended as an opportunity for further collaboration and feedback from the CHEP community.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 578,
            id: 'c17315',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11711/contribution.pdf',
            presenters: [
              {
                affiliation: 'Dortmund',
                displayOrderKey: [2, 'Gooding, Jamie'],
                emailHash: '90afc4248ea99d611fa89b3afb3c532d',
                familyName: 'Gooding',
                firstName: 'Jamie',
                name: 'Jamie Gooding',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12131,
            sessionSlotId: 2700,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'The SMARTHEP European Training Network',
            uniqueId: 'c17315',
            url: '/event/459/contributions/11711/',
          },
        },
        entryType: 'Session',
        friendlyId: 12,
        id: 's12131',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2042/session-timetable.pdf',
        room: 'Marriott Ballroom I',
        sessionCode: '',
        sessionId: 2042,
        sessionSlotId: 2700,
        slotTitle: 'Collaborative Infrastructure',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#253f08',
        title: 'Track 8 - Collaboration, Reinterpretation, Outreach and Education',
        uniqueId: 's12131',
        url: '/event/459/sessions/2042/',
      },
      s12132: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#92b6db',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Wenzel, Sandro'],
            emailHash: '7e8bcc2855b51c4e93b19e095a8b2ec9',
            familyName: 'Wenzel',
            firstName: 'Sandro',
            name: 'Sandro Wenzel',
          },
          {
            affiliation: 'University of Washington (US)',
            displayOrderKey: [0, 'Schaarschmidt, Jana'],
            emailHash: '6f556891cf4ef30c552a4ac4aa1c87dc',
            familyName: 'Schaarschmidt',
            firstName: 'Jana',
            name: 'Jana Schaarschmidt',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17170: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11749/attachments/9370/14005/slides_CHEP2023.pdf',
                  id: 14005,
                  title: 'slides_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11749,
            description:
              'Future e+e- colliders are crucial to extend the search for new phenomena possibly related to the open questions that the Standard Model presently does not explain. Among the major physics programs, the flavor physics program requires particle identification (PID) performances well beyond that of most detectors designed for the current generation. The cluster counting, which measures the number of primary ionizations (dN/dx) instead of the energy loss (dE/dx) along the particle trajectory in a gaseous detector, represents the most promising breakthrough in PID. The Poissonian nature of the dN/dx offers a more statistically significant way of ionization measurement, which makes the dN/dx potentially has a resolution two times better than the dE/dx. Drift chamber (DC) with cluster counting has been proposed as the future advanced detector candidates for Circular Electron Positron Collider (CEPC) and Future Circular Collider (FCC).\r\n\r\nMachine learning (ML) algorithms, which are designed to exploit large datasets to reduce complexity and find new features in data, are the state-of-the-art in PID. The reconstruction of dN/dx measurement needs to determine the number of peaks associated with the primary ionizations in the induced current waveform in a DC cell. The major challenges of the reconstruction are to detect peaks in the highly pileup and noisy situations, and to discriminate the peaks formed by the primary and secondary ionizations. Traditional method, such as taking derivatives, can hardly reach the required efficiency due to the inefficient use of the information. In this study, a two-step ML based algorithm is developed for the dN/dx reconstruction. The algorithm consists of an RNN-based peak finding model, and a CNN-based discrimination model. According to the simulated results, the performance of the ML algorithm surpasses the derivative algorithm in terms of detection efficiency and resolution. The algorithm is further demonstrated by analyzing the test beam data taken at CERN and preliminary results will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 138,
            id: 'c17170',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11749/contribution.pdf',
            presenters: [
              {
                affiliation: 'Institute of High Energy Physics',
                displayOrderKey: [8, 'Wu, Linghui'],
                emailHash: '3a059189f79f9babf3bbc4b69f5c4333',
                familyName: 'Wu',
                firstName: 'Linghui',
                name: 'Dr Linghui Wu',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12132,
            sessionSlotId: 2701,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'A deep-learning reconstruction algorithm for cluster counting',
            uniqueId: 'c17170',
            url: '/event/459/contributions/11749/',
          },
          c17171: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11729/attachments/9540/13940/CHEP2023%20ML-Based%20Reconstruction%20for%20LArTPCs.pdf',
                  id: 13940,
                  title: 'CHEP2023 ML-Based Reconstruction for LArTPCs.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11729,
            description:
              'Recent inroads in Computer Vision (CV), enabled by Machine Learning (ML), have motivated a new approach to the analysis of particle imaging detector data. Unlike previous efforts which tackled isolated CV tasks, this paper introduces an end-to-end, ML-based data reconstruction chain for Liquid Argon Time Projection Chambers (LArTPCs), the state-of-the-art in precision imaging at the intensity frontier of neutrino physics. The chain is a multi-task network cascade which combines voxel-level feature extraction using Sparse Convolutional Neural Networks and particle superstructure formation using Graph Neural Networks. Each individual algorithm incorporates physics-informed inductive biases, while their collective hierarchy enforces a causal relashionship between them. The output is a comprehensive description of an event that may be used for high-level physics inference. The chain is end-to-end optimizable, eliminating the need for time-intensive manual software adjustments. The ICARUS detector, part of the short baseline neutrino (SBN) program at Fermilab, is the largest LArTPC built and operated to date and is used to assess the performance of this reconstruction chain in a realistic setting.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 309,
            id: 'c17171',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11729/contribution.pdf',
            presenters: [
              {
                affiliation: 'SLAC',
                displayOrderKey: [1, 'Drielsma, Francois'],
                emailHash: '5856987511645ff4ad66b5a82a58d76b',
                familyName: 'Drielsma',
                firstName: 'Francois',
                name: 'Francois Drielsma',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12132,
            sessionSlotId: 2701,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Scalable, End-to-End, Machine-Learning-Based Data Reconstruction Chain for Particle Imaging Detectors',
            uniqueId: 'c17171',
            url: '/event/459/contributions/11729/',
          },
          c17172: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11748/attachments/9580/14256/HyperTrack_Mieskolainen_CHEP2023_v1.pdf',
                  id: 14256,
                  title: 'HyperTrack_Mieskolainen_CHEP2023_v1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11748,
            description:
              'I will introduce a new neural algorithm -- HyperTrack, designed for exponentially demanding combinatorial inverse problems of high energy physics final state reconstruction and high-level analysis at the LHC and beyond. Many of these problems can be formulated as clustering on a graph resulting in a hypergraph. The algorithm is based on a machine learned geometric-dynamical input graph constructor and a neural network operating on that graph. The neural model is built using a graph neural network and a set transformer, which are end-to-end optimized under a fusion loss function targeting simultaneously the graph node, edge and clustering objectives. The clustering procedure can be changed according to the problem complexity requirements, from a greedy diffusion like iteration to a more computationally demanding but powerful Monte Carlo search based. I will demonstrate the scalability and physics performance of this cutting-edge approach with simulations and discuss possible future directions towards a hybrid quantum computer algorithm.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 425,
            id: 'c17172',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11748/contribution.pdf',
            presenters: [
              {
                affiliation: 'Imperial College London',
                displayOrderKey: [1, 'Mieskolainen, Mikael'],
                emailHash: 'ee76e5592c214d4d7b6402acbfcb7f8c',
                familyName: 'Mieskolainen',
                firstName: 'Mikael',
                name: 'Mikael Mieskolainen',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12132,
            sessionSlotId: 2701,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'HyperTrack: neural combinatorics for high energy physics',
            uniqueId: 'c17172',
            url: '/event/459/contributions/11748/',
          },
          c17173: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11727/attachments/9547/13851/2023-05-09%20CHEP%202023.pdf',
                  id: 13851,
                  title: '2023-05-09 CHEP 2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11727,
            description:
              'Modern neutrino experiments employ hundreds to tens of thousands of photon detectors to detect scintillation photons produced from the energy deposition of charged particles. A traditional approach of modeling individual photon propagation as a look-up table requires high computational resources, and therefore it is not scalable for future experiments with multi-kiloton target volume.\r\n\r\nWe propose a new approach using SIREN, an implicit neural representation with periodic activation functions, to model the look-up table as a 3D scene. It reproduces the acceptance map with high accuracy using orders of magnitude less number of parameters than the look-up table. As a continuous and differentiable parameterization, SIREN also represents a smooth gradient surface. As such, it allows downstream applications such as inverse problem-solving and gradient-based optimizations. We demonstrate a data-driven method to optimize the SIREN model and an application of reconstruction using data collected from the Deep Underground Neutrino Experiment\u0027s (DUNE) near detector prototype.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 463,
            id: 'c17173',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11727/contribution.pdf',
            presenters: [
              {
                affiliation: 'SLAC',
                displayOrderKey: [1, 'Tsang, Patrick'],
                emailHash: 'eeeda9eed6afb42823456c56d7140b24',
                familyName: 'Tsang',
                firstName: 'Patrick',
                name: 'Patrick Tsang',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12132,
            sessionSlotId: 2701,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Implicit Neural Representation as a Differentiable Surrogate for Photon Propagation in a Monolithic Neutrino Detector',
            uniqueId: 'c17173',
            url: '/event/459/contributions/11727/',
          },
          c17174: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11757/attachments/9214/13375/ACha_CHEP_May23.pdf',
                  id: 13375,
                  title: 'ACha_CHEP_May23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11757,
            description:
              'The Deep Underground Neutrino Experiment (DUNE) will operate four large-scale Liquid-Argon Time-Projection Chambers (LArTPCs) at the far site in South Dakota, producing high-resolution images of neutrino interactions.\r\n\r\nLArTPCs represent a step-change in neutrino interaction imaging and the resultant images can be highly detailed and complex. Extracting the maximum value from LArTPC hardware requires correspondingly sophisticated pattern-recognition software to interpret signals from the detectors as physically meaningful objects that form the inputs to physics analyses. A critical component is the identification of the neutrino interaction vertex, which is non-trivial due to the interaction occurring at any point within the detector volume. Subsequent reconstruction algorithms use this location to identify the individual primary particles and ensure they each result in a separate reconstructed particle.\r\n\r\nA new vertex-finding procedure presented in this talk integrates a U-Net performing hit-level classification into the multi-algorithm approach used by the Pandora pattern recognition framework to identify the neutrino interaction vertex. The machine learning solution is seamlessly integrated into a chain of traditional pattern-recognition algorithms incorporating knowledge of the detector, demonstrating that traditional and machine learning methods need not be mutually exclusive in leveraging the potential of machine learning for neutrino physics.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 41,
            id: 'c17174',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11757/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Warwick',
                displayOrderKey: [1, 'Chappell, Andrew'],
                emailHash: 'c7c75514f9d793901384ae62b538150b',
                familyName: 'Chappell',
                firstName: 'Andrew',
                name: 'Andrew Chappell',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12132,
            sessionSlotId: 2701,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Neutrino interaction vertex-finding in a DUNE far-detector using Pandora deep-learning',
            uniqueId: 'c17174',
            url: '/event/459/contributions/11757/',
          },
          c17175: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11733/attachments/9550/13855/2023-05-09%20CHEP%20talk.pdf',
                  id: 13855,
                  title: '2023-05-09 CHEP talk.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11733,
            description:
              'The Exa.TrkX team has developed a Graph Neural Network (GNN) for reconstruction of liquid argon time projection chamber (LArTPC) data. We discuss the network architecture, a multi-head attention message passing network that classifies detector hits according to the particle type that produced them. By utilizing a heterogeneous graph structure with independent subgraphs for each 2D plane\u2019s hits and for 3D space points, the model achieves a consistent description of the neutrino interaction across all planes.\r\n\r\nPerformance results will be presented based on publicly available samples from MicroBooNE. These will include both physics performance metrics, achieving ~95% accuracy when integrated over all particle classes, and computational metrics for training on single or distributed GPU systems and for inference on CPU or GPU.\r\n\r\nWe will discuss applications of the network for additional LArTPC reconstruction tasks, such as event classification, cosmic rejection and particle instance segmentation. Prospects for integration in the data processing chains of experiments will also be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 460,
            id: 'c17175',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11733/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Cincinnati',
                displayOrderKey: [1, 'Hewes, V'],
                emailHash: 'd578b133ed7f6404d45150ba9aa143ff',
                familyName: 'Hewes',
                firstName: 'V',
                name: 'V Hewes',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12132,
            sessionSlotId: 2701,
            startDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Graph Neural Network for 3D Reconstruction in Liquid Argon Time Projection Chambers',
            uniqueId: 'c17175',
            url: '/event/459/contributions/11733/',
          },
        },
        entryType: 'Session',
        friendlyId: 13,
        id: 's12132',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2043/session-timetable.pdf',
        room: 'Hampton Roads VII',
        sessionCode: '',
        sessionId: 2043,
        sessionSlotId: 2701,
        slotTitle: 'Reconstruction',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#03070f',
        title: 'Track 9 - Artificial Intelligence and Machine Learning',
        uniqueId: 's12132',
        url: '/event/459/sessions/2043/',
      },
      s12134: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#4f144e',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CSIC',
            displayOrderKey: [0, 'Campos, Isabel'],
            emailHash: '1ba624fa939fe1db5c536ca7116de83b',
            familyName: 'Campos',
            firstName: 'Isabel',
            name: 'Isabel Campos',
          },
          {
            affiliation: 'Nikhef National institute  for subatomic physics (NL)',
            displayOrderKey: [0, 'Aaij, Roel'],
            emailHash: '6fecef0aff50325ef5ea685927dec4a5',
            familyName: 'Aaij',
            firstName: 'Roel',
            name: 'Roel Aaij',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17217: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11823/attachments/9228/13392/chep23_icecube_arm_rc3.pdf',
                  id: 13392,
                  title: 'chep23_icecube_arm_rc3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11823,
            description:
              'The IceCube experiment has substantial simulation needs and is in continuous search for the most cost-effective ways to satisfy them. The most CPU-intensive part relies on CORSIKA, a cosmic ray air shower simulation. Historically, IceCube relied exclusively on x86-based CPUs, like Intel Xeon and AMD EPYC, but recently server-class ARM-based CPUs are also becoming available, both on-prem and in the cloud.\r\nIn this paper we present our experience in running a sample CORSIKA simulation on both ARM and x86 CPUs available through Google Kubernetes Engine (GKE). We used the production binaries for the x86 instances, but had to build the binaries for ARM instances from source code, which turned out to be mostly painless. Our benchmarks show that ARM-based CPUs in GKE are not only the most cost-effective but are also the fastest in absolute terms in all the tested configurations. While the advantage is not drastic, about 20% in cost-effectiveness and less than 10% in absolute terms, it is still large enough to warrant an investment in ARM support for IceCube.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 31,
            id: 'c17217',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11823/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Sfiligoi, Igor'],
                emailHash: '3e05cf4a0269daa4b744d264789f2668',
                familyName: 'Sfiligoi',
                firstName: 'Igor',
                name: 'Igor Sfiligoi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12134,
            sessionSlotId: 2703,
            startDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Evaluation of ARM CPUs for IceCube available through Google Kubernetes Engine',
            uniqueId: 'c17217',
            url: '/event/459/contributions/11823/',
          },
          c17223: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11821/attachments/9545/13849/CHEP23%20Results%20from%20HEP-CCE.pdf',
                  id: 13849,
                  title: 'CHEP23 Results from HEP-CCE.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11821,
            description:
              'High energy physics is facing serious challenges in the coming decades due to the projected shortfall of CPU and storage resources compared to our anticipated budgets. In the past, HEP has not made extensive use of HPCs, however the U.S. has had a long term investment in HPCs and it is the platform of choice for many simulation workloads, and more recently, data processing for projects such as LIGO, the light sources, sky surveys, as well as for many AI and ML tasks. By mid to late decade, we expect on the order of 10 exaflops of peak power to be available in HPCs, and an order of magnitude more in the following decade. This is at least two orders of magnitude more than HEP requires, but it would be a significant challenge for HEP experiments to use, especially since most of the cycles will be provided by accelerators like GPUs. Can the HEP community leverage these resources to address our computational shortfalls?\r\n\r\nThe High Energy Physics Center for Computational Excellence (HEP-CCE), a 3 year pilot project which started in 2020, was formed to investigate this challenge, and provide strategies for HEP experiments to make use of HPC and other massively parallel resources. HEP-CCE functions in close co-operation with the stakeholder experiments, and  is split into 4 parts. The first is to investigate Portable Parallelization Strategies, to make use of the massive available parallelism in GPU enabled HPCs, and to engineer portable coding solutions that allow single source software to run on all architectures. The second is to tackle fine grained I/O and the related storage issues on HPCs, by enhancing the existing Darshan HPC I/O monitoring tool to handle HEP workflows and characterize those for ATLAS, CMS \u0026 DUNE, developing a I/O mimicking framework allowing scalability studies for different I/O implementations (including ROOT, HDF5) in regimes not yet accessible to HEP production jobs, using HDF5 via ROOT serialization with parallel I/O and investigating new data model with more performant I/O and offloading to GPU resources. The third looks at Event Generators, such as MadGraph and Sherpa, to convert them to run efficiently on GPUs. And the last is to understand how we can map our Complex Workflows onto HPC resources, which are very different from normal HPC workflows.\r\n\r\nIn this submission we present the results of our 3 year investigations from all 4 domains and give an outlook on recommendations for current and future HEP experiments on how to best use the U.S. HPC environment.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 263,
            id: 'c17223',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11821/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lawrence Berkeley National Laboratory',
                displayOrderKey: [0, 'Leggett, Charles'],
                emailHash: '4355184b024c148f5702ea55f82fb90a',
                familyName: 'Leggett',
                firstName: 'Charles',
                name: 'Charles Leggett',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12134,
            sessionSlotId: 2703,
            startDate: {
              date: '2023-05-09',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Results from HEP-CCE',
            uniqueId: 'c17223',
            url: '/event/459/contributions/11821/',
          },
          c17224: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11813/attachments/9388/14308/CHEP23_M100_1.pdf',
                  id: 14308,
                  title: 'CHEP23_M100_1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11813,
            description:
              'The INFN-CNAF Tier-1 located in Bologna (Italy) is a center of the WLCG e-Infrastructure providing computing power to the four major LHC collaborations and also supports the computing needs of about fifty more groups - also from non HEP research domains. The CNAF Tier1 center has been historically very active putting effort in the integration of computing resources, proposing and prototyping solutions both for extension through Cloud resources, public and private, and with remotely owned sites, as well as developing an integrated HTC+HPC system with the PRACE CINECA supercomputer center located 8Km far from the CNAF Tier-1 located in Bologna. In order to meet the requirements for the new Tecnopolo center, where the CNAF Tier-1 will be hosted, the resource integration activities keep progressing. In particular, this contribution will detail the challenges that have recently been addressed, providing opportunistic access to non standard CPU architectures, such as PowerPC and hardware accelerators (GPUs). We explain the strategy adopted to both transparently provision x86_64, ppc64le and nVidia V100 GPUs from the Marconi 100 HPC cluster managed by CINECA and to access data from the Tier1 storage system at CNAF. Finally we will discuss the results of the early experience.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 396,
            id: 'c17224',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11813/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN-CNAF',
                displayOrderKey: [1, 'Dal Pra, Stefano'],
                emailHash: '4bb37fa7bb6103869e0d7cdb7c104ea6',
                familyName: 'Dal Pra',
                firstName: 'Stefano',
                name: 'Dr Stefano Dal Pra',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12134,
            sessionSlotId: 2703,
            startDate: {
              date: '2023-05-09',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Enabling INFN-T1 to support heterogeneous computing architectures',
            uniqueId: 'c17224',
            url: '/event/459/contributions/11813/',
          },
          c17226: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11807/attachments/9286/13474/CHEP2023%20Parallel%20IO.pdf',
                  id: 13474,
                  title: 'CHEP2023 Parallel IO.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11807,
            description:
              'The computing and storage requirements of the energy and intensity frontiers will grow significantly during the Run 4 \u0026 5 and the HL-LHC era. Similarly, in the intensity frontier, with larger trigger readouts during supernovae explosions, the Deep Underground Neutrino Experiment (DUNE) will have unique computing challenges that could be addressed by the use of parallel and accelerated data-processing capabilities. Most of the requirements of the energy and intensity frontier experiments rely on increasing the role of high performance computing (HPC) in the HEP community. In this presentation, we will describe  our ongoing efforts that are focused on using HPC resources for the next generation HEP experiments. The HEP-CCE (High Energy Physics-Center for Computational Excellence) IOS (Input/Output and Storage) group has been developing approaches to map HEP data to the HDF5 , an I/O library optimized for the HPC platforms to store the intermediate HEP data. The complex HEP data products are ROOT serialized before mapping into the HDF5 format. The mapping of the data products can be designed to optimize parallel I/O. Similarly, simpler data can be directly mapped into the HDF5, which can also be suitable for offloading into the GPUs directly. We will present our works on both complex and simple data model models.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 326,
            id: 'c17226',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11807/contribution.pdf',
            presenters: [
              {
                affiliation: 'Argonne National Lab',
                displayOrderKey: [1, 'Bashyal, Amit'],
                emailHash: '5533d1e019eb0da9a1d505359c2ae58b',
                familyName: 'Bashyal',
                firstName: 'Amit',
                name: 'Amit Bashyal',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12134,
            sessionSlotId: 2703,
            startDate: {
              date: '2023-05-09',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Using parallel I/O libraries for managing HEP experimental data',
            uniqueId: 'c17226',
            url: '/event/459/contributions/11807/',
          },
          c17227: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11828/attachments/9576/14004/2023-05-09%20CHEP%20MixMaxGPU.pdf',
                  id: 14004,
                  title: '2023-05-09 CHEP MixMaxGPU.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11828/attachments/9576/13896/2023-05-09%20CHEP%20MixMaxGPU.pptx',
                  id: 13896,
                  title: '2023-05-09 CHEP MixMaxGPU.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11828,
            description:
              'Random number generation is key to many applications in a wide variety of disciplines. Depending on the application, the quality of the random numbers from a particular generator can directly impact both computational performance and critically the outcome of the calculation.\r\n\r\nHigh-energy physics applications use Monte Carlo simulations and machine learning widely, which both require high-quality random numbers. In recent years, to meet increasing performance requirements, many high-energy physics workloads leverage GPU acceleration. While on a CPU, there exist a wide variety of generators with different performance and quality characteristics, the same cannot be stated for GPU and FPGA accelerators.\r\n\r\nOn GPUs, the most common implementation is provided by cuRAND - an NVIDIA library that is not open source or peer reviewed by the scientific community. The highest-quality generator implemented in cuRAND is a version of the Mersenne Twister. Given the availability of better and faster random number generators, high-energy physics moved away from Mersenne Twister several years ago and nowadays MixMax is the standard generator in Geant4 via CLHEP.\r\n\r\nThe MixMax original design supports parallel streams with a seeding algorithm that makes it especially suited for GPU and FPGA where extreme parallelism is a key factor. In this study we implement the MixMax generator on both architectures and analyze its suitability and applicability for accelerator implementations. We evaluated the results against \u201cMersenne Twister for a Graphic Processor\u201d (MTGP32) on GPUs which resulted in 5, 13 and 14 times higher throughput when a 240, 17 and 8 sized vector space was used respectively. The MixMax generator coded in VHDL and implemented on Xilinx Ultrascale+ FPGAs, requires 50% fewer total LUTs compared to a 32-bit Mersenne Twister (MT-19337), or ~75% fewer LUTs per output bit.\r\n\r\nIn summary, the state-of-the art MixMax pseudo random number generator has been implemented on GPU and FPGA platforms and the performance benchmarked.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 164,
            id: 'c17227',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11828/contribution.pdf',
            presenters: [
              {
                affiliation: 'Imperial College London',
                displayOrderKey: [0, 'Barbone, Marco'],
                emailHash: '67f6e235a1ea8baeb03e2fbe5fd43fd9',
                familyName: 'Barbone',
                firstName: 'Marco',
                name: 'Marco Barbone',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12134,
            sessionSlotId: 2703,
            startDate: {
              date: '2023-05-09',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Fast, high-quality pseudo random number generators for heterogeneous computing',
            uniqueId: 'c17227',
            url: '/event/459/contributions/11828/',
          },
          c17228: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11820/attachments/9437/13683/XkitS%20CHEP2023-0509.pdf',
                  id: 13683,
                  title: 'XkitS CHEP2023-0509.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11820,
            description:
              'Large-scale high-energy physics experiments generate petabytes or even exabytes of scientific data, and high-performance data IO is required during their processing. However, computing and storage devices are often separated in large computing centers, and large-scale data transmission has become a bottleneck for some data-intensive computing tasks, such as data encoding and decoding, compression, sorting, etc. Data transfer can take up to 30% of the entire computing process. The more data is called in a compute task, the more obvious this cost becomes. One attractive solution to this problem is to offload some of the data processing to the storage layer. However, modifying legacy storage systems to support compute offloading is often tedious and requires an extensive understanding of the internals. \r\nTherefore, we have designed a new software framework XkitS for building computational storage systems by extending the existing storage system, EOS. The framework is deployed on EOS FTS storage servers and offloads computing tasks by calling the computing power (including CPU, FPGA, etc.) on FST. XkitS can embed multiple data processing methods into the storage layer, which can be implemented in scripting languages or evolved independently of the storage system in the form of containers. On the storage server side, XkitS implements an XRootD plugin that executes first when FTS receives a file access request. It calls the target program on the storage server by parsing the parameters of the command to open file. At this point, the input file is on the FTS storage server, and the output file is also written on it. At the end of the task execution, the file is automatically registered with the MGM metadata server. On the storage client side, XkitS is fully compatible with XRootD\u0027s API and EOS commands. Users can add tasks and parameters to be performed in the open option. XkitS processing is black-box for users, and they get the same results as they normally would, but jobs are processed faster and queues are avoided. At present, it has been tested and applied in the data processing of the Large High Altitude Air Shower Observatory (LHAASO), and the results show that the efficiency of data decoding is more than 5 times higher than the original method.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 630,
            id: 'c17228',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11820/contribution.pdf',
            presenters: [
              {
                affiliation: 'IHEP, CAS',
                displayOrderKey: [4, 'CHENG, Yaodong'],
                emailHash: '6ee3ac44f5bc2a44a90d7c47d76bd92a',
                familyName: 'CHENG',
                firstName: 'Yaodong',
                name: 'Yaodong CHENG',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12134,
            sessionSlotId: 2703,
            startDate: {
              date: '2023-05-09',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'XkitS\uff1aA computational storage framework for high energy physics based on EOS storage system',
            uniqueId: 'c17228',
            url: '/event/459/contributions/11820/',
          },
        },
        entryType: 'Session',
        friendlyId: 15,
        id: 's12134',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2045/session-timetable.pdf',
        room: 'Marriott Ballroom VII',
        sessionCode: '',
        sessionId: 2045,
        sessionSlotId: 2703,
        slotTitle: 'Software tools for Parallel Computing',
        startDate: {
          date: '2023-05-09',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffefff',
        title:
          'Track X - Exascale Science, Heterogeneous Computing and Accelerators, and Quantum Computing',
        uniqueId: 's12134',
        url: '/event/459/sessions/2045/',
      },
      s12155: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#efebc2',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'STFC-RAL',
            displayOrderKey: [0, 'Ellis, Katy'],
            emailHash: 'e4c8a78292759108e5511297386cac3a',
            familyName: 'Ellis',
            firstName: 'Katy',
            name: 'Katy Ellis',
          },
          {
            affiliation: 'KEK/IPNS',
            displayOrderKey: [0, 'Miyake, Hideki'],
            emailHash: '7ae6773a40c49ae5776d96621f5c7ab5',
            familyName: 'Miyake',
            firstName: 'Hideki',
            name: 'Hideki Miyake',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16854: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11467/attachments/9237/13403/CHEP23%20-%20CMS%20monitoring-1.pdf',
                  id: 13403,
                  title: 'CHEP23 - CMS monitoring-1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11467,
            description:
              'Data taking at the Large Hadron Collider (LHC) at CERN restarted in 2022. The CMS experiment relies on a distributed computing infrastructure based on WLCG (Worldwide LHC Computing Grid) to support the LHC Run 3 physics program. The CMS computing infrastructure is highly heterogeneous and relies on a set of centrally provided services, such as distributed workload management and data management, and computing resources hosted at almost 150 sites worldwide. Smooth data taking and processing requires all computing subsystems to be fully operational, and available computing and storage resources need to be continuously monitored. During the long shutdown between LHC Run 2 and Run 3, the CMS monitoring infrastructure has undergone major changes to increase the coverage of monitored applications and services, while becoming more sustainable and easier to operate and maintain. The used technologies are based on open-source solutions, either provided by the CERN IT department through the MONIT infrastructure, or managed by the CMS monitoring team. Monitoring applications for distributed workload management, submission infrastructure based on HTCondor, distributed data management, facilities have been ported from mostly custom-built applications to use common data flow and visualization services. Data are mostly stored in no-SQL databases and storage technologies such as ElasticSearch, VictoriaMetrics, InfluxDB and HDFS, and accessed either via programmatic APIs, Apache Spark or Sqoop jobs, or visualized preferentially using Grafana. Most CMS monitoring applications are deployed on Kubernetes clusters to minimize maintenance operations. In this contribution we present the full stack of CMS monitoring services and show how we leveraged the use of common technologies to cover a variety of monitoring applications and cope with the computing challenges of LHC Run 3.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 69,
            id: 'c16854',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11467/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Torino',
                displayOrderKey: [1, 'Legger, Federica'],
                emailHash: 'b4b1f2e02ec55f04fdbae307a3d37024',
                familyName: 'Legger',
                firstName: 'Federica',
                name: 'Federica Legger',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12155,
            sessionSlotId: 2718,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'The CMS monitoring applications for LHC Run 3',
            uniqueId: 'c16854',
            url: '/event/459/contributions/11467/',
          },
          c16855: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11474/attachments/9568/13970/2023_CHEP_BigPanDAmon_slides-2.pdf',
                  id: 13970,
                  title: '2023_CHEP_BigPanDAmon_slides-2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11474,
            description:
              'Monitoring services play a crucial role in the day-to-day operation of distributed computing systems. The ATLAS experiment at LHC uses the production and distributed analysis workload management system (PanDA WMS), which allows a million computational jobs to run daily at over 170 computing centers of the WLCG and other opportunistic resources, utilizing 600k cores simultaneously on average. The BigPanDAmon system is an essential part of the monitoring infrastructure for the ATLAS experiment that provides a wide range of views from the top-level summaries to a single computational job and its logs. Over the past few years of the PanDA WMS advancement in the ATLAS experiment several new components were developed, such as Harvester, iDDS, Data Carousel, and Global Shares. Due to its modular architecture, BigPanDAmon naturally grew into a platform where the relevant data from all PanDA WMS components and accompanying services are accumulated and displayed in the form of interactive charts and tables. Moreover the system has been adopted by other experiments beyond HEP. In this paper we describe the evolution of the BigPanDAmon system, the development of new modules, and the integration process into other experiments.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 228,
            id: 'c16855',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11474/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [0, 'Klimentov, Alexei'],
                emailHash: '6fef2a09accf1a707bc4c992c15a8438',
                familyName: 'Klimentov',
                firstName: 'Alexei',
                name: 'Alexei Klimentov',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12155,
            sessionSlotId: 2718,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'BigPanDA monitoring system evolution in the ATLAS experiment',
            uniqueId: 'c16855',
            url: '/event/459/contributions/11474/',
          },
          c16856: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11495/attachments/9431/13915/CHEP%202023%20Presentation%20-%20Kalana%20Wijethunga%20-%202.pdf',
                  id: 13915,
                  title: 'CHEP 2023 Presentation - Kalana Wijethunga - 2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11495,
            description:
              'The ALICE experiment at the CERN Large Hadron Collider relies on a massive, distributed Computing Grid for its data processing. The ALICE Computing Grid is built by combining a large number of individual computing sites distributed globally. These Grid sites are maintained by different institutions across the world and contribute thousands of worker nodes possessing different capabilities and configurations. Developing software for Grid operations that works on all nodes while harnessing the maximum capabilities offered by any given Grid site is challenging without advance knowledge of what capabilities each site offers. Site Sonar is an architecture-independent Grid infrastructure monitoring framework developed by the ALICE Grid team to monitor the infrastructure capabilities and configurations of worker nodes at sites across the ALICE Grid without the need to contact local site administrators. Site Sonar is a highly flexible and extensible framework that offers infrastructure metric collection without local agent installations at Grid sites. This paper introduces the Site Sonar Grid infrastructure monitoring framework and reports significant findings acquired about the ALICE Computing Grid using Site Sonar.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 22,
            id: 'c16856',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11495/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [6, 'Storetvedt, Maksim'],
                emailHash: 'b2cd2820aaf0912102f62b1adbc98aa2',
                familyName: 'Storetvedt',
                firstName: 'Maksim',
                name: 'Maksim Storetvedt',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12155,
            sessionSlotId: 2718,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Site Sonar - A Flexible and Extensible Infrastructure Monitoring Tool for ALICE Grid',
            uniqueId: 'c16856',
            url: '/event/459/contributions/11495/',
          },
          c16857: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11486/attachments/9275/13449/chep_2023_hammercloud_benjamin_rottler.pdf',
                  id: 13449,
                  title: 'chep_2023_hammercloud_benjamin_rottler.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11486,
            description:
              'HammerCloud (HC) is a testing service and framework for continuous functional tests, on-demand large-scale stress tests, and performance benchmarks. It checks the computing resources and various components of distributed systems with realistic full-chain experiment workflows.\r\n\r\nThe HammerCloud software was initially developed in Python 2. After support for Python 2 was discontinued in 2020, migration to Python 3 became vital in order to fulfill the latest security standards and to use the new CERN Single Sign-On, which requires Python 3.\r\n\r\nThe current deployment setup based on RPMs allowed a stable deployment and secure maintenance over several years of operations for the ATLAS and CMS experiments. However, the current model is not flexible enough to support an agile and rapid development process. Therefore, we have decided to use a containerization solution, and switched to industry-standard technologies and processes. Having an "easy to spawn" instance of HC enables a more agile development cycle and easier deployment. With the help of such a containerized setup, CI/CD pipelines can be integrated into the automation process as an extra layer of verification.\r\n\r\nA quick onboarding process for new team members and communities is essential,\r\nas there is a lot of personnel rotation and a general lack of personpower. This is achieved with the container-based setup, as developers can now work locally with a quick turnaround without needing to set up a production-like environment first. These developments empower the whole community to bravely test and prototype new ideas and deliver new types of resources or workflows to our community.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 246,
            id: 'c16857',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11486/contribution.pdf',
            presenters: [
              {
                affiliation: 'Freiburg University',
                displayOrderKey: [1, 'Rottler, Benjamin'],
                emailHash: '0cdc5a0c38fc673710de6f3fef92a42a',
                familyName: 'Rottler',
                firstName: 'Benjamin',
                name: 'Mr Benjamin Rottler',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12155,
            sessionSlotId: 2718,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Bringing the ATLAS HammerCloud setup to the next level with containerization',
            uniqueId: 'c16857',
            url: '/event/459/contributions/11486/',
          },
          c16858: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11493/attachments/9569/13971/CHEP23_Analytics.pdf',
                  id: 13971,
                  title: 'CHEP23_Analytics.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11493,
            description:
              'Operational analytics is the direction of research related to the analysis of the current state of computing processes and the prediction of the future in order to anticipate imbalances and take timely measures to stabilize a complex system. There are two relevant areas in ATLAS Distributed Computing that are currently in the focus of studies: end-user physics analysis including the forecast of samples of data popularity among users, and ranking of WLCG centers for user analysis tasks. Studies in these areas are non-trivial and require detailed knowledge of all boundary conditions, which may be numerous in large-scale distributed computing infrastructures. Forecasts of data popularity are impossible without the categorization of user tasks by their types (data transformation or physics analysis), which do not always appear on the surface but may induce noise, which introduces significant distortions for predictive analysis. Ranking the WLCG resources is also a challenging task as it is necessary to find a balance between the workload of the resource, its performance, the waiting time for jobs on it, as well as the volume of jobs that it processes. This is especially difficult in a heterogeneous computing environment, where legacy resources are used along with modern high-performance machines. We will look at these areas of research in detail and discuss what tools and methods we use in our work, demonstrating the results that we already have. The difficulties we face and how we solve them will also be described.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 248,
            id: 'c16858',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11493/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [1, 'Klimentov, Alexei'],
                emailHash: '6fef2a09accf1a707bc4c992c15a8438',
                familyName: 'Klimentov',
                firstName: 'Alexei',
                name: 'Alexei Klimentov',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12155,
            sessionSlotId: 2718,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Operational Analytics Studies for ATLAS Distributed Computing: Data Popularity Forecast and Ranking of the WLCG Centers',
            uniqueId: 'c16858',
            url: '/event/459/contributions/11493/',
          },
          c16859: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11478/attachments/9356/13562/AnalysisAndOptimization_MartaBertran.pdf',
                  id: 13562,
                  title: 'AnalysisAndOptimization_MartaBertran.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11478,
            description:
              'For LHC Run3 the ALICE experiment software stack has been completely refactored, incorporating support for multicore job execution. The new multicore jobs spawn multiple processes and threads within the payload. Given that some of the deployed processes may be short-lived, accounting for their resource consumption presents a challenge. This article presents the newly developed methodology for payload execution monitoring, which correctly accounts for the resources used by all processes within the payload. \r\n\r\nWe also present a black box analysis of the new multicore experiment software framework tracing the used resources and system function calls issued by MonteCarlo simulation jobs. Multiple sources of overhead in the processes and threads lifecycle have thus been identified. This paper describes the tracing techniques and what solutions were implemented to address them. The analysis and subsequent improvements of the code have positively impacted the resource consumption and the overall turnaround time of the payloads with a notable 35% reduction in execution time for a reference production job. We also introduce how this methodology will be used to further improve the efficiency of our experiment software and what other optimization venues are currently being pursued.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 29,
            id: 'c16859',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11478/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Bertran Ferrer, Marta'],
                emailHash: '52a8825d6326c1b1c5a615507a596723',
                familyName: 'Bertran Ferrer',
                firstName: 'Marta',
                name: 'Marta Bertran Ferrer',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12155,
            sessionSlotId: 2718,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Analysis and optimization of ALICE Run 3 multicore Grid jobs',
            uniqueId: 'c16859',
            url: '/event/459/contributions/11478/',
          },
        },
        entryType: 'Session',
        friendlyId: 8,
        id: 's12155',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2038/session-timetable.pdf',
        room: 'Marriott Ballroom II-III',
        sessionCode: '',
        sessionId: 2038,
        sessionSlotId: 2718,
        slotTitle: 'Monitoring, Testing and Analytics',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 4 - Distributed Computing',
        uniqueId: 's12155',
        url: '/event/459/sessions/2038/',
      },
      s12156: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfe555',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'FNAL',
            displayOrderKey: [0, 'Sexton, Elizabeth'],
            emailHash: 'e8fb00f4f09841337efaeff677169cd2',
            familyName: 'Sexton',
            firstName: 'Elizabeth',
            name: 'Elizabeth Sexton',
          },
          {
            affiliation: '',
            displayOrderKey: [0, 'Eulisse, Giulio'],
            emailHash: '7a6c8f49981d093b41b365a4b522378d',
            familyName: 'Eulisse',
            firstName: 'Giulio',
            name: 'Giulio Eulisse',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16890: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11545/attachments/9386/13612/ITERATIVE%20DEVELOPMENT%20OF%20THE%20ATLAS%20PUBLICATION%20TRACKING%20SYSTEM.pdf',
                  id: 13612,
                  title: 'ITERATIVE DEVELOPMENT OF THE ATLAS PUBLICATION TRACKING SYSTEM.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11545,
            description:
              'The ATLAS experiment involves almost 6000 members from approximately 300 institutes spread all over the globe and more than 100 papers published every year. This dynamic environment brings some challenges such as how to ensure publication deadlines, communication between the groups involved, and the continuity of workflows. The solution found for those challenges was automation, which was achieved through the Glance project, more specifically through the Glance Analysis systems, developed to support the analysis and publications life cycle in 2010. Now, after twelve years, in order to satisfy the experiments\u2019 most recent needs, the systems need code refactoring and database remodeling. The goal is to have only one system to accommodate all the analysis and publications workflows, the so-called ATLAS Publication Tracking system, an evolution of the current Analysis systems. This project includes a database remodeling that reflects the hierarchical relation between analyses and publications; a code base that supports non-linear workflows; the expansion of the current API so all the authorized ATLAS members can access ATLAS publication data programmatically; a service-oriented architecture for integration with external software, such as GitLab; the creation of an automatic test environment, which assures the quality of the systems on each update. The ATLAS Publication Tracking system is a long-term project being developed with an iterative and incremental approach, which ensures that the most valuable tools are implemented with priority while allowing a smooth transition between the old systems and the new one.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 332,
            id: 'c16890',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11545/contribution.pdf',
            presenters: [
              {
                affiliation: 'Universidade Federal do Rio De Janeiro',
                displayOrderKey: [0, 'Cruz, Ana Clara Loureiro'],
                emailHash: '95e5b45b487ebf07b72dc40265afade6',
                familyName: 'Cruz',
                firstName: 'Ana Clara Loureiro',
                name: 'Ana Clara Loureiro Cruz',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12156,
            sessionSlotId: 2719,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Iterative and incremental development of the ATLAS Publication Tracking system',
            uniqueId: 'c16890',
            url: '/event/459/contributions/11545/',
          },
          c16891: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11529/attachments/9320/13519/Enhancing%20data%20consistency%20in%20ATLAS%20and%20CERN%20HR%20databases%20through%20automated%20synchronization.pdf',
                  id: 13519,
                  title:
                    'Enhancing data consistency in ATLAS and CERN HR databases through automated synchronization.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11529,
            description:
              'As the largest particle physics laboratory in the world, CERN has more than 17000 collaborators spread around the globe. ATLAS, one of CERN\u2019s experiments, has around 6000 active members and 300 associate institutes, all of which must go through the standard registration and updating procedures within CERN\u2019s HR (Foundation) database. Simultaneously, the ATLAS Glance project, among other functions, also has the same goal within the ATLAS context. At the time of its first development, no tools were available to allow Glance to write into the Foundation database, therefore the solution put into place was to duplicate data. This however proved to be inefficient as the databases grew over time. Information had to be constantly updated manually by the ATLAS Secretariat to keep members and institutes data (such as names, employment information and authorship status) coherent between databases. Today, equipped with new tools, the Glance system is about to change its relationship with Foundation: a sole source of truth for the data shall be determined, removing the duplication of information. This includes automating a series of internal processes so the ATLAS secretariat need not to manually intervene to keep both databases synchronized. For this, a workflow had to be developed so that the previous manual work could be successfully replaced considering the multitude of possible actions by the Secretariat. The remodeling of the current structure of the database, along with the refactoring of the code, shall also be required to establish an easy communication between the two systems. Finally, a number of tools developed on Foundation\u2019s side (such as SQL procedures and APIs) have to be put in place to enable the writing and reading between databases.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 368,
            id: 'c16891',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11529/contribution.pdf',
            presenters: [
              {
                affiliation: 'Universidade Federal do Rio De Janeiro',
                displayOrderKey: [4, 'Aleksandravicius, Gabriel de Arag\u00e3o'],
                emailHash: '6a3b297111b38595c27e40298be12d22',
                familyName: 'Aleksandravicius',
                firstName: 'Gabriel de Arag\u00e3o',
                name: 'Gabriel de Arag\u00e3o Aleksandravicius',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12156,
            sessionSlotId: 2719,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Enhancing data consistency in ATLAS and CERN databases through automated synchronization',
            uniqueId: 'c16891',
            url: '/event/459/contributions/11529/',
          },
          c16892: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11542/attachments/9335/13605/CHEP%20-%20Glance%20Search%20Library%20(1).pdf',
                  id: 13605,
                  title: 'CHEP - Glance Search Library (1).pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11542,
            description:
              'The LHCb experiment is one of the 4 LHC experiments at CERN. With more than 1500 members and tens of thousands of assets, the Collaboration requires systems that allow the extraction of data from many databases according to some very specific criteria. In LHCb there are 4 production web applications responsible for managing members and institutes, tracking assets and their current status, presenting radiological information of the cavern and supporting the management of cables. A common requirement shared across all these systems is to allow searching information based on logic sentences. Therefore, in order to avoid rework, the Glance Search Library was created with the goal to provide components for applications to deploy frontend search interfaces capable of generating standardized queries based on users\u0027 input, and backend utility functions that compile such queries into a SQL clause. The Glance Search Library is split into 2 smaller libraries maintained in different GitLab repositories. The first one only contains Vue components and JavaScript modules and, in LHCb, it is included as a dependency of the SPAs. The second is a PHP Object-Oriented library, mainly used by REST APIs that are required to expose large amounts of data stored in their relational databases. This separation provides greater flexibility and more agile deployments. It also enables lighter applications with no graphical interface to build command line tools solely on top of the backend classes and predefined queries, for example.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 594,
            id: 'c16892',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11542/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [3, 'Ferreira Brito Filho, Carlos Henrique'],
                emailHash: 'af03b708856145feaf026124bb1c590c',
                familyName: 'Ferreira Brito Filho',
                firstName: 'Carlos Henrique',
                name: 'Carlos Henrique Ferreira Brito Filho',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12156,
            sessionSlotId: 2719,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Glance Search Interface',
            uniqueId: 'c16892',
            url: '/event/459/contributions/11542/',
          },
          c16893: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11530/attachments/9289/13478/CHEP%20-%20Glance%20Architecture.pdf',
                  id: 13478,
                  title: 'CHEP - Glance Architecture.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11530,
            description:
              'The Glance project is responsible for over 20 systems across three CERN experiments: ALICE, ATLAS and LHCb. Students, engineers, physicists and technicians have been using systems designed and managed by Glance on a daily basis for over 20 years. In order to produce quality products continuously, considering internal stakeholder\u0027s ever-evolving requests, there is the need of standardization. The adoption of such a standard had to take into account not only future developments but also legacy systems of the three experiments. These systems were built as a monolith, which, as they scaled, became difficult to maintain due to its lack of documentation and use of technologies that were becoming obsolete. Migrating them to a new architecture would mean speeding up the development process, avoiding rework and integrating CERN systems widely. Since a lot of the core functionalities of the systems are shared between them, both on the frontend and on the backend, the architecture had to assure modularity and reusability. In this architecture, the principles behind Hexagonal Architecture are followed and the systems\u2019 codebase is split into two applications: a JavaScript client and a REST backend server. The open-source framework Vue.js was chosen for the frontend. Its versatility, approachability and extended documentation made it the ideal tool for creating components that are reused throughout Glance applications. The backend uses PHP libraries created by the team to expose information through REST APIs both internally, allowing easier integration between the systems, and externally, introducing to users outside Glance information managed by the team.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 598,
            id: 'c16893',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11530/contribution.pdf',
            presenters: [
              {
                affiliation: 'UFRJ',
                displayOrderKey: [1, 'Ferreira Brito Filho, Carlos Henrique'],
                emailHash: 'af03b708856145feaf026124bb1c590c',
                familyName: 'Ferreira Brito Filho',
                firstName: 'Carlos Henrique',
                name: 'Carlos Henrique Ferreira Brito Filho',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12156,
            sessionSlotId: 2719,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'The migration to a standardized architecture for developing systems on the Glance project',
            uniqueId: 'c16893',
            url: '/event/459/contributions/11530/',
          },
          c16894: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11527/attachments/9555/13864/WebSecurityModels-CHEP2023_f.pdf',
                  id: 13864,
                  title: 'WebSecurityModels-CHEP2023_f.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11527,
            description:
              'The recent major upgrade of the ALICE Experiment at CERN\u2019s Large Hadron Collider has been coupled with the development of a new Online-Offline computing system capable of interacting with a sustained input throughput of 3.5TB/s. To facilitate the control of the experiment, new web applications have been developed and deployed to be used 24 hours a day, 365 days a year in the control room and remotely by the subsystem experts and on-call support staff.\r\nOver the past years, an exponential increase in number of exploits on applications vulnerabilities has been observed. This includes but it is not limited to malicious user input, DDoS, SQL Injection and Cross-Site Scripting attacks. Thus, the ALICE interfaces are being built using modern web technologies and a common library developed in-house which provides the core functionalities and building blocks for preventing vulnerabilities. This approach ensures a consolidated and secure environment towards maintaining data integrity and a straightforward non-malicious control of the experiment. This work showcases the tools and practices applied to enhance the application-level security and privacy needed for the experiment to be controlled and observed remotely. A report is also presented of incidents encountered during the first year of ALICE Run 3 operation.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 383,
            id: 'c16894',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11527/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Raduta, George'],
                emailHash: '82c821ccaba048a70130700f876eed57',
                familyName: 'Raduta',
                firstName: 'George',
                name: 'George Raduta',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12156,
            sessionSlotId: 2719,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Security Models for ALICE Online Web-Based Applications',
            uniqueId: 'c16894',
            url: '/event/459/contributions/11527/',
          },
          c16895: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11522/attachments/9382/13878/Building%20a%20highly%20modular%20user-oriented%20notification%20system%20-%20CHEP%202023.pdf',
                  id: 13878,
                  title:
                    'Building a highly modular user-oriented notification system - CHEP 2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11522,
            description:
              'CERN, as many large organizations, relies on multiple communication means for different use-cases and teams.\r\nEmail and mailing lists are the most popular ones, but more modern communications systems gain traction such as Mattermost and Push notifications.\r\nOn one end of the spectrum we have communication teams writing individual emails to users on a daily basis, which may be small targets, or in the order of thousands. On the other end, there are many automated tools and scripts which generate thousands of notifications daily, mostly in the form of emails.\r\n\r\nAs a consequence of the large amount of notifications received every day, for the users receiving them, it is challenging to control and keep track of where, how and when some information was received.\r\nAt the same time for those sending and maintaining the tools that deliver notifications, it is difficult to choose which targets to adopt (email, Mattermost, etc). Additionally it is difficult to please all users and take into account their preferences. Ultimately, across all those responsible for sending information, a lot of effort is spent on maintaining similar scripts and tools.\r\n\r\nThe CERN Notifications system aims at consolidating communication by providing a central place where notifications are created, maintained and distributed.\r\nIt allows to save efforts and costs by avoiding multiple parallel implementations of communication systems and their maintenance and details such as retry and failure mechanism, version updates, etc.\r\nCERN Notifications allows not only optimising the flow for the multiple people and teams which are responsible for sending, but also empowers the target users by respecting their preferences: how, where and when they receive their notifications.\r\nThe system was designed to allow those who send information to focus on the the content and relevance of the communication without knowing the technical details of the many frameworks available to distribute information.\r\n\r\nThis paper describes the design and architecture of the CERN Notifications system and its components, how it was designed with a flexible and highly modular architecture which allows adding further device targets with little effort. Furthermore, it presents implementation details and the decisions behind those. And last but not least it describes the features that empower users to choose how to consume information send to them.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 406,
            id: 'c16895',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11522/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'Antunes, Carina'],
                emailHash: 'c0b33760a00ce5eaf89458c8fb830d6e',
                familyName: 'Antunes',
                firstName: 'Carina',
                name: 'Carina Antunes',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12156,
            sessionSlotId: 2719,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Building a highly modular user-oriented notification system',
            uniqueId: 'c16895',
            url: '/event/459/contributions/11522/',
          },
        },
        entryType: 'Session',
        friendlyId: 9,
        id: 's12156',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2039/session-timetable.pdf',
        room: 'Chesapeake Meeting Room',
        sessionCode: '',
        sessionId: 2039,
        sessionSlotId: 2719,
        slotTitle: 'Glance and Web based  applications',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 5 - Sustainable and Collaborative Software Engineering',
        uniqueId: 's12156',
        url: '/event/459/sessions/2039/',
      },
      s12157: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#feffbf',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CNU',
            displayOrderKey: [0, 'Heddle, Dave'],
            emailHash: 'a9a8498d1e59570298c4ee6af05530e7',
            familyName: 'Heddle',
            firstName: 'Dave',
            name: 'Dave Heddle',
          },
          {
            affiliation: 'University of Manchester',
            displayOrderKey: [0, 'Skidmore, Nicole'],
            emailHash: 'a9b5fe5778eaecee042fcc8cab09ec26',
            familyName: 'Skidmore',
            firstName: 'Nicole',
            name: 'Nicole Skidmore',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16935: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11590/attachments/9311/14061/Kinematic%20Kalman%20Fit.key',
                  id: 14061,
                  title: 'Kinematic Kalman Fit.key',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11590/attachments/9311/14062/Kinematic%20Kalman%20Fit.pdf',
                  id: 14062,
                  title: 'Kinematic Kalman Fit.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11590,
            description:
              'The primary physics goal of the Mu2e experiment requires reconstructing an isolated 105 MeV electron with better than 500 KeV/c momentum resolution.  Mu2e uses a low-mass straw tube tracker, and a CsI crystal calorimeter, to reconstruct tracks.\r\nIn this paper, we present the design and performance of a track reconstruction algorithm optimized for Mu2e\u2019s unusual requirements.  The algorithm is based on the KinKal kinematic Kalman filter track fit package.  KinKal supports multiple track parameterizations, including one optimized for looping tracks, such as Mu2e signal tracks, and others optimized for straight or slightly-curved tracks, such as the high-momentum (P\u003e1 GeV/c) cosmic ray muons used to calibrate and align the Mu2e detectors.  All KinKal track parameterizations include the track origin time, to correctly model correlations arising from measurements that couple time and space, such as the straw drift time or the calorimeter cluster time.  KinKal employs magnetic field inhomogeneity and material effect correction algorithms with 10-4 fractional precision.  The Mu2e fit uses Artificial Neural Net functions to discriminate background hits from signal hits, and to resolve the straw tube hit left-right ambiguity, while iterating the extended Kalman filter.  The efficiency, accuracy, and precision of the Mu2e track reconstruction, as tested on detailed simulations of Mu2e data, will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 35,
            id: 'c16935',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11590/contribution.pdf',
            presenters: [
              {
                affiliation: 'LBL',
                displayOrderKey: [1, 'Brown, David'],
                emailHash: '01d8b4651ebc30fa461a08ef3a9a6d53',
                familyName: 'Brown',
                firstName: 'David',
                name: 'David Brown',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12157,
            sessionSlotId: 2720,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'A Kinematic Kalman Filter Track Reconstruction Algorithm for the Mu2e Experiment',
            uniqueId: 'c16935',
            url: '/event/459/contributions/11590/',
          },
          c16936: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11584/attachments/9271/13444/Medusa.pdf',
                  id: 13444,
                  title: 'Medusa.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11584/attachments/9271/13445/Medusa.pptx',
                  id: 13445,
                  title: 'Medusa.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11584,
            description:
              'Among the biggest computational challenges for High Energy Physics (HEP) experiments there are the increasingly larger datasets that are being collected, which often require correspondingly complex data analyses. In particular, the PDFs used for modeling the experimental data can have hundreds of free parameters. The optimization of such models involves a significant computational effort and a considerable amount of time, of the order of days, before reaching a result.\r\n\r\nMedusa is a C++ application designed to perform physics data analyses of generic 4-body decays deploying massively parallel platforms (multicore CPUs and GPUs) on Linux systems. It relies on Hydra, a header-only library that provides a high-level and user-friendly interface for common algorithms used in HEP, abstracting away the complexities associated with the implementation of code for different massively parallel architectures.\r\n\r\nMedusa has been tested through the measurement of the CP-violating phase phi_s in b-hadron decays exploiting the data collected by the LHCb experiment. By deploying such technologies as CUDA, TBB and OpenMP, Medusa accelerates the optimization of the full model, running over 500000 events, by factors 74 (multicore CPU) and 370 (GPU) in comparison with a non-parallelized program.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 37,
            id: 'c16936',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11584/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Pisa and INFN Pisa',
                displayOrderKey: [1, 'Ricci, Alessandro Maria'],
                emailHash: '73ab0899098f8fc4c6c00ebcf95aa2e0',
                familyName: 'Ricci',
                firstName: 'Alessandro Maria',
                name: 'Dr Alessandro Maria Ricci',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12157,
            sessionSlotId: 2720,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'MEDUSA, A MULTITHREAD 4-BODY DECAY FITTING AND SIMULATION SOFTWARE',
            uniqueId: 'c16936',
            url: '/event/459/contributions/11584/',
          },
          c16939: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11556/attachments/9385/13611/dilks__ePIC-Analysis.pdf',
                  id: 13611,
                  title: 'dilks__ePIC-Analysis.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11556,
            description:
              'Performing a physics analysis of data from simulations of a high energy experiment requires the application of several common procedures, from obtaining and reading the data to producing detailed plots for interpretation. Implementing common procedures in a general analysis framework allows the analyzer to focus on the unique parts of their analysis.  Over the past few years, EIC simulations have been performed using differing frameworks and data models; we thus developed `epic-analysis`, a common analysis framework to support all of them, allowing for comparison studies and cross checks while the design of the EIC continues to evolve.  The reconstruction of kinematic variables is fundamental to several physics channels, including inclusive, semi-inclusive, and jet physics.  `epic-analysis` includes many different kinematics reconstruction methods, ranging from using the scattered electron to machine learning methods, each of which produce the same set of kinematic variables needed for physics analysis.  Since the number of variables is large, a multi-dimensionally binned analysis is also often employed.  We thus developed `adage`, a novel graph-based data structure that not only associates data to their bins, but also stores and can execute user-specified algorithms on any lower dimensional subsets. This approach allows the analyzer to write analysis algorithms that are fully independent of the binning strategy, expediting the exploration of the high dimensional phase space.  Finally, as part of the EPIC software stack, `epic-analysis` continuous integration tests can be triggered by upstream changes in the simulation or reconstruction. For example, this automation allows for the physics impact on a detector design change to be quickly assessed, completing the full feedback loop for EIC detector design.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 608,
            id: 'c16939',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11556/contribution.pdf',
            presenters: [
              {
                affiliation: 'Duke University',
                displayOrderKey: [1, 'Dilks, Christopher'],
                emailHash: '2dd8789667a78528cb2610a9dd9a7963',
                familyName: 'Dilks',
                firstName: 'Christopher',
                name: 'Christopher Dilks',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12157,
            sessionSlotId: 2720,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: '`epic-analysis`: Common Physics Analysis Software for the EIC',
            uniqueId: 'c16939',
            url: '/event/459/contributions/11556/',
          },
          c17255: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11603/attachments/9639/14038/230501_chep_conference_slides.pdf',
                  id: 14038,
                  title: '230501_chep_conference_slides.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11603,
            description:
              'Apache Spark is a distributed computing framework which can process very large datasets using large clusters of servers. Laurelin is a Java-based implementation of ROOT I/O which allows Spark to read and write ROOT files from common HEP storage systems without a dependency on the C++ implementation of ROOT. We discuss improvements due to the migration to an Arrow-based in-memory representation as well as detail the performance difference for analyses over data stored in either ROOT or the Parquet format.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 360,
            id: 'c17255',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11603/contribution.pdf',
            presenters: [
              {
                affiliation: 'Vanderbilt University',
                displayOrderKey: [1, 'Melo, Andrew'],
                emailHash: 'e7312cc786478f54f0fe1f90ab19c891',
                familyName: 'Melo',
                firstName: 'Andrew',
                name: 'Andrew Melo',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12157,
            sessionSlotId: 2720,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Laurelin: A ROOT I/O implementation for Apache Spark',
            uniqueId: 'c17255',
            url: '/event/459/contributions/11603/',
          },
          c17298: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11602/attachments/9594/13932/CHEP%202023%20MC%20Tuning%20with%20MC%20uncertainties.pdf',
                  id: 13932,
                  title: 'CHEP 2023 MC Tuning with MC uncertainties.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11602,
            description:
              'To accurately describe data, tuning the parameters of MC event Generators is essential.  At first, experts performed tunings manually based on their sense of physics and goodness of fit. The software, Professor, made tuning more objective by employing polynomial surrogate functions to model the relationship between generator parameters and experimental observables (inner-loop optimization), then optimizing an objective function to obtain generator parameters (outer-loop optimization). Finally, Apprentice, a purely python-based tool, was developed to leverage High-Performance Computing and introduced rational approximation as an alternative surrogate function. However, none of these tuning methods includes MC systematic uncertainties. More importantly, the estimated uncertainties of tuned parameters are unreliable because the objective distribution does not match a chi-squared distribution, and one has to manually set a cutoff threshold on the objective function using educated guesses. In this work, we integrate the MC systematic uncertainties into the inner-loop optimization and outer-loop optimization. With our new method, we find that the objective function nicely follows the chi-square distribution; thus, the uncertainty of the tuned generator parameters is better quantified.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 99,
            id: 'c17298',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11602/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lawrence Berkeley National Laboratory',
                displayOrderKey: [0, 'Ju, Xiangyang'],
                emailHash: 'cdae8bb08ef4238c3dbc49a841ef65f1',
                familyName: 'Ju',
                firstName: 'Xiangyang',
                name: 'Xiangyang Ju',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12157,
            sessionSlotId: 2720,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Event Generator Tuning Incorporating MC Systematic Uncertainty',
            uniqueId: 'c17298',
            url: '/event/459/contributions/11602/',
          },
        },
        entryType: 'Session',
        friendlyId: 10,
        id: 's12157',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2040/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VIII',
        sessionCode: '',
        sessionId: 2040,
        sessionSlotId: 2720,
        slotTitle: 'Reconstruction and Amplitude Fitting',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#1f1f02',
        title: 'Track 6 - Physics Analysis Tools',
        uniqueId: 's12157',
        url: '/event/459/sessions/2040/',
      },
      s12158: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#1a3f14',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Nebraska-Lincoln',
            displayOrderKey: [0, 'Weitzel, Derek'],
            emailHash: 'ca0d239ab6ae0ec500eff8eb24779855',
            familyName: 'Weitzel',
            firstName: 'Derek',
            name: 'Derek Weitzel',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Wiebalck, Arne'],
            emailHash: 'a7f74ac17e363dd0ab1ffe2342ba5c5f',
            familyName: 'Wiebalck',
            firstName: 'Arne',
            name: 'Arne Wiebalck',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17100: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11625/attachments/9257/13426/Sobie-CHEP-Bmk.pdf',
                  id: 13426,
                  title: 'Sobie-CHEP-Bmk.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11625,
            description:
              'HEPscore is a CPU benchmark, based on HEP applications, that the HEPiX Working Group is proposing as a replacement of the HEPSpec06 benchmark (HS06), which is currently used by the WLCG for procurement, computing resource requests and pledges, accounting and performance studies. At the CHEP 2019 conference, we presented the reasons for building a benchmark for the HEP community that is based on HEP applications instead of standard industrial benchmarks. In this contribution we describe the mix of HEP workloads selected to build HEPscore. We present the results of the 2022 campaign of measurements that studied the performance of eleven HEP applications on more than 70 unique computer systems on multiple WLCG sites. We provide an update on the current status of a HEPScore candidate and its deployment plans for 2023. We also discuss how HEPscore can be used to assess the power efficiency of different CPU architectures.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 120,
            id: 'c17100',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11625/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Victoria',
                displayOrderKey: [2, 'Sobie, Randall'],
                emailHash: 'e106556bcded279b116df4aee2de6c31',
                familyName: 'Sobie',
                firstName: 'Randall',
                name: 'Dr Randall Sobie',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12158,
            sessionSlotId: 2721,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'HEPscore: a new benchmark for WLCG compute resources',
            uniqueId: 'c17100',
            url: '/event/459/contributions/11625/',
          },
          c17101: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11635/attachments/9527/13817/CRIU_Andrijauskas.pdf',
                  id: 13817,
                  title: 'CRIU_Andrijauskas.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11635,
            description:
              'Creating new materials, discovering new drugs, and simulating systems are essential processes for research and innovation, and require substantial computational power. While many applications can be split into many smaller independent tasks, some cannot and may take hours or weeks to run to completion. To better manage those longer-running jobs, it would be desirable to stop them at any arbitrary point in time and later continue their computation on another compute resource; this is usually referred to as checkpointing. While some applications can manage checkpointing in a programmatic way, it would be preferable if the batch scheduling system could do that independently. In this paper, we evaluate the feasibility of using CRIU (Checkpoint Restore in Userspace), an open-source tool available for the GNU/Linux environment, with an emphasis on the OSG\u2019s OSPool HTCondor setup. CRIU allows for checkpointing of the process state into a disk image, and is able to seamlessly deal with both open files and established network connections. Furthermore, it can be used for checkpointing of both traditional Linux processes and containerized workloads. The functionality seems adequate for many scenarios supported in the OSPool. although there are some limitations that prevent it from being usable in all circumstances.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 49,
            id: 'c17101',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11635/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Andrijauskas, Fabio'],
                emailHash: '0078d106b8d33e1c6ca918958fb59267',
                familyName: 'Andrijauskas',
                firstName: 'Fabio',
                name: 'Fabio Andrijauskas',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12158,
            sessionSlotId: 2721,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Using CRIU for checkpointing batch jobs',
            uniqueId: 'c17101',
            url: '/event/459/contributions/11635/',
          },
          c17102: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11646/attachments/9456/13958/Preparing%20for%20a%20new%20Data%20Center_%20Automated%20Management%20of%20a%2010000%20node%20bare-metal%20fleet%20-%20CHEP%2009MAY2023.pdf',
                  id: 13958,
                  title:
                    'Preparing for a new Data Center_ Automated Management of a 10000 node bare-metal fleet - CHEP 09MAY2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11646,
            description:
              'CERN IT has consolidated all life-cycle management of its physical server fleet on the Ironic bare-metal API. From the initial registration upon the first boot, over the inventory checking, the burn-in and the benchmarking for acceptance, the provisioning to the end users and the repairs during its service, up to the retirement at the end of the servers\u2019 life, all stages can be managed within this framework. In this presentation we will follow a server throughout its life in the CERN data center, and explain how this enables us to handle a fleet of 10\u2019000 nodes in an automated and efficient way and to prepare for the new data centre which is currently being built. We will add the top challenges we faced when moving to this system, like the transparent adoption of already in-production nodes or after-the-fact inventory updates, and eventually round things up with our \u201cGRUBsetta stone\u201d, a collection of boot errors and what they really mean.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 323,
            id: 'c17102',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11646/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Wiebalck, Arne'],
                emailHash: 'a7f74ac17e363dd0ab1ffe2342ba5c5f',
                familyName: 'Wiebalck',
                firstName: 'Arne',
                name: 'Dr Arne Wiebalck',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12158,
            sessionSlotId: 2721,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Preparing for a new Data Center: Automated Management of a 10\u2019000 node bare-metal fleet in CERN IT',
            uniqueId: 'c17102',
            url: '/event/459/contributions/11646/',
          },
          c17103: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11626/attachments/9277/13950/20230508%20CHEP%202023%20UVic%20T2%20on%20k8s.pdf',
                  id: 13950,
                  title: '20230508 CHEP 2023 UVic T2 on k8s.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11626,
            description:
              'The University of Victoria (UVic) operates an Infrastructure-as-a-Service science cloud for Canadian researchers, and a WLCG T2 grid site for the ATLAS experiment at CERN. At first, these were two distinctly separate systems, but over time we have taken steps to migrate the T2 grid services to the cloud. This process has been significantly facilitated by basing our approach on Kubernetes, a versatile, robust, and very widely-adopted automation platform for orchestrating and managing containerized applications. Previous work exploited the batch capabilities of Kubernetes to run the computing jobs of the UVic ATLAS T2, and replace the conventional grid Computing Elements, by interfacing with the Harvester workload management system of the ATLAS experiment. However, the required functionality of a T2 site encompasses more than just batch computing. Likewise, the capabilities of Kubernetes extend far beyond running batch jobs, and include for example scheduling recurring tasks and hosting long-running externally-accessible services in a resilient way. We are now undertaking the more complex and challenging endeavour of adapting and migrating all remaining functions of the T2 site - such as APEL accounting and Squid caching proxies, but in particular the grid Storage Element - to cloud-native deployments on Kubernetes. We aim to enable fully comprehensive deployment of a complete ATLAS T2 site on a Kubernetes cluster via Helm charts, which will benefit the community by providing a streamlined and replicable way to install and configure an ATLAS site. We also describe our experience running a high-performance self-managed Kubernetes ATLAS T2 cluster at the scale of 8,000 CPU cores for the last 2 years, and compare with the conventional setup of grid services.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 340,
            id: 'c17103',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11626/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Victoria',
                displayOrderKey: [1, 'Taylor, Ryan Paul'],
                emailHash: '32e62a0d1cb79b90404b2e9b5eae5126',
                familyName: 'Taylor',
                firstName: 'Ryan Paul',
                name: 'Ryan Paul Taylor',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12158,
            sessionSlotId: 2721,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'A grid site reimagined: building a fully cloud-native ATLAS T2 on Kubernetes',
            uniqueId: 'c17103',
            url: '/event/459/contributions/11626/',
          },
          c17104: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11636/attachments/9246/13414/CHEP%202023%20-%20ATLAS%20Cloud%20(5).pdf',
                  id: 13414,
                  title: 'CHEP 2023 - ATLAS Cloud.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11636,
            description:
              'The ATLAS experiment at CERN is one of the largest scientific machines built to date and will have ever growing computing needs as the Large Hadron Collider collects an increasingly larger volume of data over the next 20 years. ATLAS is conducting R\u0026D projects on Amazon and Google clouds as complementary resources for distributed computing, focusing on some of the key features of commercial clouds: lightweight operation, elasticity and availability of multiple chip architectures.\r\n\r\nThe proof of concept phases have concluded with the cloud-native, vendor-agnostic integration with the experiment\u2019s data and workload management frameworks. Google has been used to evaluate elastic batch computing, ramping up ephemeral clusters of up to O(100k) cores to process tasks requiring quick turnaround. Amazon cloud has been exploited for the successful physics validation of the Athena simulation software on ARM processors.\r\nWe have also set up an interactive facility for physics analysis allowing end-users to spin up private, on-demand clusters for parallel computing with up to 4000 cores, or run GPU enabled notebooks and jobs for machine learning applications.\r\n\r\nThe success of the proof of concept phases has led to the extension of the Google cloud project, where ATLAS will study the total cost of ownership of a production cloud site during 15 months with 10k cores on average, fully integrated with distributed grid computing resources and continue the R\u0026D projects.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 372,
            id: 'c17104',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11636/contribution.pdf',
            presenters: [
              {
                affiliation: 'The University of Texas at Arlington',
                displayOrderKey: [0, 'Megino, Fernando Harald Barreiro'],
                emailHash: 'f04de6eee7b8dcc20dedefcb9fe659da',
                familyName: 'Megino',
                firstName: 'Fernando Harald Barreiro',
                name: 'Fernando Harald Barreiro Megino',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12158,
            sessionSlotId: 2721,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Accelerating science: the usage of commercial clouds in ATLAS distributed computing',
            uniqueId: 'c17104',
            url: '/event/459/contributions/11636/',
          },
          c17105: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11617/attachments/9592/14077/CHEP-2023-Cloud-Comparison-v1.4.pdf',
                  id: 14077,
                  title: 'CHEP-2023-Cloud-Comparison-v1.4.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11617,
            description:
              'An all-inclusive analysis of costs for on-premises and public cloud-based solutions to handle the bulk of HEP computing requirements shows that dedicated on-premises deployments are still the most cost-effective. Since the advent of public cloud services,  the HEP community has engaged in multiple proofs of concept to study the technical viability of using cloud resources; however, the financial viability of using cloud resources for HEP computing and storage is of greater importance. We present the results of a study comparing the cost of providing computing resources ina public cloud and a comprehensive estimate for the cost of an on-premises solution for HEP computing. Like previous studies, the fundamental conclusion is that for the bulk of HEP computing needs, on premises is significantly more cost effective than public clouds.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 566,
            id: 'c17105',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11617/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [1, 'Misawa, Shigeki'],
                emailHash: '3df94402d8712e6772d5b92b49771631',
                familyName: 'Misawa',
                firstName: 'Shigeki',
                name: 'Shigeki Misawa',
              },
              {
                affiliation: 'Brookhaven Science Associates',
                displayOrderKey: [2, 'LAURET, Jerome'],
                emailHash: '60e98fb28af2edf9a8a6c39567ee11c7',
                familyName: 'LAURET',
                firstName: 'Jerome',
                name: 'Jerome LAURET',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12158,
            sessionSlotId: 2721,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Financial Case Study on the Use of Cloud Resources in HEP Computing',
            uniqueId: 'c17105',
            url: '/event/459/contributions/11617/',
          },
        },
        entryType: 'Session',
        friendlyId: 11,
        id: 's12158',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2041/session-timetable.pdf',
        room: 'Marriott Ballroom IV',
        sessionCode: '',
        sessionId: 2041,
        sessionSlotId: 2721,
        slotTitle: 'Computing Centre Infrastructure and Cloud',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#f1ffef',
        title: 'Track 7 - Facilities and Virtualization',
        uniqueId: 's12158',
        url: '/event/459/sessions/2041/',
      },
      s12159: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#e3f2d3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Paul Scherrer Institut',
            displayOrderKey: [0, 'Lange, Clemens'],
            emailHash: '429cdad04fa5f47ba2f6993b0b32ec69',
            familyName: 'Lange',
            firstName: 'Clemens',
            name: 'Clemens Lange',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Gazzarrini, Elena'],
            emailHash: '39921bb8f8e49b6f44e880111530f493',
            familyName: 'Gazzarrini',
            firstName: 'Elena',
            name: 'Elena Gazzarrini',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17140: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11677/attachments/9566/14068/ub-opensamples-CHEP_wide.pdf',
                  id: 14068,
                  title: 'ub-opensamples-CHEP_wide.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11677,
            description:
              'Among liquid argon time projection chamber (LArTPC) experiments MicroBooNE is the one that continually took physics data for the longest time (2015-2021), and represents the state of the art for reconstruction and analysis with this detector. Recently published analyses include oscillation physics results, searches for anomalies and other BSM signatures, and cross section measurements. LArTPC detectors are being used in current experiments such as ICARUS and SBND, and being planned for future experiments such as DUNE.\r\n\r\nMicroBooNE has recently released to the public two of its data sets, with the goal of enabling collaborative software developments with other LArTPC experiments and with AI or computing experts. These datasets simulate neutrino interactions on top of off-beam data, which include cosmic ray background and noise. The datasets are released in two formats: the native artroot format used internally by the collaboration and familiar to other LArTPC experts, and the HDF5 format which contains reduced and simplified content and is suitable for usage by the broader community.\r\n\r\nThis contribution presents the open data sets, discusses their motivation, the technical implementation, and the extensive documentation - all inspired by FAIR principles. Finally, opportunities for collaborations are discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 128,
            id: 'c17140',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11677/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Cerati, Giuseppe'],
                emailHash: 'ab4fb4c89222ae5bbd0f47b9ed01e601',
                familyName: 'Cerati',
                firstName: 'Giuseppe',
                name: 'Giuseppe Cerati',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12159,
            sessionSlotId: 2722,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title:
              'MicroBooNE Public Data Sets: a Collaborative Tool for LArTPC Software Development',
            uniqueId: 'c17140',
            url: '/event/459/contributions/11677/',
          },
          c17141: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11675/attachments/9477/13738/ATLAS%20Open%20Data%20-%20Developing%20Education%20and%20Outreach%20Resources%20From%20Research%20Data%202023.05.09..pdf',
                  id: 13738,
                  title:
                    'ATLAS Open Data - Developing Education and Outreach Resources From Research Data 2023.05.09..pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11675,
            description:
              'The ATLAS Open Data project aims to deliver open-access resources for education and outreach in High Energy Physics using real data recorded by the ATLAS detector. The Open Data release so far has resulted in the release of a substantial amount of data from 8 TeV and 13 TeV collisions in an easily-accessible format and supported by dedicated software and documentation to allow its fruitful use by users at a range of experience levels. To maximise the value of  the data, software, and documentation resources provided ATLAS has developed initiatives and promotes stakeholder engagement in the creation of these materials through on-site and remote training schemes such as high-school work experience and summer schools programs, university projects and PhDs qualification tasks. We present examples of how multiple training programs inside and outside CERN have helped and continue to help development the ATLAS Open Data project, lessons learnt, impacts, and future goals.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 183,
            id: 'c17141',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11675/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Krasznahorkay, Attila'],
                emailHash: '8d4c0721de129e328dffeb903264d729',
                familyName: 'Krasznahorkay',
                firstName: 'Attila',
                name: 'Attila Krasznahorkay',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12159,
            sessionSlotId: 2722,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'ATLAS Open Data: developing education and outreach resources from research data',
            uniqueId: 'c17141',
            url: '/event/459/contributions/11675/',
          },
          c17143: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11701/attachments/9467/13983/chep2023-opendata-cms-slides.pdf',
                  id: 13983,
                  title: 'chep2023-opendata-cms-slides.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11701,
            description:
              'In this paper we discuss the CMS open data publishing workflows, summarising experience with eight releases of CMS open data on the CERN Open Data portal since its initial launch in 2014. We present the recent enhancements of data curation procedures, including (i) mining information about collision and simulated datasets with accompanying generation parameters and processing configuration files, (ii) building an API service covering information related to luminosity, run number ranges and other contextual dataset information, as well as (iii) configuring the CERN Open Data storage area as a Rucio endpoint that manages over four petabytes of released CMS open data and serves as a WLCG Tier 3 site to simplify data transfers. Finally, we discuss the latest CMS content released as open data (completed Run 1 data, first samples from Run 2 data) and the associated runnable analysis examples demonstrating its use in containerised data analysis workflows. We conclude by a short list of lessons learnt as well as general recommendations to facilitate upcoming releases of Run 2 data.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 380,
            id: 'c17143',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11701/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Simko, Tibor'],
                emailHash: '4e67297166a80955b72e69e066491263',
                familyName: 'Simko',
                firstName: 'Tibor',
                name: 'Dr Tibor Simko',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12159,
            sessionSlotId: 2722,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title:
              'Automatising open data publishing workflows: experience with CMS open data curation',
            uniqueId: 'c17143',
            url: '/event/459/contributions/11701/',
          },
          c17144: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11696/attachments/9197/13356/FitzgeraldCHEP2023_final.pdf',
                  id: 13356,
                  title: 'FitzgeraldCHEP2023_final.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11696,
            description:
              'Making the large datasets collected at the LHC accessible to the public is a considerable challenge given the complexity and volume of data. Yet to harness the full scientific potential of the facility, it is essential to enable meaningful access to the data by the broadest physics community possible. Here we present an application, the LHCb Ntuple Wizard, which leverages the existing computing infrastructure available to the LHCb collaboration in order to enable third-party users to request derived data samples in the same format used in LHCb physics analysis. An intuitive user interface built with the React-JS framework allows for the discovery of available particle or decay channel datasets through a flexible search engine, and guides the user through the request for producing Ntuples: collections of N particle or decay candidates, each candidate corresponding to a tuple cataloguing measured quantities chosen by the user. Necessary documentation and metadata is rendered in the appropriate context within the application to guide the user through the core components of the application, dataset discovery and Ntuple configuration. In the Ntuple configuration step, decays are represented by an interactive directed acyclic graph where the nodes depict (intermediate) particles and the edges indicate a mother-daughter relationship, each graph corresponding to the configuration of a single Ntuple. Standard tools used at LHCb for saving measured or derived quantities to Ntuples can be applied to specific nodes, or collections of nodes, allowing for customization of information saved about the various subsamples used to build the physics candidate (e.g. various particles in a decay). Ntuples in this context are saved as simply structured ROOT files containing the catalogued quantities, requiring no external usage of the LHCb software stack. Issues of computer security and access control arising from offering this service are addressed by keeping the configuration output of the Ntuple Wizard in a pure data structure format (YAML) to be interpreted by internal parsers. The parsers produce the necessary Python scripts for steering the Ntuple production job, the output of which will be delivered to the CERN Open Data Portal.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 560,
            id: 'c17144',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11696/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Michigan',
                displayOrderKey: [1, 'Fitzgerald, Dillon'],
                emailHash: '103de9c07a56c07f21ddb60d45b3400a',
                familyName: 'Fitzgerald',
                firstName: 'Dillon',
                name: 'Dillon Fitzgerald',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12159,
            sessionSlotId: 2722,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'An Ntuple production service for accessing LHCb Open Data: the Ntuple Wizard',
            uniqueId: 'c17144',
            url: '/event/459/contributions/11696/',
          },
          c17145: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11694/attachments/9577/13898/2023-05-09_FAIR4UFO-CHEP23.pdf',
                  id: 13898,
                  title: '2023-05-09_FAIR4UFO-CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11694,
            description:
              'Research in high energy physics (HEP) heavily relies on domain-specific digital contents. We reflect on the interpretation of principles of Findability, Accessibility, Interoperability, and Reusability (FAIR) in preservation and distribution of such digital objects. As a case study, we demonstrate the implementation of an end-to-end support infrastructure for preserving and accessing Universal FeynRules Output (UFO) models guided by the FAIR principles. UFO models are custom-made python libraries used by the HEP community for Monte Carlo simulation of collider physics events. Our framework provides simple but robust tools to preserve and access the UFO models and corresponding metadata in accordance with the FAIR principles.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 583,
            id: 'c17145',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11694/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Illinois at Urbana-Champaign',
                displayOrderKey: [0, 'Roy, Avik'],
                emailHash: '00152a535f6f4d00b5aaeef45ccf672b',
                familyName: 'Roy',
                firstName: 'Avik',
                name: 'Avik Roy',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12159,
            sessionSlotId: 2722,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title:
              'FAIR principles for Digital Objects in High Energy Physics: A Case Study with Universal FeynRules Output (UFO) Models',
            uniqueId: 'c17145',
            url: '/event/459/contributions/11694/',
          },
          c17316: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11710/attachments/9575/14065/BaBarPreservationCHEP2023.pdf',
                  id: 14065,
                  title: 'BaBarPreservationCHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11710,
            description:
              'The BaBar experiment collected electron-positron collisions at the SLAC National Accelerator Laboratory from 1999-2008. Although data taking has stopped 15 years ago, the collaboration is still actively doing data analyses, publishing results, and giving presentations at international conferences. Special considerations were needed to do analyses using a computing environment that was developed more than a decade ago. A framework is required that preserves the data, data access, and the capability of doing analyses using a well defined and preserved  environment. Also, BaBar\u2019s support by SLAC National Accelerator Laboratory, the place where the experiment took place, ended at the beginning of 2021. Fortunately, the HEP Research Computing group at the University of Victoria (UVic), Canada, offered to be the new home for the main BaBar computing infrastructure, GridKa offered to host all data for access by analyses running at UVic, and CERN and IN2P3 offered to store a backup of all data. In this talk, we will present what was done at BaBar to preserve the data and analysis capabilities and needed to move the whole computing infrastructure, including collaboration tools and data files, away from the SLAC National Accelerator Laboratory. It will be shown how BaBar  preserved the ability to continue to do data analyses and also have a working collaboration tools infrastructure. The talk will detail what was needed to move the different bits of an experiment\u2019s computing infrastructure to a new home, access the data from a different location, and how to move to more modern systems where older infrastructure could not be used anymore. The talk will be focused on BaBar\u2019s experience with such a big change in its infrastructure and what was learned from it, which may be useful to other experiments which are interested in long term analysis support and data preservation in general.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 464,
            id: 'c17316',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11710/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Victoria',
                displayOrderKey: [1, 'Ebert, Marcus'],
                emailHash: 'd2e0cef05ea1a024f0c2163a3aec8619',
                familyName: 'Ebert',
                firstName: 'Marcus',
                name: 'Marcus Ebert',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12159,
            sessionSlotId: 2722,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'BaBar\u2019s Experience with the Preservation of Data and Analysis Capabilities',
            uniqueId: 'c17316',
            url: '/event/459/contributions/11710/',
          },
        },
        entryType: 'Session',
        friendlyId: 12,
        id: 's12159',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2042/session-timetable.pdf',
        room: 'Marriott Ballroom I',
        sessionCode: '',
        sessionId: 2042,
        sessionSlotId: 2722,
        slotTitle: 'Open Data and Open Science',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#253f08',
        title: 'Track 8 - Collaboration, Reinterpretation, Outreach and Education',
        uniqueId: 's12159',
        url: '/event/459/sessions/2042/',
      },
      s12160: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#92b6db',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Wenzel, Sandro'],
            emailHash: '7e8bcc2855b51c4e93b19e095a8b2ec9',
            familyName: 'Wenzel',
            firstName: 'Sandro',
            name: 'Sandro Wenzel',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'VALLECORSA, SOFIA'],
            emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
            familyName: 'VALLECORSA',
            firstName: 'SOFIA',
            name: 'SOFIA VALLECORSA',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17176: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11743/attachments/9591/13967/Influencer%20Object%20Condensation%20(5).pptx',
                  id: 13967,
                  title: 'Influencer Object Condensation Murnane.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11743,
            description:
              'Significant progress has been made in applying graph neural networks (GNNs) and other geometric ML ideas to the track reconstruction problem. State-of-the-art results are obtained using approaches such as the Exatrkx pipeline, which currently applies separate edge construction, classification and segmentation stages. One can also treat the problem as an object condensation task, and cluster hits into tracks in a single stage, such as in the GravNet architecture. However, condensation with such an architecture may still require non-differentiable operations. In this work, we extend the ideas of geometric attention applied in the GravNetNorm architecture to the task of fully geometric (and therefore fully differentiable) end-to-end track reconstruction in one step.\r\n\r\nTo realize this goal, we introduce a novel condensation loss function called Influencer Loss, which allows an embedded representation of tracks to be learned in tandem with the most representative hit(s) in each track. This loss has global optima that formally match the task of track reconstruction, namely smooth condensation of tracks to a single point, and we demonstrate this empirically on the TrackML dataset. We combine the Influencer approach with geometric attention to build an Influencer pooling operation, that allows a GNN to learn a hierarchy of hits-to-tracks in a fully differentiable fashion. Finally, we show how these ideas naturally lead to a representation of collision point clouds that can be used for downstream predictive and generative tasks.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 600,
            id: 'c17176',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11743/contribution.pdf',
            presenters: [
              {
                affiliation: 'LBNL',
                displayOrderKey: [1, 'Calafiura, Paolo'],
                emailHash: 'abe1eab839bcd16b6fe1dc8165c71774',
                familyName: 'Calafiura',
                firstName: 'Paolo',
                name: 'Paolo Calafiura',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12160,
            sessionSlotId: 2723,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'End-to-End Geometric Representation Learning for Track Reconstruction',
            uniqueId: 'c17176',
            url: '/event/459/contributions/11743/',
          },
          c17177: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11741/attachments/9643/14086/230507_oct_chep_v3.pdf',
                  id: 14086,
                  title: '230507_oct_chep_v3.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11741/attachments/9643/14044/230507_oct_chep_v3.pptx',
                  id: 14044,
                  title: '230507_oct_chep_v3.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11741,
            description:
              'Recent work has demonstrated that graph neural networks (GNNs) trained for charged particle tracking can match the performance of traditional algorithms while improving scalability. Most approaches are based on the edge classification paradigm, wherein tracker hits are connected by edges, and a GNN is trained to prune edges, resulting in a collection of connected components representing tracks. These connected components are usually collected by a clustering algorithm and the resulting hit clusters are passed to downstream modules that may assess track quality or fit track parameters. \r\nIn this work, we consider an alternative approach based on object condensation (OC), a multi-objective learning framework designed to cluster points belonging to an arbitrary number of objects, in this context tracks, and regress the properties of each object. We demonstrate that object condensation shows promising results in various simplified scenarios and present a modular and extensible open-source implementation that allows us to efficiently train and evaluate the performance of various OC architectures and related approaches.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 621,
            id: 'c17177',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11741/contribution.pdf',
            presenters: [
              {
                affiliation: 'Princeton University',
                displayOrderKey: [1, 'Lieret, Kilian'],
                emailHash: 'cb53547a5eada9b961f65f5ce163858f',
                familyName: 'Lieret',
                firstName: 'Kilian',
                name: 'Kilian Lieret',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12160,
            sessionSlotId: 2723,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'An Object Condensation Pipeline for Charged Particle Tracking',
            uniqueId: 'c17177',
            url: '/event/459/contributions/11741/',
          },
          c17178: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11721/attachments/9408/14067/chep2023.pdf',
                  id: 14067,
                  title: 'chep2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11721,
            description:
              'Track reconstruction is one of the most important and challenging tasks in the offline data processing of collider experiments. For the BESIII detector working in the tau-charm energy region, plenty of efforts were made previously to improve the tracking performance with traditional methods, such as template matching and Hough transform etc. However, for difficult tracking tasks, such as the tracking of low momentum tracks, tracks from secondary vertices and tracks with high noise level, there is still large room for improvement.\r\nIn this contribution, we demonstrate a novel tracking algorithm based on machine learning method. In this method, a hit pattern map representing the connectivity between drift cells is established using an enormous MC sample, based on which we design an optimal method of graph construction, then an edge-classifying Graph Neural Network is trained to distinguish the hit-on-track from noise hits. Finally, a clustering method based on DBSCAN is developed to cluster hits from multiple tracks. Track fitting algorithm based on GENFIT is also studied to obtain the track parameters, where deterministic annealing filter are implemented to deal with ambiguities and potential noises. \r\nThe preliminary results on BESIII MC sample presents promising performance, showing potential to apply this method to other trackers based on drift chamber as well, such as the CEPC and STCF detectors under pre-study.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 440,
            id: 'c17178',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11721/contribution.pdf',
            presenters: [
              {
                affiliation: 'Shandong University, CN',
                displayOrderKey: [1, 'Jia, Xiaoqian'],
                emailHash: '1c6b5cedc13e9299d0a4d70bb0574d72',
                familyName: 'Jia',
                firstName: 'Xiaoqian',
                name: 'Xiaoqian Jia',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12160,
            sessionSlotId: 2723,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'BESIII track reconstruction algorithm based on machine learning',
            uniqueId: 'c17178',
            url: '/event/459/contributions/11721/',
          },
          c17179: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11745/attachments/9491/13757/CHEP-05-2023-Gavalian.pdf',
                  id: 13757,
                  title: 'CHEP-05-2023-Gavalian.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11745,
            description:
              'Particle track reconstruction is the most computationally intensive process in nuclear physics experiments.\r\nTraditional algorithms use a combinatorial approach that exhaustively tests track measurements (hits) to\r\nidentify those that form an actual particle trajectory. In this article we describe the development of machine \r\nlearning models that assist the tracking algorithm by identifying valid track candidates from the measurement\r\n("hits") in drift chambers. Several types of machine learning models were tested, including: Convolutional Neural Networks (CNN),\r\nMulti-Layer Perceptron (MLP), Extremely Randomized Trees (ERT) and Recurrent Neural Networks (RNN).\r\nAs a result of this work the CLAS12 tracking efficiency increased by ~15% for single particle tracking, and\r\n20%-40% gained efficiency in multi-particle final states. The tracking code also increased in speed by 35%.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 12,
            id: 'c17179',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11745/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [0, 'Gavalian, Gagik'],
                emailHash: '861a3d542f0e3d66f4142d1f96b0d24a',
                familyName: 'Gavalian',
                firstName: 'Gagik',
                name: 'Gagik Gavalian',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12160,
            sessionSlotId: 2723,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Track Identification for CLAS12 using Artificial Intelligence',
            uniqueId: 'c17179',
            url: '/event/459/contributions/11745/',
          },
          c17180: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11713/attachments/9572/13900/chep2023_Sylvain_Caillou.pdf',
                  id: 13900,
                  title: 'chep2023_Sylvain_Caillou.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11713,
            description:
              'Data from the LHC detectors are not easily represented using regular data structures. These detectors are comprised of several species of subdetectors and therefore produce heterogeneous data. LHC detectors are granular by design so that nearby particles may be distinguished. As a consequence, LHC data are sparse, in that many detector channels are not active during a given collision event. Graphs offer a flexible and efficient alternative to rectilinear data structures for representing LHC data. Accordingly, graph-based machine learning algorithms are becoming increasingly popular for a large number of LHC physics tasks [1]. This popularity, and the corresponding potential for substantial increase in physics output, are illustrated on the cover of a recent issue [2] of the CERN Courier magazine.\r\n\r\nThe graphs used in almost all practical applications at the LHC so far are homogeneous, i.e. each node is assigned the same features, and each edge is assigned the same features [3]. In other words, the power of graphs to represent sparse data has been exploited in applications for the LHC, but the potential of graphs to represent heterogeneous data has not. The pink graph on the cover of the CERN Courier [2] can be seen as an illustration of this limitation: all nodes are pink, regardless of their position in the detector. \r\n\r\nWe present novel fully-heterogeneous GNN designs and apply them to simulated data from a tracking detector that resembles the trackers that will be used at the HL-LHC. It contains a pixel subsystem that provides 3D hits and a strip subsystem that provides 2D hits. Our designs aim at solving the degraded performance that is observed in the strip detector in the first GNN-based tracking studies presented by the ATLAS Collaboration [4].\r\n\r\n\r\n[1] Shlomi, Battaglia and Vlimant, \u201cGraph neural networks in particle physics\u201d, Mach. Learn.: Sci. Technol. 2 021001 (2021), https://doi.org/10.1088/2632-2153/abbf9a\r\n\r\n[2] https://cerncourier.com/wp-content/uploads/2021/08/CERNCourier2021SepOct-digitaledition.pdf\r\n\r\n[3] Sometimes quasi-heterogeneous node representations are used: the same data structure is assigned to each node, but different parts of it are zeroed out in subsets of nodes.\r\n\r\n[4] http://cds.cern.ch/record/2815578',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 550,
            id: 'c17180',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11713/contribution.pdf',
            presenters: [
              {
                affiliation: 'L2IT, Toulouse',
                displayOrderKey: [0, 'Caillou, Sylvain'],
                emailHash: '05aac5d59c4826c4e67d0978d74e78bd',
                familyName: 'Caillou',
                firstName: 'Sylvain',
                name: 'Mr Sylvain Caillou',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12160,
            sessionSlotId: 2723,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Novel fully-heterogeneous GNN designs for track reconstruction at the HL-LHC',
            uniqueId: 'c17180',
            url: '/event/459/contributions/11713/',
          },
          c17181: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11714/attachments/9420/13920/Sokoloff_Garg_CHEP-2023.pdf',
                  id: 13920,
                  title: 'Sokoloff_Garg_CHEP-2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11714,
            description:
              'We have been studying the use of deep neural networks (DNNs) to identify and locate primary vertices (PVs) in proton-proton collisions at the LHC.  Earlier work focused on finding primary vertices in simulated LHCb data using a hybrid approach that started with kernel density estimators (KDEs) derived from the ensemble of charged track parameters heuristically and predicted \u201ctarget histogram\u201d proxies from which PV positions are extracted.  We have recently demonstrated that using a UNet architecture performs indistinguishably from a \u201cflat\u201d convolutional neural network model and that \u201cquantization\u201d, using FP16 rather than FP32 arithmetic, degrades its performance minimally.  We have demonstrated that the KDE-to-hists algorithm developed for LHCb data can be adapted to ATLAS and ACTS data. Within ATLAS/ACTS, the algorithm has been validated against the standard vertex finder algorithm. \r\n\r\nWe have developed an \u201cend-to-end\u201d tracks-to-hists DNN that predicts target histograms directly from track parameters using simulated LHCb data that provides better performance (a lower false positive rate for the same high efficiency) than the best KDE-to-hists model studied. This DNN also provides better efficiency than the default heuristic algorithm for the same low false positive rate.  We are currently instantiating the end-to-end tracks-to-hists DNN within the software stack for Allen, LHCb\u2019s GPU-resident, first-level software trigger.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 102,
            id: 'c17181',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11714/contribution.pdf',
            presenters: [
              {
                affiliation: 'Stanford University',
                displayOrderKey: [0, 'Garg, Rocky'],
                emailHash: 'b1cf93eba33cde4c13e539a0ae6e9d24',
                familyName: 'Garg',
                firstName: 'Rocky',
                name: 'Rocky Garg',
              },
              {
                affiliation: 'University of Cincinnati',
                displayOrderKey: [0, 'Sokoloff, Michael'],
                emailHash: '5e2a0270182c7b8d47188feec1f6878b',
                familyName: 'Sokoloff',
                firstName: 'Michael',
                name: 'Michael Sokoloff',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12160,
            sessionSlotId: 2723,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Advances in developing deep neural networks for finding primary vertices in proton-proton collisions at the LHC',
            uniqueId: 'c17181',
            url: '/event/459/contributions/11714/',
          },
        },
        entryType: 'Session',
        friendlyId: 13,
        id: 's12160',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2043/session-timetable.pdf',
        room: 'Hampton Roads VII',
        sessionCode: '',
        sessionId: 2043,
        sessionSlotId: 2723,
        slotTitle: 'Tracking',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#03070f',
        title: 'Track 9 - Artificial Intelligence and Machine Learning',
        uniqueId: 's12160',
        url: '/event/459/sessions/2043/',
      },
      s12164: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#4f144e',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Fermi National Accelerator Laboratory',
            displayOrderKey: [0, 'Timm, Steven'],
            emailHash: '40848b6b99ba5ca40be682891f2cc812',
            familyName: 'Timm',
            firstName: 'Steven',
            name: 'Steven Timm',
          },
          {
            affiliation: 'CSIC',
            displayOrderKey: [0, 'Campos, Isabel'],
            emailHash: '1ba624fa939fe1db5c536ca7116de83b',
            familyName: 'Campos',
            firstName: 'Isabel',
            name: 'Isabel Campos',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17229: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11825/attachments/9463/14024/GPU_workflows-CKK-CHEP-2023_v2.pdf',
                  id: 14024,
                  title: 'GPU_workflows-CKK-CHEP-2023_v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11825,
            description:
              'The CMS experiment at CERN accelerates several stages of its online reconstruction by making use of GPU resources at its High Level Trigger (HLT) farm for LHC Run 3. Additionally, during the past years, computing resources available to the experiment for performing offline reconstruction, such as Tier-1 and Tier-2 sites, have also started to integrate accelerators into their systems. In order to make efficient use of these heterogeneous platforms, it is essential to adapt both the CMS production system and the CMSSW reconstruction code to make use of GPUs. Ths CMSSW offline reconstruction can now partially run on GPUs, inheriting from the work done at the HLT. Parts of the production systems infrastructure have also been adapted to successfully map, schedule and run the available GPU-enabled workflows on different sites across the computing grid. This talk will describe the process of commissioning GPU-enabled CMSSW workflows through the production system and will present first results from the deployment of GPU-enabled offline reconstruction workflows.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 63,
            id: 'c17229',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11825/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Wisconsin Madison (US)',
                displayOrderKey: [1, 'Koraka, Charis Kleio'],
                emailHash: 'facbe3ec625b34e011eeaf7d505cfd07',
                familyName: 'Koraka',
                firstName: 'Charis Kleio',
                name: 'Charis Kleio Koraka',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12164,
            sessionSlotId: 2727,
            startDate: {
              date: '2023-05-09',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Running GPU-enabled CMSSW workflows through the production system',
            uniqueId: 'c17229',
            url: '/event/459/contributions/11825/',
          },
          c17230: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11817/attachments/9504/13779/LHCb_GPU_CHEP.pdf',
                  id: 13779,
                  title: 'LHCb_GPU_CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11817,
            description:
              'The LHCb experiment uses a triggerless readout system where its first stage (HLT1) is implemented on GPU cards. The full LHC event rate of 30 MHz is reduced to 1 MHz using efficient parallellisation techniques in order to meet throughput requirements. The GPU cards are hosted in the same servers as the FPGA cards receiving the detector data which reduces the network to a minimum. In this talk, the commissioning of this heterogeneous architecture using the first Run 3 data is presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 595,
            id: 'c17230',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11817/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Fitzpatrick, Conor'],
                emailHash: '1bfe8d300d5c15a0f0492c114d3d4b5d',
                familyName: 'Fitzpatrick',
                firstName: 'Conor',
                name: 'Conor Fitzpatrick',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12164,
            sessionSlotId: 2727,
            startDate: {
              date: '2023-05-09',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'LHCb GPU trigger commissioning with first data - LHCb',
            uniqueId: 'c17230',
            url: '/event/459/contributions/11817/',
          },
          c17231: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11822/attachments/9333/13534/Run-3_Commissioning_of_CMS_HLT_%20Final.pdf',
                  id: 13534,
                  title: 'Run-3_Commissioning_of_CMS_HLT_ Final.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11822,
            description:
              'The software based High Level Trigger (HLT) of CMS reduces the data readout rate from 100kHz (obtained from Level 1 trigger) to around 2kHz. It makes use of all detector subsystems and runs a streamlined version of CMS reconstruction. Run-1 and Run-2 of the LHC saw the reconstruction algorithm run on a CPU farm (~30000 CPUs in 2018). But the need to have increased computational power as we approach the high luminosity phase of LHC demands the use of Graphical Processing Units (GPUs) to reign in the cost, size and power consumption of the HLT farm. Parallelization of the reconstruction algorithms, on top of the multi-threading functionality introduced in Run2, allowed parts of HCAL, ECAL and pixel reconstruction to be offloaded to NVIDIA GPUs. In order to ensure the reproducibility of physics results on any machine, the HLT configuration was designed to run seamlessly with and without GPUs, that is, the algorithms were automatically offloaded to a GPU when one was available, and otherwise fell back to running on the CPU. This contribution will describe the development of GPU-based algorithms for the HLT and the challenges they presented, along with the comprehensive validation and commissioning activity undertaken by CMS to ensure the successful operations of the new HLT farm.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 159,
            id: 'c17231',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11822/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Wisconsin Madison',
                displayOrderKey: [1, 'Parida, Ganesh'],
                emailHash: 'f87389687f454c64d9479dbdf86f3060',
                familyName: 'Parida',
                firstName: 'Ganesh',
                name: 'Ganesh Parida',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12164,
            sessionSlotId: 2727,
            startDate: {
              date: '2023-05-09',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Run-3 Commissioning of CMS Online HLT reconstruction using GPUs',
            uniqueId: 'c17231',
            url: '/event/459/contributions/11822/',
          },
          c17232: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11803/attachments/9598/13969/LHCbHPCStateOfPractice_CHEP2023.pdf',
                  id: 13969,
                  title: 'LHCbHPCStateOfPractice_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11803,
            description:
              'To better understand experimental conditions and performances of the Large Hadron Collider (LHC), CERN experiments execute tens of thousands of loosely-coupled Monte Carlo simulation workflows per hour on hundreds of thousands - small to mid-size - distributed computing resources federated by the Worldwide LHC Computing Grid (WLCG). While this approach has been reliable during the first LHC runs, WLCG will be limited to meet future computing needs. In the meantime, High-Performance Computing resources, and more specifically supercomputers, offers a significant additional amount of computing resources but they also come with higher integration challenges.\r\n\r\nThis state-of-practice paper outlines years of integration of LHCb simulation workflows on several supercomputers. The main contributions of this paper are: (i) an extensive description of the gap to address to run High-Energy Physics Monte Carlo simulation workflows on supercomputers; (ii) various methods and proposals to submit High-Throughput Computing workflows and maximize the use of allocated CPU resources; (iii) a comprehensive analysis of LHCb production workflows running on diverse supercomputers.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 481,
            id: 'c17232',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11803/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Boyer, Alexandre'],
                emailHash: '600157e111d2c152848eafcaa55af962',
                familyName: 'Boyer',
                firstName: 'Alexandre',
                name: 'Alexandre Boyer',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12164,
            sessionSlotId: 2727,
            startDate: {
              date: '2023-05-09',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Integrating LHCb workflows on Supercomputers: State of Practice',
            uniqueId: 'c17232',
            url: '/event/459/contributions/11803/',
          },
          c17233: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11809/attachments/9402/13640/CHEP23%20FCS%20Portability.pdf',
                  id: 13640,
                  title: 'CHEP23 FCS Portability.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11809,
            description:
              'FastCaloSim is a parameterized simulation of the particle energy response and of the energy distribution in the ATLAS calorimeter. It is a relatively small and self-contained package with massive inherent parallelism and captures the essence of GPU offloading via important operations like data transfer, memory initialization, floating point operations, and reduction. Thus, it was identified as a good testbed for evaluating the performance and ease\r\n of portability of programming models. As a part of the HEP Center for Computational Excellence project, FastCaloSim had been ported to GPU using CUDA, Kokkos, SYCL, and similar ports for std::parallel, Alpaka, and OpenMP are being developed. \r\n\r\nIn this presentation, we will give an overview of the progress made with the std::parallel, Alpaka and OpenMP implementations of FastCaloSim. In particular, performance benchmarks on NVIDIA\r\n GPUs, AMD GPUs and multicore CPUs will be reported for each programming model wherever available, along with the comparison of pros and cons of each approach.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 137,
            id: 'c17233',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11809/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lawrence Berkeley National Laboratory',
                displayOrderKey: [3, 'Leggett, Charles'],
                emailHash: '4355184b024c148f5702ea55f82fb90a',
                familyName: 'Leggett',
                firstName: 'Charles',
                name: 'Charles Leggett',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12164,
            sessionSlotId: 2727,
            startDate: {
              date: '2023-05-09',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Porting ATLAS FastCaloSim to GPUs with Performance Portable Programming Models',
            uniqueId: 'c17233',
            url: '/event/459/contributions/11809/',
          },
          c17234: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11810/attachments/9269/13926/20230509-CHEP23_CMSSWHeterogeneousScheduling.pdf',
                  id: 13926,
                  title: '20230509-CHEP23_CMSSWHeterogeneousScheduling.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11810,
            description:
              'The CMS experiment started to utilize Graphics Processing Units (GPU) to accelerate the online reconstruction and event selection running on its High Level Trigger (HLT) farm in the 2022 data taking period. The projections of the HLT farm to the High-Luminosity LHC foresee a significant use of compute accelerators in the LHC Run 4 and onwards in order to keep the cost, size, and power budget of the farm under control. This direction of leveraging compute accelerators has synergies with the increasing use of HPC resources in HEP computing, as HPC machines are employing more and more compute accelerators that are predominantly GPUs today. In this work we review the features developed for the CMS data processing framework, CMSSW, to support the effective utilization of both compute accelerators and many-core CPUs within a highly concurrent task-based framework. We measure the impact of various design choices for the scheduling of heterogeneous algorithms on the event processing throughput, using the Run-3 HLT application as a realistic use case.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 77,
            id: 'c17234',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11810/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermi National Accelerator Laboratory',
                displayOrderKey: [1, 'Kortelainen, Matti'],
                emailHash: '256b1809c77f751f332ca80c677a9e31',
                familyName: 'Kortelainen',
                firstName: 'Matti',
                name: 'Dr Matti Kortelainen',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12164,
            sessionSlotId: 2727,
            startDate: {
              date: '2023-05-09',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Performance of Heterogeneous Algorithm Scheduling in CMSSW',
            uniqueId: 'c17234',
            url: '/event/459/contributions/11810/',
          },
        },
        entryType: 'Session',
        friendlyId: 15,
        id: 's12164',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2045/session-timetable.pdf',
        room: 'Marriott Ballroom VII',
        sessionCode: '',
        sessionId: 2045,
        sessionSlotId: 2727,
        slotTitle: 'GPUs in Online and Offline',
        startDate: {
          date: '2023-05-09',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffefff',
        title:
          'Track X - Exascale Science, Heterogeneous Computing and Accelerators, and Quantum Computing',
        uniqueId: 's12164',
        url: '/event/459/sessions/2045/',
      },
      s12165: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d9dfc3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Pittsburgh',
            displayOrderKey: [0, 'Bandieramonte, Marilena'],
            emailHash: 'f4b45dbfcfef5c90c1b484b28e777c71',
            familyName: 'Bandieramonte',
            firstName: 'Marilena',
            name: 'Marilena Bandieramonte',
          },
          {
            affiliation: 'Fermilab',
            displayOrderKey: [0, 'Yang, Tingjun'],
            emailHash: '521815c4f74228afa6e1c602c0d8d4a7',
            familyName: 'Yang',
            firstName: 'Tingjun',
            name: 'Tingjun Yang',
          },
        ],
        description: '',
        duration: 105.0,
        endDate: {
          date: '2023-05-09',
          time: '18:15:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17065: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11434/attachments/9529/13982/2023_05_09CHEP_ABUDINEN_EvtGen.pdf',
                  id: 13982,
                  title: '2023_05_09CHEP_ABUDINEN_EvtGen.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11434,
            description:
              'EvtGen is a simulation generator specialized for decays of heavy hadrons. Since its early development in the 90\u2019s, the generator has been extensively used and has become today an essential tool for heavy-flavour physics analyses. Throughout this time, its source code has remained mostly unchanged, except for additions of new decay models. In view of the upcoming boom of multi-threaded processing, we have launched a modernization campaign with the chief goal of making EvtGen thread safe. This talk will cover the challenges encountered in this endeavour and the milestones reached so far.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 249,
            id: 'c17065',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11434/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Warwick',
                displayOrderKey: [1, 'Abudin\u00e9n, Fernando'],
                emailHash: '3df0328718b5c7b41bf08885cc8306d6',
                familyName: 'Abudin\u00e9n',
                firstName: 'Fernando',
                name: 'Fernando Abudin\u00e9n',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12165,
            sessionSlotId: 2728,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'EvtGen \u2013 recent developments and prospects',
            uniqueId: 'c17065',
            url: '/event/459/contributions/11434/',
          },
          c17066: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11448/attachments/9478/13739/2023_CHEP_HIJING++_final.pdf',
                  id: 13739,
                  title: '2023_CHEP_HIJING++_final.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11448,
            description:
              'The upgrade of the Large Hadron Collider (LHC) is going well, during next decade we will face the ten-fold increase in experimental data. The application of state-of-the-art detectors and data acquisition systems requires high-performance simulation support, which even more demanding in case of heavy ion collisions. Our basic aim was to develop a Monte-Carlo simulation code for heavy ion collisions (but applicable for proton-proton collisions, as well), which can adopt all available acceleration opportunities from hardware side (e.g. GPUs) and innovative software solutions (e.g. machine learning). The new version of HIJING (Heavy Ion Jet INteraction Generator) fulfil these expectations. We present the latest results on hardware accelerations, indicate the novel software solutions and display the applicability of HIJING++ on existing heavy ion data. The presentation of our developments will indicate and summarize the main efforts of near future MC-codes for LHC physics.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 579,
            id: 'c17066',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11448/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Krasznahorkay, Attila'],
                emailHash: '8d4c0721de129e328dffeb903264d729',
                familyName: 'Krasznahorkay',
                firstName: 'Attila',
                name: 'Attila Krasznahorkay',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12165,
            sessionSlotId: 2728,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Introducing HIJING++: the Heavy Ion Monte Carlo Generator for the High-Luminosity LHC Era',
            uniqueId: 'c17066',
            url: '/event/459/contributions/11448/',
          },
          c17067: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11455/attachments/9539/13930/CHEP2023_VecGeom_surface_model.pdf',
                  id: 13930,
                  title: 'CHEP2023_VecGeom_surface_model.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11455,
            description:
              'In a context where the HEP community is striving to improve the software to cope with higher data throughput, detector simulation is adapting to benefit from new performance opportunities. Given the complexity of the particle transport modeling, new developments such as adapting to accelerator hardware represent a scalable R\u0026D effort. \r\n\r\nThe AdePT and Celeritas projects have already demonstrated the feasibility of porting realistic detector simulations to GPUs, which are becoming more and more available as computing resources. However, achieving efficiencies comparable to the standard CPU-based simulation still has essential work sites, and improving GPU support for geometry is one of them. VecGeom library is one of the geometry back-ends available for Geant4, used in production by several experiments. VecGeom is CUDA-aware, but recent studies have pinpointed the current GPU implementation as a major source of divergence and inefficiency in GPU simulation workflows.\r\n\r\nWe will present the results of a one-year effort to develop a fully portable geometry model mapping the existing Geant4 geometry descriptions to a GPU-friendly surface-based approach. The implementation is completely transparent and aims to provide a GPU implementation that factorizes the divergent code of the 3D primitive solids into simpler and more balanced 2D surface algorithms.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 66,
            id: 'c17067',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11455/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Gheata, Andrei'],
                emailHash: '5e451b3b7969b96f79563ef9500e6f3e',
                familyName: 'Gheata',
                firstName: 'Andrei',
                name: 'Andrei Gheata',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12165,
            sessionSlotId: 2728,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Surface-based GPU-friendly geometry modeling for detector simulation',
            uniqueId: 'c17067',
            url: '/event/459/contributions/11455/',
          },
          c17068: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11427/attachments/9538/13835/CHEP2023-AdePT.pdf',
                  id: 13835,
                  title: 'CHEP2023-AdePT.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11427,
            description:
              'Motivated by the need to have large Monte Carlo data statistics to be able to perform the physics analysis for the coming runs of HEP experiments, particularly for HL-LHC, there are a number of efforts exploring different avenues for speeding up particle transport simulation. In particular, one of the possibilities is to re-implement the simulation code to run efficiently on GPUs. This could allow future large Monte Carlo productions to utilise GPU resources, as well as traditional CPUs.\r\n\r\nWe present the status and plans of the Accelerated demonstrator of electromagnetic Particle Transport (AdePT) R\u0026D project. The goal of this development is to provide a realistic demonstrator of electromagnetic calorimeter simulation on GPUs, with the geometry as complex as the LHC experiments\u2019 detectors, complete electromagnetic physics, and all the required energy scoring infrastructure. We will discuss the GPU-specific workflow of this prototype, and describe the implementation of its different components.\r\nWe will also look into the aspect of integrating the new GPU-based simulation module with the existing CPU-based ones, namely the interfacing with the Geant4 toolkit. We will show a possible scenario of running the existing Geant4 simulations with their calorimeter part delegated to AdePT on GPUs.\r\nWe will present the performance both in the standalone mode as well as when integrated into Geant4, discuss the identified bottlenecks and propose a plan of possible further optimizations.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 163,
            id: 'c17068',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11427/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Gheata, Andrei'],
                emailHash: '5e451b3b7969b96f79563ef9500e6f3e',
                familyName: 'Gheata',
                firstName: 'Andrei',
                name: 'Andrei Gheata',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12165,
            sessionSlotId: 2728,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Accelerated demonstrator of electromagnetic Particle Transport (AdePT) status and plans',
            uniqueId: 'c17068',
            url: '/event/459/contributions/11427/',
          },
          c17069: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11442/attachments/9636/14090/20230509_slides.pdf',
                  id: 14090,
                  title: '20230509_slides.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11442,
            description:
              'Monte Carlo detector transport codes are one of the backbones in high-energy physics. They simulate the transport of a large variety of different particle types through complex detector geometries based on a multitude of physics models.\r\n\r\nThose simulations are usually configured or tuned through large sets of parameters. Often, tuning the physics accuracy on the one hand and optimising the resource needs on the other hand are competing requirements.\r\n\r\nIn this area, we are presenting a toolchain to tune Monte Carlo transport codes which is capable of automatically optimising large sets of parameters based on user-defined metrics.\r\n\r\nThe toolchain consists of two central components. Firstly, the MCReplay engine which is a quasi-Monte-Carlo transport engine able to fast replay pre-recorded MC steps. This engine for instance allows to study the impact of cut variations on quantities such as hits without the need to perform new full-simulations. Secondly, it consists of an automatic and generic parameter optimisation framework called O2Tuner.\r\n\r\nThe toolchain\u2019s application in concrete use-cases will be presented. Its first application in ALICE led to the reduction of CPU time of Monte Carlo detector transport by 30\\%. In addition, further possible scenarios will be discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 328,
            id: 'c17069',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11442/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Volkel, Benedikt'],
                emailHash: 'f846fe931c9d3c7795f7a38bd459bb7a',
                familyName: 'Volkel',
                firstName: 'Benedikt',
                name: 'Benedikt Volkel',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12165,
            sessionSlotId: 2728,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'A parameter optimisation toolchain for Monte Carlo detector simulation',
            uniqueId: 'c17069',
            url: '/event/459/contributions/11442/',
          },
          c17070: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11438/attachments/9328/13528/CHEP-2023-Oral-352.pdf',
                  id: 13528,
                  title: 'CHEP-2023-Oral-352.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11438,
            description:
              'Geant4, the leading detector simulation toolkit used in High Energy Physics, employs a set of physics models to simulate interactions of particles with matter across a wide range of interaction energies. These models, especially the hadronic ones, rely largely on directly measured cross-sections and inclusive characteristics, and use physically motivated parameters. However, they generally aim to cover a very wide range of possible simulation tasks and may not always be optimized for a particular process or a given material.\r\nThe Geant4 collaboration recently made many parameters of the models accessible via a configuration interface.  This opens a possibility to fit simulated distributions to thin target experimental datasets and extract optimal values of the model parameters and the associated uncertainties.  Such efforts are currently undertaken by the Geant4 Collaboration with the goal of offering alternative sets of model parameters, aka \u201ctunes\u201d,  for certain applications. The efforts should subsequently lead to more accurate estimates of the systematic errors in physics measurements given the detector simulation role in performing the physics measurements.  \r\nResults from the study will be presented to illustrate how Geant4 model parameters can be optimized through applying fitting techniques, to improve the agreement between the Geant4 and the experimental data.   \r\n\r\nKeywords: Geant4 toolkit, hadronic interactions, optimizations of phenomenological models, fitting technique',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 352,
            id: 'c17070',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11438/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermi National Accelerator Laboratory',
                displayOrderKey: [1, 'Yarba, Julia'],
                emailHash: 'c1a8df445b1115f5cc44ad274887576c',
                familyName: 'Yarba',
                firstName: 'Julia',
                name: 'Julia Yarba',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12165,
            sessionSlotId: 2728,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Optimizing  Geant4 Hadronic Model Parameters Through Global Fits to Thin Target Data',
            uniqueId: 'c17070',
            url: '/event/459/contributions/11438/',
          },
        },
        entryType: 'Session',
        friendlyId: 7,
        id: 's12165',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2037/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VI',
        sessionCode: '',
        sessionId: 2037,
        sessionSlotId: 2728,
        slotTitle: 'Simulation (part 2)',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#272f09',
        title: 'Track 3 - Offline Computing',
        uniqueId: 's12165',
        url: '/event/459/sessions/2037/',
      },
      s12166: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#efebc2',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Unive',
            displayOrderKey: [0, 'Barreiro Megino, Fernando'],
            emailHash: 'f04de6eee7b8dcc20dedefcb9fe659da',
            familyName: 'Barreiro Megino',
            firstName: 'Fernando',
            name: 'Fernando Barreiro Megino',
          },
          {
            affiliation: 'STFC-RAL',
            displayOrderKey: [0, 'Ellis, Katy'],
            emailHash: 'e4c8a78292759108e5511297386cac3a',
            familyName: 'Ellis',
            firstName: 'Katy',
            name: 'Katy Ellis',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16860: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11489/attachments/9483/13748/20230507-CHEP-SOC.pptx',
                  id: 13748,
                  title: '20230508-CHEP-SOC.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11489,
            description:
              'No single organisation has the resources to defend its services alone against most modern malicious actors and so we must protect ourselves as a community. In the face of determined and well-resourced attackers, we must actively collaborate in this effort across HEP and more broadly across Research and Education (R\u0026E).\r\n\r\nParallel efforts are necessary to appropriately respond to this requirement. We must both share threat intelligence about ongoing cybersecurity incidents with our trusted partners, and deploy the fine-grained security network monitoring necessary to make active use of this intelligence. We must also engage with senior management in our organisations to ensure that we work alongside any broader organisational cybersecurity development programmes.\r\n\r\nWe report on progress of the Security Operations Centre (SOC) Working Group, established by the WLCG but with membership encompassing the R\u0026E sector. The goal of the Working Group is to develop reference designs for SOC deployments and empower R\u0026E organisations to collect, leverage and act upon targeted, contextualised, actionable threat intelligence. This report will include recent SOC deployment activities at sites with network connectivity in excess of 100Gb/s, as well as new technology designs. An important development, which is likely to form a key part of the WLCG security strategy, is the potential use of passive DNS logs to allow sites without fine-grained network monitoring to benefit from the threat intelligence available to our community.\r\n\r\nWe also report on higher level progress in engaging with the broader community in establishing common approaches to this vital area of cybersecurity.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 480,
            id: 'c16860',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11489/contribution.pdf',
            presenters: [
              {
                affiliation: 'UKRI - STFC',
                displayOrderKey: [1, 'Crooks, David'],
                emailHash: '281c7701ded042cad09869e9f987e103',
                familyName: 'Crooks',
                firstName: 'David',
                name: 'David Crooks',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12166,
            sessionSlotId: 2729,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Collaborative Operational Security: The future of cybersecurity for Research and Education',
            uniqueId: 'c16860',
            url: '/event/459/contributions/11489/',
          },
          c16861: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11497/attachments/9472/13957/2fa-chep.pdf',
                  id: 13957,
                  title: '2fa-chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11497,
            description:
              'In 2022, CERN ran its annual phishing campaign in which 2000 users gave away their passwords (Note: this number is in line with results of campaigns at other organisations). In a real phishing incident this would have meant 2000 compromised accounts... unless they were protected by Two-Factor Authentication (2FA)! In the same year, CERN introduced 2FA for accounts with access to critical services. The new login flow requires users to always authenticate with a 2FA token (either TOTP or WebAuthn), introducing a significant security improvement for the individual and the laboratory. In this paper we will discuss the rationale behind the 2FA deployment, as well as the technical setup of 2FA in CERN\u0027s Single Sign-On, Keycloak. We will share statistics on how users are responding to the change, and concrete actions we have taken thanks to their feedback. Finally, we briefly cover our custom extensions to Keycloak for specific use cases, which include, persistent cookies and our Kerberos setup.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 270,
            id: 'c16861',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11497/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Ahmad, Adeel'],
                emailHash: 'b9865016eec8fb0ba07578a73379f39f',
                familyName: 'Ahmad',
                firstName: 'Adeel',
                name: 'Adeel Ahmad',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12166,
            sessionSlotId: 2729,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Improving computer security in HEP with multiple factor authentication: experience and reflections',
            uniqueId: 'c16861',
            url: '/event/459/contributions/11497/',
          },
          c16862: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11492/attachments/9358/13998/TDack_Chep23_WLCGTokens.pdf',
                  id: 13998,
                  title: 'TDack_Chep23_WLCGTokens.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11492/attachments/9358/13564/TDack_Chep23_WLCGTokens.pptx',
                  id: 13564,
                  title: 'TDack_Chep23_WLCGTokens.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11492,
            description:
              'Since 2017, the Worldwide LHC Computing Grid (WLCG) has been working towards enabling token-based authentication and authorization throughout its entire middleware stack. Following the initial publication of the WLCG v1.0 Token Schema in 2019, work has been done to integrate OAuth2.0 token flows across the Grid middleware. There are many complex challenges to be addressed before the WLCG can be end-to-end token-based, including not just technical hurdles but also interoperability with the wider authentication and authorization landscape.\r\nThis paper presents the status of the WLCG coordination and deployment work, and how it relates to software providers and partner communities. The authors also detail how the WLCG token transition timeline has progressed, and how it has changed since its publication.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 139,
            id: 'c16862',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11492/contribution.pdf',
            presenters: [
              {
                affiliation: 'STFC UKRI',
                displayOrderKey: [1, 'Dack, Tom'],
                emailHash: '5621ea9eaba4a40919267a5988f03d35',
                familyName: 'Dack',
                firstName: 'Tom',
                name: 'Tom Dack',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12166,
            sessionSlotId: 2729,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'WLCG transition from X.509 to tokens: Status and Plans',
            uniqueId: 'c16862',
            url: '/event/459/contributions/11492/',
          },
          c16863: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11479/attachments/9600/13944/CHEP2003-GlideinWMSToken.pdf',
                  id: 13944,
                  title: 'CHEP2003-GlideinWMSToken.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11479,
            description:
              'GlideinWMS is a distributed workload manager that has been used in production for many years to provision resources for experiments like CERN\u0027s CMS, many Neutrino experiments, and the OSG. Its security model was based mainly on  GSI (Grid Security Infrastructure), using x509 certificate proxies and VOMS (Virtual Organization Membership Service) extensions. Even if other credentials, like ssh keys, were possible to authenticate with resources, proxies were also added all the time, to establish the identity of the requestor and the associated memberships or privileges. This single credential was used for everything and was, often implicitly, forwarded wherever needed.\r\nThe addition of identity and access tokens and the phase-out of GSI forced us to reconsider the security model of GlideinWMS, in order to handle multiple credentials which can differ in type, technology, and functionality.\r\nBoth identity tokens and access tokens are possible. GSI proxies even if no more mandatory, are still used, together with various JWT (JSON Web Token) based tokens and other certificates. The functionality of the credentials, defined by issuer, audience, and scope, also differ: a credential can allow access to a computing resource, or can protect the GlideinWMS framework from tampering, or can grant read or write access to storage, can provide an identity for accounting or auditing, or can provide a combination of any the formers. Furthermore, the tools in use do not include automatic forwarding and renewal of the new credentials so credential lifetime and renewal requirements became part of the discussion as well.\r\nIn this paper, we will present how GlideinWMS was able to change its design and code to respond to all these changes.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 624,
            id: 'c16863',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11479/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Mambelli, Marco'],
                emailHash: 'f3deb9637a78bebf20c5c69407f5b777',
                familyName: 'Mambelli',
                firstName: 'Marco',
                name: 'Marco Mambelli',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12166,
            sessionSlotId: 2729,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Transitioning GlideinWMS, a multi domain distributed workload manager, from GSI proxies to tokens and other granular credentials',
            uniqueId: 'c16863',
            url: '/event/459/contributions/11479/',
          },
          c16864: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11513/attachments/9604/13955/20230511%20CMS%20SI%20Tokens%20CHEP23.pdf',
                  id: 13955,
                  title: '20230511 CMS SI Tokens CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11513,
            description:
              'The CMS Submission Infrastructure (SI) is the main computing resource provisioning system for CMS workloads. A number of HTCondor pools are employed to manage this infrastructure, which aggregates geographically distributed resources from the WLCG and other providers. Historically, the model of authentication among the diverse components of this infrastructure has relied on the Grid Security Infrastructure (GSI), based on identities and X509 certificates. In contrast, commonly used modern authentication standards are based on capabilities and tokens. The WLCG has identified this trend and aims at a transparent replacement of GSI for all its workload management, data transfer and storage access operations, to be completed during the current LHC Run 3. As part of this effort, and within the context of CMS computing, the Submission Infrastructure group is in the process of phasing out the GSI part of its authentication layers, in favor of IDTokens and Scitokens. The use of tokens is already well integrated into the HTCondor Software Suite, which has allowed us to fully migrate the authentication between internal components of SI. Additionally, recent versions of the HTCondor-CE support tokens as well, enabling CMS resource requests to Grid sites employing this CE technology to be granted by means of token exchange. After a rollout campaign to sites, successfully completed by the third quarter of 2022, the totality of HTCondor CEs in use by CMS are already receiving Scitoken-based pilot jobs. On the ARC CE side, a parallel campaign was launched to foster the adoption of the REST interface at CMS sites (required to enable token-based job submission via HTCondor-G), which is nearing completion as well. In this contribution, the newly adopted authentication model will be described. We will then report on the migration status and final steps towards complete GSI phase out in the CMS SI.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 212,
            id: 'c16864',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11513/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of California San Diego',
                displayOrderKey: [1, 'Mascheroni, Marco'],
                emailHash: '91b6e0458cfc38c6e2b8701f5a7b9a24',
                familyName: 'Mascheroni',
                firstName: 'Marco',
                name: 'Marco Mascheroni',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12166,
            sessionSlotId: 2729,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Adoption of a token-based authentication model for the CMS Submission Infrastructure',
            uniqueId: 'c16864',
            url: '/event/459/contributions/11513/',
          },
          c16865: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11468/attachments/9597/13961/DIRACDev_CHEP2023.pdf',
                  id: 13961,
                  title: 'DIRACDev_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11468,
            description:
              'DIRAC is the interware for building and operating large scale distributed computing systems. It is adopted by multiple collaborations from various scientific domains for implementing their computing models.\r\nDIRAC provides a framework and a rich set of ready-to-use services for Workload, Data and Production Management tasks of small, medium and large scientific communities having different computing requirements. The base functionality can be easily extended by custom components supporting community specific workflows. A single DIRAC service can provide a complete solution for the distributed computing of one, or multiple collaborations. The Workload Management System provides a transparent, uniform interface for managing computing resources and complex workflows. The Data Management System offers several tools to ensure data handling operations. DIRAC put special emphasis on the large scale data productions and datasets management.\r\nThis contribution will highlight DIRAC\u0027s current, upcoming and planned capabilities and technologies. Examples include, but are not limited to, adoption of security tokens and interactions with Identity Provider services, integration of Clouds and High Performance Computers, interface with Rucio, improved monitoring and deployment procedures.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 313,
            id: 'c16865',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11468/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [5, 'Boyer, Alexandre'],
                emailHash: '600157e111d2c152848eafcaa55af962',
                familyName: 'Boyer',
                firstName: 'Alexandre',
                name: 'Dr Alexandre Boyer',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12166,
            sessionSlotId: 2729,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'DIRAC: current, upcoming and planned capabilities and technologies',
            uniqueId: 'c16865',
            url: '/event/459/contributions/11468/',
          },
        },
        entryType: 'Session',
        friendlyId: 8,
        id: 's12166',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2038/session-timetable.pdf',
        room: 'Marriott Ballroom II-III',
        sessionCode: '',
        sessionId: 2038,
        sessionSlotId: 2729,
        slotTitle: 'Security and Tokens',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 4 - Distributed Computing',
        uniqueId: 's12166',
        url: '/event/459/sessions/2038/',
      },
      s12168: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfe555',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Liverpool',
            displayOrderKey: [0, 'Rodrigues, Eduardo'],
            emailHash: '69164b74ffee995c119954e624bd24ec',
            familyName: 'Rodrigues',
            firstName: 'Eduardo',
            name: 'Eduardo Rodrigues',
          },
          {
            affiliation: 'FNAL',
            displayOrderKey: [0, 'Sexton, Elizabeth'],
            emailHash: 'e8fb00f4f09841337efaeff677169cd2',
            familyName: 'Sexton',
            firstName: 'Elizabeth',
            name: 'Elizabeth Sexton',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16888: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11525/attachments/9360/13567/CHEP2023_Ensuring_Simulation_Quality_in_the_LHCb_with_updates.pdf',
                  id: 13567,
                  title: 'CHEP2023_Ensuring_Simulation_Quality_in_the_LHCb_with_updates.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11525,
            description:
              'Monte Carlo simulations are a key tool for the physics program of High Energy Experiments. Their accuracy and reliability is of the utmost importance. A full suite of verifications is in place for the LHCb Simulation software to ensure the quality of the simulated samples produced.\r\nIn this contribution we will give a short overview of the procedure and the tests in place, that exploits the LHCb software testing infrastructure. First level verifications are performed as soon as new software is submitted for integration in the LHCb GitLab repository. The first step consists of Continous Integration (CI) tests and so called \u2018nightly tests\u2019 performed to verify the integrity of the software with short jobs run every night. Next, in-depth performance and regression tests are carried out with the LHCbPR dedicated infrastructure. Samples of O(1000) events are generated and plots of a wide spectrum of physics observables are compared to references. Trends of performance metrics are also produced. The most recent and final step is performed after the software is deployed for production. By verifying distributions of key quantities for all simulation productions on a fraction of the events, we ensure that the output is as expected before the full samples are produced.\r\nSimulation Data Quality shifters verify the outcome of all steps in the verification chain, and alert experts of anomalies and unexpected changes.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 528,
            id: 'c16888',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11525/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Couturier, Benjamin'],
                emailHash: '244c8becc3b603c6e5722f14640d8e65',
                familyName: 'Couturier',
                firstName: 'Benjamin',
                name: 'Benjamin Couturier',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12168,
            sessionSlotId: 2731,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Ensuring Simulation Quality in the LHCb experiment',
            uniqueId: 'c16888',
            url: '/event/459/contributions/11525/',
          },
          c16896: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11533/attachments/9496/13762/CoffeaCHEP_LindseyGray_09052023.pdf',
                  id: 13762,
                  title: 'CoffeaCHEP_LindseyGray_09052023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11533,
            description:
              'The recent release of AwkwardArray 2.0 significantly changes the way that lazy evaluation and task-graph building are handled in columnar analysis. The Dask parallel processing library is now used for these pieces of functionality with AwkwardArray, and this change affords new ways of optimizing columnar analysis and distributing it on clusters. In particular this allows optimization of a task graph all the way to the user code, possibly obviating the \u201cprocessor\u201d pattern Coffea has relied upon up to now. Utilizing this functionality completely required a major retooling of Coffea for this new infrastructure, which has resulted in a more extensible and easily maintainable codebase depending on the dask-awkward, and dask-histogram packages. We will demonstrate comparative performance benchmarks between Awkward-array 1.0 and Awkward-array 2.0 based releases of Coffea, as well as between processor-based and fully-dask-optimized compute graphs in AwkwardArray 2.0.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 88,
            id: 'c16896',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11533/contribution.pdf',
            presenters: [
              {
                affiliation: 'FNAL',
                displayOrderKey: [0, 'Gray, Lindsey'],
                emailHash: '8155b1dbe44fae6fa833ef9731425a83',
                familyName: 'Gray',
                firstName: 'Lindsey',
                name: 'Lindsey Gray',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12168,
            sessionSlotId: 2731,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Fine-Grained HEP Analysis Task Graph Optimization with Coffea and Dask',
            uniqueId: 'c16896',
            url: '/event/459/contributions/11533/',
          },
          c16897: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11539/attachments/9571/13890/2023-05-09_rieger_law_chep.pdf',
                  id: 13890,
                  title: '2023-05-09_rieger_law_chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11539,
            description:
              'In particle physics, workflow management systems are primarily used as tailored solutions in dedicated areas such as Monte Carlo production. However, physicists performing data analyses are usually required to steer their individual, complex workflows manually, frequently involving job submission in several stages and interaction with distributed storage systems by hand. This process is not only time-consuming and error-prone, but also leads to undocumented relations between particular workloads, rendering the steering of an analysis a serious challenge.\r\nThis contribution presents the Luigi Analysis Workflow (law) Python package which is based on the open-source pipelining tool luigi, originally developed by Spotify. It establishes a generic design pattern for analyses of arbitrary scale and complexity, and shifts the focus from executing to defining the analysis logic. Law provides the building blocks to seamlessly integrate with interchangeable remote resources without, however, limiting itself to a specific choice of infrastructure.\r\nIn particular, it introduces the concept of complete separation between analysis algorithms on the one hand, and run locations, storage locations, and software environments on the other hand. To cope with the sophisticated demands of end-to-end HEP analyses, law supports job execution on WLCG infrastructure (ARC, gLite, CMS-crab) as well as on local computing clusters (HTCondor, Slurm, LSF), remote file access via various protocols using the Grid File Access Library (GFAL2), and an environment sandboxing mechanism with support for sub-shells and virtual environments, as well as Docker and Singularity containers. Moreover, the novel approach ultimately aims for analysis preservation out-of-the-box.\r\nLaw is developed open-source and independent of any experiment or the language of executed code. Over the past years, its user-base increased steadily with applications now ranging from (pre-)processing workflows in CMS physics objects groups, to pipelines performing the statistical inference in most CMS di-Higgs searches, and it serves as the underlying core software for large scale physics analyses across various research groups.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 110,
            id: 'c16897',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11539/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Hamburg',
                displayOrderKey: [1, 'Rieger, Marcel'],
                emailHash: '2649d10aea72ee92f56468d4dc217b91',
                familyName: 'Rieger',
                firstName: 'Marcel',
                name: 'Marcel Rieger',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12168,
            sessionSlotId: 2731,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Law: End-to-End Analysis Automation over Distributed Resources',
            uniqueId: 'c16897',
            url: '/event/459/contributions/11539/',
          },
          c16898: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11547/attachments/9200/13359/schreiner-pivarski-analysis-of-physicists.pdf',
                  id: 13359,
                  title: 'Slides',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11547,
            description:
              'Data analysis in particle physics is socially distributed: unlike centrally developed and executed reconstruction pipelines, the analysis work performed after Analysis Object Descriptions (AODs) are made and before the final paper review\u2014which includes particle and event selection, systematic error handling, decay chain reconstruction, histogram aggregation, fitting, statistical models, and machine learning\u2014are often performed \u201coff the GRID.\u201d\r\n\r\nThis presents a challenge for developers of analysis tools, who need to know how their tools are being used in order to focus efforts in development, documentation, and training. The most common methods have traditionally been direct conversations with known users, wide-cast surveys, and download counts, but each of these has its limitations.\r\n\r\nIn this talk, I will discuss the above as well as new methods of analyzing user behavior: collecting issue comments through GitHub and GitLab APIs, statically analyzing code from thousands of git repositories matching search criteria, and web analytics of documentation sites. Applying these methods to the Awkward Array library reveals the most commonly used functions, slice idioms, and data types, as well as what libraries Awkward Array is commonly used with and how data are transferred between them. Finally, I apply these methods to other physics analysis libraries to show the generality of the techniques.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 307,
            id: 'c16898',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11547/contribution.pdf',
            presenters: [
              {
                affiliation: 'Princeton University',
                displayOrderKey: [1, 'Schreiner, Henry'],
                emailHash: 'd650b9966acbbc9dbb5005deb583a407',
                familyName: 'Schreiner',
                firstName: 'Henry',
                name: 'Henry Schreiner',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12168,
            sessionSlotId: 2731,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Analysis of physics analysis',
            uniqueId: 'c16898',
            url: '/event/459/contributions/11547/',
          },
          c16899: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11555/attachments/9610/13977/2023-05-09_LbMCSubmit_CHEP_2023.pdf',
                  id: 13977,
                  title: '2023-05-09_LbMCSubmit_CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11555,
            description:
              'In the LHCb experiment, a wide variety of Monte Carlo simulated samples need to be produced for the experiment\u2019s physics programme. LHCb has a centralised production system for simulating, reconstructing and processing collision data, which runs on the DIRAC backend on the WLCG.\r\nTo cope with a large set of different types of sample, requests for simulation production are based on a concept of \u201cmodels\u201d (templates) for each data-taking period, with variations for different generators and fast-simulation techniques. Request are then customised via pre-defined configuration per type of event (i.e. decay). This allows requests to be created and handled efficiently on a world-wide distributed system by a small team of people. However, maintenance and regular updates of these models, as well as the creation of bespoke requests (e.g. with filtered output) can be time-consuming tasks, prone to human error.\r\nWe present LbMCSubmit: a new scriptable submission system which generates the necessary requests from a parametrisation of the desired samples. The numerous request models are replaced by a set of rules for creating requests, thus ensuring consistency and reducing the workload required for their maintenance. Support for common use-cases is built-in, while also allowing for fine-grained customisation as needed.\r\nData-files specifying production requests are collected in a GitLab repository, then tested and submitted by CI jobs, using a shared infrastructure with the existing Analysis Productions package. LbMCSubmit may also be used at the command-line for running local tests or submitting user jobs (e.g. for generator tuning studies) to DIRAC.\r\nLbMCSubmit results in a significant reduction in the time spent maintaining and updating request models, preparing and submitting the requests themselves, as well as ensuring that newly released configuration files (e.g. for new decay types) are immediately available in production.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 517,
            id: 'c16899',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11555/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [2, 'Burr, Christopher'],
                emailHash: '7c7a588eb87d4ea2e902e48588221559',
                familyName: 'Burr',
                firstName: 'Christopher',
                name: 'Dr Christopher Burr',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12168,
            sessionSlotId: 2731,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'LbMCSubmit: A new flexible and scalable request submission system for LHCb simulation',
            uniqueId: 'c16899',
            url: '/event/459/contributions/11555/',
          },
          c16900: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11524/attachments/9450/13703/Intelligent_Data_AS4BMX_HKS.pdf',
                  id: 13703,
                  title: 'Intelligent_Data_AS4BMX_HKS.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11524,
            description:
              'With the construction and operation of fourth-generation light sources like European Synchrotron Radiation Facility Extremely Brilliant Source (ESRF-EBS), Advanced Photon Source Upgrade (APS-U), Advanced Light Source Upgrade (ALS-U), High Energy Photon Source (HEPS), etc., several advanced biological macromolecule crystallography (MX) beamlines are or will be built and thereby the huge amount of raw experimental data will be accumulated. Besides, high-resolution hybrid pixel array detectors are equipped and thus such large-scale and excellent-quality data will bring stringent challenges on the traditional manual or semi-automatic processing procedures. In this report, we will introduce a user-friendly, AI-empowered, auto-pipelining data analysis system for MX. It consists of four modules: (1) a boosted decision tree (BDT) based module to intelligently utilize suitable tools or algorithms for data reduction i.e. from X-ray diffraction images (TIFF/HDF5 files) to reference reflection files (MTZ); (2) a structure prediction module using database-querying or AlphaFold/OpenFold real-time prediction, i.e. from FASTA sequences to protein data bank (PDB) files; (3) a model auto-building module composed of two branches, one is for high accuracy which is time-consuming and the other is fast with lose of accuracy; (4) a structure refinement module by deep learning. This system works in two modes. One is for real-time/online analysis that operated automatically in the background by monitoring the user experimental data folder and taking default processing parameters. And the other is usually called batch mode. Firstly, users will configure the analysis procedures in GUI and then process multiple data concurrently for performances. All the equipped tools or algorithms are designed as plugins and can be substituted in a convenient way. This data analysis system is based on and developed for HEPS initially, aiming at an automatic, intelligent, and high-efficiency software and will be open-source for academic research.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 537,
            id: 'c16900',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11524/contribution.pdf',
            presenters: [
              {
                affiliation: 'IHEP, CAS',
                displayOrderKey: [1, 'Sun, Hao-Kai'],
                emailHash: '6386dfaefc2a2e75fb9b1ae2db8f8588',
                familyName: 'Sun',
                firstName: 'Hao-Kai',
                name: 'Hao-Kai Sun',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12168,
            sessionSlotId: 2731,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'An Intelligent Data Analysis System for Biological Macromolecule Crystallography',
            uniqueId: 'c16900',
            url: '/event/459/contributions/11524/',
          },
        },
        entryType: 'Session',
        friendlyId: 9,
        id: 's12168',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2039/session-timetable.pdf',
        room: 'Chesapeake Meeting Room',
        sessionCode: '',
        sessionId: 2039,
        sessionSlotId: 2731,
        slotTitle: 'Sustainable Analysis',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 5 - Sustainable and Collaborative Software Engineering',
        uniqueId: 's12168',
        url: '/event/459/sessions/2039/',
      },
      s12169: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#feffbf',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Manchester',
            displayOrderKey: [0, 'Skidmore, Nicole'],
            emailHash: 'a9b5fe5778eaecee042fcc8cab09ec26',
            familyName: 'Skidmore',
            firstName: 'Nicole',
            name: 'Nicole Skidmore',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Hageboeck, Stephan'],
            emailHash: '64d6d832b57aa9e7597d4011228b0da0',
            familyName: 'Hageboeck',
            firstName: 'Stephan',
            name: 'Stephan Hageboeck',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16940: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11593/attachments/9558/13867/BenchmarkingAF-RDF-spiga.pdf',
                  id: 13867,
                  title: 'BenchmarkingAF-RDF-spiga.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11593,
            description:
              'The challenges expected for the HL-LHC era are pushing LHC experiments to re-think their computing models at many levels. The evolution toward solutions that allow an effortless interactive analysis experience is, among others, one of the topics followed closely by the CMS experiment. In this context, ROOT RDataFrame offers a high-level, lazy programming model which makes it a flexible and user-friendly tool for HEP analysis workflows. To support this paradigm shift even further, a distributed infrastructure which leverages Dask to offload interactive payloads has been set up in production on INFN resources, transparently integrating Grid, clouds and possibly HPC. It was then a natural fit to integrate the efforts on both solutions to get a peek on how a Phase2 analysis might look like. The presented work will provide an overview of the main technologies involved and will describe the results of the first benchmark using the analysis of Vector Boson Scattering (VBS) of same-sign W boson pairs processes with one hadronically-decaying tau lepton and one light lepton (electron or muon) in the final state .The analysis workflow includes systematic variations as well as pre- and post-selection phases. The proposed comparison between a \u201clegacy\u201d batch-based strategy and the interactive RDataframe is based on several metrics from event throughput to resource consumption. To achieve a fair comparison both cases have been executed running the same analysis on the very same set of resources hosted at the INFN distributed analysis facility.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 292,
            id: 'c16940',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11593/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN',
                displayOrderKey: [1, 'spiga, daniele'],
                emailHash: '5880cbe703708cd98c1fed43a96d322e',
                familyName: 'spiga',
                firstName: 'daniele',
                name: 'daniele spiga',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12169,
            sessionSlotId: 2732,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'Benchmarking distributed-RDataFrame with CMS analysis workflows on the INFN analysis infrastructure',
            uniqueId: 'c16940',
            url: '/event/459/contributions/11593/',
          },
          c16941: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11596/attachments/9573/13892/ATO-618%20RootInteractive%20tool%20for%20multidimensional%20statistical%20analysis,%20machine%20learning%20and%20analytical%20model%20validation.%20(2).pdf',
                  id: 13892,
                  title:
                    'ATO-618 RootInteractive tool for multidimensional statistical analysis, machine learning and analytical model validation. (2).pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11596,
            description:
              'ALICE, one of the four large experiments at CERN LHC, is a detector for the physics of heavy ions. In a high interaction rate environment, the pile-up of multiple events leads to an environment that requires advanced multidimensional data analysis methods.\r\n\r\nMachine learning (ML) has become very popular in multidimensional data analysis in recent years. Compared to the simple, low-dimensional analytical approaches used in the past, it is more difficult to interpret machine learning models and evaluate their uncertainties. On the other hand, oversimplification and reduction of dimensionality in the analysis lead to explanations becoming more complex or wrong.\r\n\r\nOur goal was to provide a tool for dealing with multidimensional problems, to simplify data analysis in many (optimally all relevant) dimensions, to fit and visualize multidimensional functions including their uncertainties and biases, to validate assumptions and approximations, to easy define the functional composition of analytical parametric and non-parametric functions, to use symmetries and to define multidimensional "invariant" functions/alarms.\r\n\r\nRootInteractive is a general-purpose tool for multidimensional statistical analysis. We use a declarative programming paradigm where we build the structure and elements of computer programs and express the logic of a computation without describing its control flow. This approach makes it easy to be used for domain experts, students and educators. RootInteractive provides functions for interactive, easily configurable visualization of unbinned and binned data, interactive n-dimensional histogramming/projection and derived aggregate information extraction on the server (Python/C++) and client (Javascript). We support client/server applications using Jupyter, or we can create a stand-alone client-side application/dashboard.\r\n\r\nUsing a combination of lossy and lossless data compression, datasets with, for example, O(10^7) entries times O(25) attributes can be analyzed interactively in the standalone application in the O(500 MBy) browser. By applying a suitable representative downsampling O(10^-2-10^-3) and subsequent reweighting or pre-aggregation on the server or bach farm, the effective monthly/annual statistics ALICE can be analyzed interactively in many dimensions for calibration/reconstruction validation/QA/QC or statistical/physical analysis.\r\n\r\nIn this contribution, we introduce the main features of our general-purpose statistical tool and demonstrate them with examples from ALICE, used in the development of simulations/calibrations/reconstructions for combined particle identification, the spatial point distortion algorithm and the combined multiplicity-centrality estimators.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 355,
            id: 'c16941',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11596/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [2, 'Eulisse, Giulio'],
                emailHash: '7a6c8f49981d093b41b365a4b522378d',
                familyName: 'Eulisse',
                firstName: 'Giulio',
                name: 'Giulio Eulisse',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12169,
            sessionSlotId: 2732,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'RootInteractive tool for multidimensional statistical analysis, machine learning and analytical model validation',
            uniqueId: 'c16941',
            url: '/event/459/contributions/11596/',
          },
          c16942: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11558/attachments/9664/14088/20230509_AGC_CHEP.pdf',
                  id: 14088,
                  title: '20230509_AGC_CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11558,
            description:
              'Realistic environments for prototyping, studying and improving analysis workflows are a crucial element on the way towards user-friendly physics analysis at HL-LHC scale. The IRIS-HEP Analysis Grand Challenge (AGC) provides such an environment. It defines a scalable and modular analysis task that captures relevant workflow aspects, ranging from large-scale data processing and handling of systematic uncertainties to statistical inference and analysis preservation. By being based on publicly available Open Data, the AGC provides a point of contact for the broader community. Multiple different implementations of the analysis task that make use of various pipelines and software stacks already exist.\r\n\r\nThis contribution presents an updated AGC analysis task. It features a machine learning component and expanded analysis complexity, including the handling of an extended and more realistic set of systematic uncertainties. These changes both align the AGC further with analysis needs at the HL-LHC and allow for probing an increased set of functionality.\r\n\r\nAnother focus is the showcase of a reference AGC implementation, which is heavily based on the HEP Python ecosystem and uses modern analysis facilities. The integration of various data delivery strategies is described, resulting in multiple analysis pipelines that are compared to each other.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 365,
            id: 'c16942',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11558/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Wisconsin\u2013Madison',
                displayOrderKey: [0, 'Held, Alexander'],
                emailHash: 'fed5998fb15c9323f97c094991d6da70',
                familyName: 'Held',
                firstName: 'Alexander',
                name: 'Alexander Held',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12169,
            sessionSlotId: 2732,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'Physics analysis for the HL-LHC: concepts and pipelines in practice with the Analysis Grand Challenge',
            uniqueId: 'c16942',
            url: '/event/459/contributions/11558/',
          },
          c16943: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11582/attachments/9645/14047/padulano_chep_2023.pdf',
                  id: 14047,
                  title: 'padulano_chep_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11582,
            description:
              'The growing amount of data generated by the LHC requires a shift in how HEP analysis tasks are approached. Efforts to address this computational challenge have led to the rise of a middle-man software layer, a mixture of simple, effective APIs and fast execution engines underneath. Having common, open and reproducible analysis benchmarks proves beneficial in the development of these modern tools. One such benchmark is provided by the Analysis Grand Challenge (AGC), which represents a blueprint for a realistic analysis pipeline. This contribution presents the first AGC implementation that leverages ROOT RDataFrame, a powerful, modern and scalable execution engine for the HENP use cases. The different steps of the benchmarks are written with a composable, flexible and fully Pythonic API. RDataFrame can then transparently run the computations on all the cores of a machine or on multiple nodes thanks to automatic dataset splitting and transparent workload distribution. The portability of this implementation is shown by running on various resources, from managed facilities to open cloud platforms for research, showing usage of interactive and distributed environments.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 369,
            id: 'c16943',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11582/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Padulano, Vincenzo Eduardo'],
                emailHash: 'b9c4463f34f23e6902c9283a48c0778a',
                familyName: 'Padulano',
                firstName: 'Vincenzo Eduardo',
                name: 'Vincenzo Eduardo Padulano',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12169,
            sessionSlotId: 2732,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'First implementation and results of the Analysis Grand Challenge with a fully Pythonic RDataFrame',
            uniqueId: 'c16943',
            url: '/event/459/contributions/11582/',
          },
          c16945: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11589/attachments/9609/13976/2023-05-09_Analysis_Productions_CHEP_2023.pdf',
                  id: 13976,
                  title: '2023-05-09_Analysis_Productions_CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11589,
            description:
              'Most analyses in the LHCb experiment start by filtering data and simulation stored on the WLCG. Traditionally this has been achieved by submitting user jobs that each process a small fraction of the total dataset. While this has worked well, it has become increasingly complex as the LHCb datasets have grown and this model requires all analysts to understand the intricacies of the grid. This model also burdens individuals with needing to document the way in which each file was processed.\r\n\r\nHere we present a more robust and efficient approach, known within LHCb as Analysis Productions. Filtering LHCb datasets to create ntuples is done by creating a merge request in GitLab, which is then tested automatically on a small subset of the data using Continuous Integration. Results of these tests are exposed via a dedicated website that aggregates the most important details. Once the merge request is reviewed and accepted, productions are submitted and run automatically using the power of the DIRAC transformation system. The output data is stored on grid storage and tools are provided to make it easily accessible for analysis.\r\n\r\nThis new approach has the advantage of being faster and simpler for analysts while also ensuring that the full processing chain is preserved and reproducible. Using GitLab to manage submissions encourages code review and the sharing of derived datasets between analyses.\r\n\r\nThe Analysis Productions system has been stress-tested with legacy data for a couple of years and is becoming the de facto standard by which data, legacy or run-3, is prepared for physics analysis. It has been scaled to analyses that process thousands of datasets and the approach of testing prior to submission is now being expanded to other production types in LHCb.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 565,
            id: 'c16945',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11589/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Burr, Chris'],
                emailHash: '7c7a588eb87d4ea2e902e48588221559',
                familyName: 'Burr',
                firstName: 'Chris',
                name: 'Chris Burr',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12169,
            sessionSlotId: 2732,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Analysis Productions: A declarative approach to ntupling',
            uniqueId: 'c16945',
            url: '/event/459/contributions/11589/',
          },
          c17317: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11598/attachments/9404/13642/PyPWA_CHEP_2023_v0.pdf',
                  id: 13642,
                  title: 'PyPWA_CHEP_2023_v0.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11598,
            description:
              'Abstract\r\nPyPWA is a toolkit designed to fit (regression) parametric models to data and to generate distributions (simulation) according to a given model (function). PyPWA software has been written under the python ecosystem with the goal of performing Amplitude or Partial Wave Analysis (PWA) in nuclear and particle physics experiments. The aim of spectroscopy experiments is often the identification short lived (strongly interacting) resonances that have decayed to the observed multi-particle final states. The PyPWA toolkit is built from individual and mostly disjoint components that the user can arrange in a variety of ways. PyPWA can solve broad collection of problems. Users just need to provide a function (model), data and simulation in their preferred formats. PyPWA will provide tools for two basic components, Data Processing (read, write, splitting) and Analysis (simulation, fitting and prediction). It also provides various ways of speeding up calculations through multi-threading and the use of GPUs. The flexibility of PyPWA and its use of many standard packages make it an ideal tool for both new and experienced scientists wanting to perform fits of models to data. The examples provided with the code allow for a quick start and the user-friendly Python ecosystem comes with a large user base with a lot of support. We will briefly describe the general features of amplitude analysis and describe the PyPWA software philosophy, structure and use.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 256,
            id: 'c17317',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11598/contribution.pdf',
            presenters: [
              {
                affiliation: 'Norfolk State University',
                displayOrderKey: [2, 'Jones, Mark'],
                emailHash: 'c692c5dfb08fef9f6ba425707bbd2c91',
                familyName: 'Jones',
                firstName: 'Mark',
                name: 'Mr Mark Jones',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12169,
            sessionSlotId: 2732,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'PyPWA: A Software Toolkit for Parameter Optimization and Amplitude Analysis',
            uniqueId: 'c17317',
            url: '/event/459/contributions/11598/',
          },
        },
        entryType: 'Session',
        friendlyId: 10,
        id: 's12169',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2040/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VIII',
        sessionCode: '',
        sessionId: 2040,
        sessionSlotId: 2732,
        slotTitle: 'Physics Analysis Workflows',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#1f1f02',
        title: 'Track 6 - Physics Analysis Tools',
        uniqueId: 's12169',
        url: '/event/459/sessions/2040/',
      },
      s12170: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#1a3f14',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'KEK',
            displayOrderKey: [0, 'Kishimoto, Tomoe'],
            emailHash: '1c2a9b3c698c4de9040fbe367da6851b',
            familyName: 'Kishimoto',
            firstName: 'Tomoe',
            name: 'Tomoe Kishimoto',
          },
          {
            affiliation: 'University of Nebraska-Lincoln',
            displayOrderKey: [0, 'Weitzel, Derek'],
            emailHash: 'ca0d239ab6ae0ec500eff8eb24779855',
            familyName: 'Weitzel',
            firstName: 'Derek',
            name: 'Derek Weitzel',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17106: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11800/attachments/9268/13523/GNAGNextGenNetworkSystem_CHEP2023.pptx',
                  id: 13523,
                  title: 'GNAGNextGenNetworkSystem_CHEP2023.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11800,
            description:
              'We will present the rapid progress, vision and outlook across multiple state of the art development lines within the Global Network Advancement Group and its Data Intensive Sciences and  SENSE/AutoGOLE working groups, which are designed to meet the present and future needs and address the challenges of the Large Hadron Collider and other science programs with global reach. Since it was founded in the Fall of 2019 and the working groups were formed in 2020, in partnership with ESnet, Internet2, CENIC, GEANT, ANA, RNP, StarLight, NRP, N-DISE, AmLight, and many other leading research and education networks and network R\u0026D projects, as well as Caltech, UCSD/SDSC, Fermilab, CERN, LBL, and many other leading universities and laboratories, the GNA-G working groups have deployed expanding virtual circuit and programmable testbed spanning six continents which supports continuous developments aimed at the next generation of programmable networks interworking with the science programs\u0027 computing and data management systems. The talk will cover examples of recent progress in developing and deploying new methods and approaches in multidomain virtual circuits, flow steering, path selection, load balancing and congestion avoidance, segment routing and machine learning based traffic prediction and optimization. Examples of results demonstrated at the Supercomputing 2022 conference (SC22) and under persistent development will be included.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 288,
            id: 'c17106',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11800/contribution.pdf',
            presenters: [
              {
                affiliation: 'California Institute of Technology',
                displayOrderKey: [0, 'Newman, Harvey'],
                emailHash: '050cb184c9548fc276c92cfc21327048',
                familyName: 'Newman',
                firstName: 'Harvey',
                name: 'Harvey Newman',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12170,
            sessionSlotId: 2733,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'The Global Network Advancement Group: A Next Generation System for the LHC and Data Intensive Sciences',
            uniqueId: 'c17106',
            url: '/event/459/contributions/11800/',
          },
          c17107: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11620/attachments/9244/13845/CHEP23_NOTED_CarmenMisaMoreira.pdf',
                  id: 13845,
                  title: 'CHEP23_NOTED_CarmenMisaMoreira.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11620,
            description:
              'The NOTED (Network Optimised Transfer of Experimental Data) project has successfully demonstrated the ability to dynamically reconfigure network links to increase the effective bandwidth available for FTS-driven transfers between endpoints, such as WLCG sites by inspecting on-going data transfers and so identifying those that are bandwidth-limited for a long period of time. Recently, the architecture of NOTED has been improved and the software has been packaged for easy distribution.\r\n\r\nThese improved capabilities and features of NOTED have been tested and demonstrated at various international conferences. For example, during demonstrations at Supercomputing 2022, independent instances of NOTED at CERN (Switzerland) and DE-KIT (Germany) monitored large data transfers generated by the ATLAS experiment between these sites and TRIUMF (Canada). We report here on this and other events, highlighting how NOTED can predict link congestion or a notable increase in the network utilisation over an extended period of time and, where appropriate, automatically recon\ufb01gure network topology to introduce an additional\u2014or an alternative and better performing\u2014path by using dynamic circuit provisioning systems such as SENSE and AutoGOLE.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 122,
            id: 'c17107',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11620/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Misa Moreira, Carmen'],
                emailHash: 'b68cb8dc9f41e0a70e34e9eeff037b8f',
                familyName: 'Misa Moreira',
                firstName: 'Carmen',
                name: 'Carmen Misa Moreira',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12170,
            sessionSlotId: 2733,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'NOTED: An intelligent network controller to improve the throughput of large data transfers in File Transfer Services by handling dynamic circuits',
            uniqueId: 'c17107',
            url: '/event/459/contributions/11620/',
          },
          c17108: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11640/attachments/9435/15243/20230509_CHEP_janus_data_caching_esnet.pdf',
                  id: 15243,
                  title: '20230509_CHEP_janus_data_caching_esnet.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11640,
            description:
              'Data caches of various forms have been widely deployed in the context of commercial and research and education networks, but their common positioning at the Edge limits their utility from a network operator perspective. When deployed outside the network core, providers lack visibility to make decisions or apply traffic engineering based on data access patterns and caching node location. \r\n\r\nAs an alternative, in-the-network caching provides a different type of content delivery network for scientific data infrastructure, supporting on-demand temporary caching service. It also allows providers to design data hotspots into the network topology, and to manage traffic movement and congestion by data-driven traffic engineering. There is also an opportunity for strategies around regional in-network cache placement to reduce the data access latency for the users and increase the overall computing application performance.\r\n\r\nWe will describe the status of in-network caching nodes deployed within ESnet in support of the US CMS data federation, which includes caches maintained by the University of Wisconsin-Madison, MIT, UCSD, Caltech, and ESnet. We will describe the container and networking architecture used to deploy data caches within ESnet, and update on the evolving tooling around service management lifecycle. An analysis of cache usage will also be provided along with an outlook for expanding the in-network cache footprint.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 231,
            id: 'c17108',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11640/contribution.pdf',
            presenters: [
              {
                affiliation: 'Energy Sciences Network (ESnet)',
                displayOrderKey: [1, 'Kissel, Ezra'],
                emailHash: '5c541ad89a885c6e498b57b5dad77b25',
                familyName: 'Kissel',
                firstName: 'Ezra',
                name: 'Ezra Kissel',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12170,
            sessionSlotId: 2733,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Experiences in deploying in-network data caches',
            uniqueId: 'c17108',
            url: '/event/459/contributions/11640/',
          },
          c17109: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11611/attachments/9312/13509/2023-05-09-CHEP-2023-End-to-END-SENSE.pdf',
                  id: 13509,
                  title: '2023-05-09-CHEP-2023-End-to-END-SENSE.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11611,
            description:
              'The Caltech team, in collaboration with network, computer science, and HEP partners at the DOE laboratories and universities, is building intelligent network services ("The Software-defined network for End-to-end Networked Science at Exascale (SENSE) research project") to accelerate scientific discovery.\r\nThe overarching goal of SENSE is to enable National Labs and universities to request and provision end-to-end intelligent network services for their application workflows leveraging SDN capabilities. The project\u0027s architecture, models, and demonstrated prototype define the mechanisms needed to dynamically build end-to-end virtual guaranteed networks across administrative domains, with no manual intervention from sysadmins or wide-are network engineers. In addition, a highly intuitive intent-based interface, as defined by the project, allows applications to express their high-level service requirements, and an intelligent, scalable model-based software orchestrator converts that intent into appropriate network services configured across multiple types of devices.\r\nThe overarching goal of SENSE is to enable National Labs and universities to request and provision end-to-end intelligent network services for their application workflows leveraging SDN capabilities.\r\nIn this paper, we will present the system\u0027s architecture and components, the first integration results with Scientific Collaboration tools, Quality of Service, and the next steps for better network use and utilization.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 60,
            id: 'c17109',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11611/contribution.pdf',
            presenters: [
              {
                affiliation: 'ESnet',
                displayOrderKey: [1, 'Guok, Chin'],
                emailHash: 'dde57bec332b3e9cf364f7ff1bd8af78',
                familyName: 'Guok',
                firstName: 'Chin',
                name: 'Chin Guok',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12170,
            sessionSlotId: 2733,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Complete End-to-End Network Path Control for Scientific Communities with QoS Capabilities',
            uniqueId: 'c17109',
            url: '/event/459/contributions/11611/',
          },
          c17110: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11630/attachments/9245/13981/CHEP23_P4_CarmenMisaMoreira.pdf',
                  id: 13981,
                  title: 'CHEP23_P4_CarmenMisaMoreira.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11630,
            description:
              'A comprehensive analysis of the HEP (High Energy Physics) experiment traffic across LHCONE (Large Hadron Collider Open Network Environment) and other networks, is essential for immediate network optimisation (for example by the NOTED project) and highly desirable for long-term network planning. Such an analysis requires two steps: tagging of network packets to indicate the type and owner of the traffic being carried and then processing of the tagged packets. The RNTWG (Research Network Technical Working Group) has defined a specification to identify the experiment and the application that originates a given network flow, named scitags (Scientific Network Tags) that is applied to the flow label field of the IPv6 header, this is being reported elsewhere at this conference. We report here on the second step: our processing of packets tagged according to this specification.\r\n\r\nWe developed P4flow as a software-defined networking approach by using P4 (Programming Protocol-Independent Packet Processors), a language for programming the data plane of network devices to accounting and process IPv6 packets with a scitags-based stamp in the \ufb02ow label \ufb01eld, to understand the network utilisation and the applications used by the WLCG (Worldwide Large Hadron Collider Computing Grid) sites. With P4$_{\\text{flow}}$, and exploiting the control plane capabilities provided by RARE/freeRtr (an Open Source Network Operating System developed by the G\u00c9ANT community), we can not only generate statistics concerning the traffic per experiment and application but can also, using an Intel Tofino P4-programmable ASIC Ethernet Switch, decide how to forward traffic matching defined \ufb02ow labels. This latter capability is particularly interesting as we prepare for a future where LHC experiments will be sharing network links with other major science collaborations such as, for example, SKA.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 123,
            id: 'c17110',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11630/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Misa Moreira, Carmen'],
                emailHash: 'b68cb8dc9f41e0a70e34e9eeff037b8f',
                familyName: 'Misa Moreira',
                firstName: 'Carmen',
                name: 'Carmen Misa Moreira',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12170,
            sessionSlotId: 2733,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'P4flow: A software-defined networking approach with programmable switches for accounting and forwarding IPv6 packets with user-defined flow label tags',
            uniqueId: 'c17110',
            url: '/event/459/contributions/11630/',
          },
          c17288: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11669/attachments/9273/13447/CHEP23_Talk_ApptainerWithoutSetuid.pdf',
                  id: 13447,
                  title: 'CHEP23_Talk_ApptainerWithoutSetuid.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11669,
            description:
              'Apptainer (formerly known as Singularity) since its beginning implemented many of its container features with the assistance of a setuid-root program. It still supports that mode, but as of version 1.1.0 it no longer uses setuid by default. This is feasible because it now can mount squash filesystems, mount ext2/3/4 filesystems, and use overlayfs using unprivileged user namespaces and FUSE. It also now enables unprivileged users to build containers, even without requiring system administrators to configure /etc/subuid and /etc/subgid unlike other \u201crootless\u201d container systems. As a result, all the unprivileged functions can be used nested inside of another container, even if the container runtime prevents any elevated privileges.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 23,
            id: 'c17288',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11669/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Dykstra, Dave'],
                emailHash: '5f7b6f0056d2d65a23659da9c0e4f79d',
                familyName: 'Dykstra',
                firstName: 'Dave',
                name: 'Dave Dykstra',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12170,
            sessionSlotId: 2733,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Apptainer Without Setuid',
            uniqueId: 'c17288',
            url: '/event/459/contributions/11669/',
          },
        },
        entryType: 'Session',
        friendlyId: 11,
        id: 's12170',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2041/session-timetable.pdf',
        room: 'Marriott Ballroom IV',
        sessionCode: '',
        sessionId: 2041,
        sessionSlotId: 2733,
        slotTitle: 'Networking',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#f1ffef',
        title: 'Track 7 - Facilities and Virtualization',
        uniqueId: 's12170',
        url: '/event/459/sessions/2041/',
      },
      s12171: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#e3f2d3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'DESY',
            displayOrderKey: [0, 'Hernandez Villanueva, Michel'],
            emailHash: '19113800d40b0ae945d3d902953e2b1c',
            familyName: 'Hernandez Villanueva',
            firstName: 'Michel',
            name: 'Michel Hernandez Villanueva',
          },
          {
            affiliation: 'Jefferson Lab',
            displayOrderKey: [0, 'Diefenthaler, Markus'],
            emailHash: 'af834e0ec8ce37c7bc6e53f554561c99',
            familyName: 'Diefenthaler',
            firstName: 'Markus',
            name: 'Markus Diefenthaler',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17146: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11682/attachments/9293/13485/erum-data-hub-chep23.pdf',
                  id: 13485,
                  title: 'erum-data-hub-chep23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11682,
            description:
              'Large research infrastructures, such as DESY and CERN, in the field of the exploration of the universe and matter (ErUM) are significantly driving the digital transformation of the future. The German action plan "ErUM-Data" promotes this transformation through the interdisciplinary networking and financial support of 20.000 scientists.\r\nThe ErUM-Data-Hub (https://erumdatahub.de) serves as a central networking and transfer office to meet these ambitions. One central task is the training of (prospective) scientists with schools and workshops in the areas of Big Data, Deep Learning, Sustainable Computing and many more. \r\nWe present the achievements up to the first anniversary of the ErUM-Data-Hub in the German ErUM community.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 284,
            id: 'c17146',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11682/contribution.pdf',
            presenters: [
              {
                affiliation: 'RWTH Aachen University Physics Institute III A',
                displayOrderKey: [2, 'Fackeldey, Peter'],
                emailHash: '8871f721d9afd5b6ee86e5634db7fac5',
                familyName: 'Fackeldey',
                firstName: 'Peter',
                name: 'Peter Fackeldey',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12171,
            sessionSlotId: 2734,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title:
              'The Networking and Transfer Office ErUM-Data-Hub serving Digital Transformation in Research on Universe and Matter',
            uniqueId: 'c17146',
            url: '/event/459/contributions/11682/',
          },
          c17147: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11700/attachments/9449/14049/XAS%20Curve%20Match%20Application%20based%20on%20Larch_JIANLI_LIU.pptx',
                  id: 14049,
                  title: 'XAS Curve Match Application based on Larch_JIANLI_LIU.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11700,
            description:
              'XAS (synchrotron X-ray absorption spectroscopy) uses X-ray photon energy as a variable to measure the structure of X-ray absorption coefficient that changes with energy. In spectral experiments, the determination of the composition and structure of unknown samples requires data collection first, and then data processing and analysis. It takes a lot of time and there can be no errors in the middle, which seriously restricts the development of spectral experiments. The absorption spectrum lines of the same material are approximately the same. Using this feature, the absorption spectrum and its related information of the material are stored in the database, and then the known spectrum information of the unknown sample that is close to the unknown sample is listed through the spectral matching algorithm to facilitate the processing of the spectral line station. Therefore, how to quickly and accurately match the composition of sample materials in real time is one of the focuses of spectroscopic line station scientists. In this paper, we propose a set of XAS spectroscopy matching and related processing software, which can provide an intuitive real-time interface display for scientists at the spectroscopy line station, and facilitate users to collect data while processing data. The software includes a variety of spectral matching algorithms, which can give the spectral lines most similar to the input spectral lines in the database and the detailed information of the spectral lines in the database; It integrates the normalization processing, principal component analysis algorithm, linear fitting algorithm, and extended edge processing algorithm of Larch software, enhances the interaction between users and spectral lines and displays the results, and weakens the details of data processing. Through the real-time display and convenient processing of line station data, the process of spectroscopy experiment is promoted and more scientists are attracted to participate in the spectroscopy experiment.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 474,
            id: 'c17147',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11700/contribution.pdf',
            presenters: [
              {
                affiliation: 'Institute of High Energy Physics, CAS',
                displayOrderKey: [1, 'Liu, Jianli'],
                emailHash: '9572dbe8439c01c5854eb281b9c03d68',
                familyName: 'Liu',
                firstName: 'Jianli',
                name: 'Jianli Liu',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12171,
            sessionSlotId: 2734,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'XAS Curve Match Application based on Larch',
            uniqueId: 'c17147',
            url: '/event/459/contributions/11700/',
          },
          c17148: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11705/attachments/9394/13808/rinaldi-zoomevents.pdf',
                  id: 13808,
                  title: 'rinaldi-zoomevents.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11705/attachments/9394/13807/rinaldi-zoomevents.pptx',
                  id: 13807,
                  title: 'rinaldi-zoomevents.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11705,
            description:
              'The organization of seminars and conferences was strongly influenced by the covid-19 pandemic.\r\nIn the early period of the pandemic, many events were canceled or held completely online, using video conferencing tools such as ZOOM or MS Teams. Later, thanks to large-scale vaccination and immunization, it was possible to organize again large events in the presence. Nevertheless, given some local restrictions to people temporarily affected by Covid-19, it was still necessary to provide online modalities for participants who could not participate in presence, having in fact hybrid events with both remote and in-person participation.\r\nIn this contribution we describe the experience with the ZOOM-Events platform, used for the ICHEP 2022 International Conference on High Energy Physics, held in Bologna in July 2022, with about 1100 participants in presence and 300 connected remotely. We describe in detail how the ZOOM Events platform was configured for the management of the numerous parallel sessions and the granting of access to participants and how we dealt with the problems that emerged in the organizational phases.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 503,
            id: 'c17148',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11705/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Rinaldi, Lorenzo'],
                emailHash: 'b3f279e7aaba52e6ad8f914770581cac',
                familyName: 'Rinaldi',
                firstName: 'Lorenzo',
                name: 'Lorenzo Rinaldi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12171,
            sessionSlotId: 2734,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: ': Using ZOOM Events for Scientific Conferences: the ICHEP2022 Experience',
            uniqueId: 'c17148',
            url: '/event/459/contributions/11705/',
          },
          c17149: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11697/attachments/9649/14058/CHEP2023_AI4EIC_May9.pdf',
                  id: 14058,
                  title: 'CHEP2023_AI4EIC_May9.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11697,
            description:
              'Recently, a workshop on Artificial Intelligence for the Electron Ion Collider (AI4EIC) has been held at the College of William\u0026Mary. The workshop covered all active and potential areas of applications of AI/ML for the EIC; it also had a strong outreach and educational component, with different tutorials given by experts in AI and machine learning from national labs, universities, and industry as well as a hackathon satellite event during the last day of the workshop. The format of the hackathon was hybrid and international\r\n(both local and remote participation). For this hackathon, we proposed problems with increased level of difficulty and that are deemed to be solvable in a one-day event, starting from a problem that is accessible to everyone. We focused on the dual-radiator Ring Imaging Cherenkov (dRICH) detector under development as part of the particle-identification (PID) system at the future EPIC detector at EIC. Data have been produced using the EPIC software stack. Documentation and data sets have been made available on zenodo. This experience has been highly educational, particularly for students; interestingly, it also showed the potential advantages of modern AI/ML approaches to PID for imaging Cherenkov detectors compared to traditional approaches.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 613,
            id: 'c17149',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11697/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Regina',
                displayOrderKey: [0, 'Suresh, Karthik'],
                emailHash: 'ef81b866cdfece75358ef98fc4ed79cd',
                familyName: 'Suresh',
                firstName: 'Karthik',
                name: 'Karthik Suresh',
              },
              {
                affiliation: 'William \u0026 Mary, Jefferson Lab',
                displayOrderKey: [8, 'Fanelli, Cristiano'],
                emailHash: 'aa7084c4c6bfd33cb8e030c293d332fc',
                familyName: 'Fanelli',
                firstName: 'Cristiano',
                name: 'Cristiano Fanelli',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12171,
            sessionSlotId: 2734,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'AI4EIC Hackathon: PID with the EPIC dRICH',
            uniqueId: 'c17149',
            url: '/event/459/contributions/11697/',
          },
          c17150: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11685/attachments/9665/14144/230509_hsf_training_community.pdf',
                  id: 14144,
                  title: '230509_hsf_training_community.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11685/attachments/9665/14089/230509_hsf_training_community.pptx',
                  id: 14089,
                  title: '230509_hsf_training_community.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11685,
            description:
              'To meet the computing challenges of upcoming experiments, software training efforts play an essential role in imparting best practices and popularizing new technologies. Because many of the taught skills are experiment-independent, the HSF/IRIS-HEP training group coordinates between different training initiatives while building a training center that provides students with various training modules. Both the events and the development of the training material are driven by a community of motivated educators. In this talk, we describe tools and organizational aspects with which we cultivate a strong sense of community ownership, provide recognition for individual contributions, and continue to motivate our members. We also describe new initiatives to foster further growth and increased reach. Among these is the evolution of our Training Center into a dynamic web page that allows us to significantly increase the scope of listed content without sacrificing readability.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 617,
            id: 'c17150',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11685/contribution.pdf',
            presenters: [
              {
                affiliation: 'Princeton University',
                displayOrderKey: [3, 'Lieret, Kilian'],
                emailHash: 'cb53547a5eada9b961f65f5ce163858f',
                familyName: 'Lieret',
                firstName: 'Kilian',
                name: 'Kilian Lieret',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12171,
            sessionSlotId: 2734,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Building a Global HEP Software Training Community',
            uniqueId: 'c17150',
            url: '/event/459/contributions/11685/',
          },
          c17151: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11690/attachments/9616/14098/CHEP23-vDUNE-DeMuth-v1.2.pdf',
                  id: 14098,
                  title: 'CHEP23-vDUNE-DeMuth-v1.2.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11690/attachments/9616/14106/chep23-vdune-vid1.mp4',
                  id: 14106,
                  title: 'chep23-vdune-vid1.mp4',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11690/attachments/9616/14099/chep23-vdune-vid2.mp4',
                  id: 14099,
                  title: 'chep23-vdune-vid2.mp4',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11690,
            description:
              'Deep underground, the removal of rock to fashion three soccer field\r\nsized caverns is underway, as are detector prototypings. In 2024, the\r\nfirst DUNE far detector will be constructed as a large cryostat,\r\ninstrumented as a traditional tracking calorimeter but in a cold bath of\r\nzenon doped liquidized argon. An Epic Game UnReal Engine rendered 3D\r\nsimulation of the underground laboratory has been developed from\r\nelectronic engineering drawings for the purpose of outreach to middle\r\nand high school students and to stimulate an academic interest to know\r\nmore about high energy physics to a computer savvy generation of\r\nstudents. and ideally enroll in high school and college classes that\r\nmight one day lead to becoming a HEP scientist. An overview of the\r\nproject and a virtual lab tour will be provided in this presentation.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 626,
            id: 'c17151',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11690/contribution.pdf',
            presenters: [
              {
                affiliation: 'Valley City State University',
                displayOrderKey: [1, ' DeMuth, David'],
                emailHash: '60ff649c3c719bbb5600329f38c1219b',
                familyName: ' DeMuth',
                firstName: 'David',
                name: 'David  DeMuth',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12171,
            sessionSlotId: 2734,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'vDUNE, a Virtual Tour of the DUNE South Dakota Laboratory',
            uniqueId: 'c17151',
            url: '/event/459/contributions/11690/',
          },
        },
        entryType: 'Session',
        friendlyId: 12,
        id: 's12171',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2042/session-timetable.pdf',
        room: 'Marriott Ballroom I',
        sessionCode: '',
        sessionId: 2042,
        sessionSlotId: 2734,
        slotTitle: 'Public Science Training',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#253f08',
        title: 'Track 8 - Collaboration, Reinterpretation, Outreach and Education',
        uniqueId: 's12171',
        url: '/event/459/sessions/2042/',
      },
      s12172: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#92b6db',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'VALLECORSA, SOFIA'],
            emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
            familyName: 'VALLECORSA',
            firstName: 'SOFIA',
            name: 'SOFIA VALLECORSA',
          },
          {
            affiliation: 'University of Washington (US)',
            displayOrderKey: [0, 'Schaarschmidt, Jana'],
            emailHash: '6f556891cf4ef30c552a4ac4aa1c87dc',
            familyName: 'Schaarschmidt',
            firstName: 'Jana',
            name: 'Jana Schaarschmidt',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17182: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11739/attachments/9521/14253/Kishan_CHEP23_UQ.pdf',
                  id: 14253,
                  title: 'Kishan_CHEP23_UQ.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11739/attachments/9521/13809/Kishan_CHEP23_UQ.pptx',
                  id: 13809,
                  title: 'Kishan_CHEP23_UQ.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11739,
            description:
              'Machine learning (ML) and deep learning (DL) are powerful tools for modeling complex systems. However, most of the standard models in ML/DL do not provide a measure of confidence or uncertainties associated with their predictions. Further, these models can only be trained on available data. During operation, models may encounter data samples poorly reflected in training data. These data samples are called Out-of-Distribution (OOD) samples, and the predictions on these can be arbitrarily wrong. Uncertainty Quantification is a technique that provides insight into a model\u2019s confidence in predictions, including OOD samples. \r\n\r\n \r\n\r\nGaussian Process (GP) is a well-known ML method that provides accurate estimation of prediction uncertainties. We will present our work with GP for AI-based Experimental Control to stabilize gain measurement of the Central Drift Chamber in the GlueX experiment at Jefferson Lab.  \r\n\r\n \r\n\r\nAs the number of observed data points and/or input features increases, traditional GP implementations do not scale well, and different approximation methods are applied to improve the scaling. To provide accurate uncertainty quantification for DL models, we developed and applied Deep Gaussian Process Approximation (DGPA) methods. We will discuss our work with DGPA for three different applications namely 1) uncertainty aware errant beam prediction at the Spallation Neutron Source accelerator, 2) uncertainty aware particle identification for Solenoidal Large Intensity Device experiment at Thomas Jefferson National Accelerator Facility, and 3) uncertainty aware surrogate model for the Fermi National Accelerator Laboratory booster complex.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 348,
            id: 'c17182',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11739/contribution.pdf',
            presenters: [
              {
                affiliation: 'Thomas Jefferson National Accelerator Facility',
                displayOrderKey: [1, 'Rajput, Kishansingh'],
                emailHash: '7aa6fa344b0835c6414817ba9362e86c',
                familyName: 'Rajput',
                firstName: 'Kishansingh',
                name: 'Kishansingh Rajput',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12172,
            sessionSlotId: 2735,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Uncertainty Aware Machine Learning Models for Particle Physics Applications',
            uniqueId: 'c17182',
            url: '/event/459/contributions/11739/',
          },
          c17183: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11737/attachments/9578/13899/2023-05-09_XAI4TopTagger-CHEP23.pdf',
                  id: 13899,
                  title: '2023-05-09_XAI4TopTagger-CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11737,
            description:
              'We explore interpretability of deep neural network (DNN) models designed for identifying jets coming from top quark decay in the high energy proton-proton collisions at the Large Hadron Collider (LHC). Using state-of-the-art methods of explainable AI (XAI), we identify which features play the most important roles in identifying the top jets, how and why feature importance varies across different XAI metrics, and how latent space representations encode information as well as correlate with physical quantities. We additionally illustrate the activity of hidden layers as Neural Activation Pattern (NAP) diagrams to understand how DNNs relay information across the layers and how this understanding can help us to make such models significantly simpler by allowing effective model reoptimization and hyperparameter tuning.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 581,
            id: 'c17183',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11737/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Illinois at Urbana-Champaign',
                displayOrderKey: [3, 'Roy, Avik'],
                emailHash: '00152a535f6f4d00b5aaeef45ccf672b',
                familyName: 'Roy',
                firstName: 'Avik',
                name: 'Avik Roy',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12172,
            sessionSlotId: 2735,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Exploring Interpretability of Deep Neural Networks in Top Tagging',
            uniqueId: 'c17183',
            url: '/event/459/contributions/11737/',
          },
          c17184: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11750/attachments/9603/14039/FlavourTaggingAtLHCb.pdf',
                  id: 14039,
                  title: 'FlavourTaggingAtLHCb.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11750,
            description:
              'The task of identifying B meson flavor at the primary interaction point in the LHCb detector is crucial for measurements of mixing and time-dependent CP violation.\r\n\r\nFlavor tagging is usually done with a small number of expert systems that find important tracks to infer the B flavor from.\r\n\r\nRecent advances show that replacing all of those expert systems with one ML algorithm that considers all tracks in an event yields an increase in tagging power. However, training the current classifier takes a long time and it is not suitable for use in real time triggers.\r\n\r\nIn this work we present a new classifier, based on the DeepSet architecture.\r\nWith the right inductive bias of permutation invariance, we achieve great speedups in training (multiple hours vs 10 minutes), a factor of 4-5 speed-up in inference for use in real time environments like the trigger and less tagging asymmetry.\r\n\r\nFor the first time we investigate and compare performances of these \u201cInclusive Flavor Taggers\u201d on simulation of the upgraded LHCb detector for the third run of the LHC.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 259,
            id: 'c17184',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11750/contribution.pdf',
            presenters: [
              {
                affiliation: 'Universidade de Santiago de Compostela (ES)',
                displayOrderKey: [1, 'Prouve, Claire'],
                emailHash: '5a8f152d8deda760a1643e523e6b5900',
                familyName: 'Prouve',
                firstName: 'Claire',
                name: 'Claire Prouve',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12172,
            sessionSlotId: 2735,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Fast Inclusive Flavor Tagging at LHCb',
            uniqueId: 'c17184',
            url: '/event/459/contributions/11750/',
          },
          c17185: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11730/attachments/9549/13853/DNN_Likelihood_CHEP.pdf',
                  id: 13853,
                  title: 'DNN_Likelihood_CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11730,
            description:
              'An increasingly frequent challenge faced in HEP data analysis is to characterize the agreement between a prediction that depends on a dozen or more model parameters\u2013such as predictions coming from an effective field theory (EFT) framework\u2013and the observed data. Traditionally, such characterizations take the form of a negative log likelihood (NLL) distribution, which can only be evaluated numerically. The lack of a closed-form description of the NLL function makes it difficult to convey results of the statistical analysis. Typical results are limited to extracting "best fit" values of the model parameters and 1-D intervals or 2-D contours extracted from scanning the higher dimensional parameter space. It is desirable to explore these high-dimensional model parameter spaces in more sophisticated ways. One option for overcoming this challenge is to use a neural network to approximate the NLL function. This approach has the advantage of being continuous and differentiable by construction, which are essential properties for an NLL function and may also provide useful handles in exploring the NLL as a function of the model parameters. In this talk, we describe the advantages and limitations of this approach in the context of applying it to a CMS data analysis using the framework of EFT.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 327,
            id: 'c17185',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11730/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Notre Dame',
                displayOrderKey: [0, 'Liu, Shenghua'],
                emailHash: '7f258f8a335b234b809ecda86d429f9e',
                familyName: 'Liu',
                firstName: 'Shenghua',
                name: 'Shenghua Liu',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12172,
            sessionSlotId: 2735,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Using a Neural Network to Approximate the Negative Log Likelihood Distribution',
            uniqueId: 'c17185',
            url: '/event/459/contributions/11730/',
          },
          c17186: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11747/attachments/9393/14081/CHEP%202023%20-%20Active%20Learning.pdf',
                  id: 14081,
                  title: 'CHEP 2023 - Active Learning.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11747,
            description:
              'Searches for new physics set exclusion limits in parameter spaces of typically up to 2 dimensions. However, the relevant theory parameter space is usually of a higher dimension but only a subspace is covered due to the computing time requirements of signal process simulations. An Active Learning approach is presented to address this limitation. Compared to the usual grid sampling, it reduces the number of parameter space points for which exclusion limits need to be determined. Hence it allows to extend interpretations of searches to higher dimensional parameter spaces and therefore to raise their value, e.g. via the identification of barely excluded subspaces which motivate dedicated new searches. \r\nIn an iterative procedure, a Gaussian Process is fit to excluded signal cross-sections. Within the region close to the exclusion contour predicted by the Gaussian Process, Poisson disc sampling is used to determine further parameter space points for\r\nwhich the cross-section limits are determined. The procedure is aided by a warm-start phase based on computationally inexpensive, approximate limit estimates such as total signal cross-sections. A python package, excursion [1], provides the Gaussian Process routine. The procedure is applied to a Dark Matter search performed by the ATLAS experiment, extending its interpretation from a 2 to a 4-dimensional parameter space while keeping the computational effort at a low level. \r\nThe result is published in two formats: on one hand there is a publication of the Gaussian Process model. On the other hand, a visualization of the full 4-dimensional contour is presented as a collection of 2-dimensional exclusion contours where the 2 remaining parameters are chosen by the user.\r\n[1] https://github.com/diana-hep/excursion',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 336,
            id: 'c17186',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11747/contribution.pdf',
            presenters: [
              {
                affiliation: 'New York University',
                displayOrderKey: [0, 'Bhatti, Zubair'],
                emailHash: 'b3f67115982ff08ba6dcba44e3b47b09',
                familyName: 'Bhatti',
                firstName: 'Zubair',
                name: 'Zubair Bhatti',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12172,
            sessionSlotId: 2735,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Efficient search for new physics using Active Learning in the ATLAS Experiment',
            uniqueId: 'c17186',
            url: '/event/459/contributions/11747/',
          },
          c17187: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11732/attachments/9619/13993/cINN_farkas_CHEP23.pdf',
                  id: 13993,
                  title: 'cINN_farkas_CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11732,
            description:
              'The continuous growth in model complexity in high-energy physics (HEP) collider experiments demands increasingly time-consuming model fits. We show first results on the application of conditional invertible networks (cINNs) to this challenge. Specifically, we construct and train a cINN to learn the mapping from signal strength modifiers to observables and its inverse. The resulting network infers the posterior distribution of the signal strength modifiers rapidly and for low computational cost. We present performance indicators of such a setup including the treatment of systematic uncertainties. Additionally, we highlight the features of cINNs estimating the signal strength for a vector boson associated Higgs production analysis carried out at the CMS experiment on Monte Carlo samples.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 226,
            id: 'c17187',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11732/contribution.pdf',
            presenters: [
              {
                affiliation: 'CMS',
                displayOrderKey: [1, 'Farkas, M\u00e1t\u00e9 Zolt\u00e1n'],
                emailHash: '166e121c0bd9f977dbc04688d7726d9a',
                familyName: 'Farkas',
                firstName: 'M\u00e1t\u00e9 Zolt\u00e1n',
                name: 'M\u00e1t\u00e9 Zolt\u00e1n Farkas',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12172,
            sessionSlotId: 2735,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'A method for inferring signal strength modifiers by conditional invertible neural networks',
            uniqueId: 'c17187',
            url: '/event/459/contributions/11732/',
          },
        },
        entryType: 'Session',
        friendlyId: 13,
        id: 's12172',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2043/session-timetable.pdf',
        room: 'Hampton Roads VII',
        sessionCode: '',
        sessionId: 2043,
        sessionSlotId: 2735,
        slotTitle: 'Analysis',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#03070f',
        title: 'Track 9 - Artificial Intelligence and Machine Learning',
        uniqueId: 's12172',
        url: '/event/459/sessions/2043/',
      },
      s12174: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#4f144e',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Nikhef National institute  for subatomic physics (NL)',
            displayOrderKey: [0, 'Aaij, Roel'],
            emailHash: '6fecef0aff50325ef5ea685927dec4a5',
            familyName: 'Aaij',
            firstName: 'Roel',
            name: 'Roel Aaij',
          },
          {
            affiliation: 'CSIC',
            displayOrderKey: [0, 'Campos, Isabel'],
            emailHash: '1ba624fa939fe1db5c536ca7116de83b',
            familyName: 'Campos',
            firstName: 'Isabel',
            name: 'Isabel Campos',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-09',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17235: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11839/attachments/9596/14021/quantum_co_design_NHEP.pdf',
                  id: 14021,
                  title: 'quantum_co_design_NHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11839,
            description:
              'Quantum Computing (QC) is a promising early-stage technology that offers novel approaches to simulation and analysis in nuclear and high energy physics (NHEP). By basing computations directly on quantum mechanical phenomena, speedups and other advantages for many computationally hard tasks are potentially achievable, albeit both, the theoretical underpinning and the practical realization, are still subject to considerable scientific debate, which raises the question of applicability in NHEP.\r\n\r\nIn this contribution, we describe the current state of affairs in QC: Currently available noisy, intermediate-scale quantum (NISQ) computers suffer from a very limited number of quantum bits, and are subject to considerable imperfections, which narrows their practical computational capabilities. Our recent work on optimization problems suggests that the Co-Design of quantum hardware and algorithms is one route towards practical utility. This approach offers near-term advantages throughout a variety of domains, but requires interdisciplinary exchange between communities.\r\n\r\nTo this end, we identify possible classes of applications in NHEP, ranging from quantum process simulation  over event classification directly at the quantum level to optimal real-time control of experiments. These types of algorithms are particularly suited for quantum algorithms that involve Variational Quantum Circuits, but might also benefit from more unusual special-purpose techniques like (Gaussian) Boson Sampling. We outline challenges and opportunities in the cross-domain cooperation between QC and NHEP, and show routes towards co-designed systems and algorithms. In particular, we aim at furthering the interdisciplinary exchange of ideas by establishing a joint understanding of requirements, limitations and possibilities.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 540,
            id: 'c17235',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11839/contribution.pdf',
            presenters: [
              {
                affiliation: 'Technical University of Applied Sciences Regensburg, Germany',
                displayOrderKey: [1, 'Franz, Maja'],
                emailHash: 'db2d0a8e39d81a4127fa679dcdebd678',
                familyName: 'Franz',
                firstName: 'Maja',
                name: 'Maja Franz',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12174,
            sessionSlotId: 2737,
            startDate: {
              date: '2023-05-09',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Co-Design of Quantum Hardware and Algorithms in Nuclear and High Energy Physics',
            uniqueId: 'c17235',
            url: '/event/459/contributions/11839/',
          },
          c17236: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11833/attachments/9405/13643/Pasquale_CHEP2023.pdf',
                  id: 13643,
                  title: 'Pasquale_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11833,
            description:
              'Over the last 20 years, thanks to the development of quantum technologies, it has been\r\npossible to deploy quantum algorithms and applications, that before were only\r\naccessible through simulation, on real quantum hardware. The current devices available are often refereed to as noisy intermediate-scale quantum (NISQ) computers and they require\r\ncalibration routines in order to obtain consistent results.\r\n\r\nIn this context, we present the latest developments of Qibo, an open-source framework for quantum computing.\r\nQibo was initially born as a tool for simulating quantum circuits.\r\nThrough its modular layout for backend abstraction it is possible to change effortlessly between different backends, including a high-performance simulator based on just-in-time compilation, Qibojit, which is able to simulate circuits with large number of qubits (greater than 35).\r\n\r\nThe latest additions have been Qibolab and Qibocal. The first one is a module that makes possible to employ the language developed by Qibo to execute quantum circuits on real quantum hardware, also based on different electronics. The second one is a general framework for performing calibration, characterization and randomized benchmarking protocols on all the platforms compatible with Qibolab. The advantage of these tools is that we are able to use different setups while accessing them through the same language.\r\n\r\nWe illustrate two applications of Quantum Machine Learning aimed at HEP and implemented thanks to our framework: a generative model (quantum GAN) used in the context of Monte Carlo event generation and a variational quantum circuit used to determine the content of the proton.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 513,
            id: 'c17236',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11833/contribution.pdf',
            presenters: [
              {
                affiliation:
                  'Universit\u00e0 degli Studi di Milano - INFN Sezione di Milano - Technology Innovation Institute Abu Dhabi',
                displayOrderKey: [1, 'Pasquale, Andrea'],
                emailHash: 'f048ffb85cc9fe580730afbfb8e651c8',
                familyName: 'Pasquale',
                firstName: 'Andrea',
                name: 'Andrea Pasquale',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12174,
            sessionSlotId: 2737,
            startDate: {
              date: '2023-05-09',
              time: '16:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Towards a hybrid quantum operating system',
            uniqueId: 'c17236',
            url: '/event/459/contributions/11833/',
          },
          c17238: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11836/attachments/9331/13879/20230509_CHEP_v2.pdf',
                  id: 13879,
                  title: '20230509_CHEP_v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11836,
            description:
              'In the near future, the LHC detector will deliver much more data to be processed. Therefore, new techniques are required to deal with such a large amount of data. Recent studies showed that one of the quantum computing techniques, quantum annealing (QA), can be used to perform the particle tracking with efficiency higher than 90% even in the dense environment. The algorithm starts from determining the connection between the hits, and classifies the objects with their pattern as doublet (pair of hits), triplet (three hits in a roll) or quadruplet (four hits in a roll). In order to perform the QA process, all these objects have to be constructed into a Quadratic Unconstrained Binary Optimization (QUBO) format. The current study aims to reduce the computational cost in the QA-based tracking algorithm by implementing a graph neural network (GNN) in the pre-processing stage to select input object for QUBO, and by optimizing the tightness of the selection. Moreover, the tracking performances between the standard QA-based tracking algorithm and the GNN-QA tracking algorithm are also compared.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 146,
            id: 'c17238',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11836/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'CHAN, Wai Yuen'],
                emailHash: '67cbe44897205ac3dfec59a49cad3ffe',
                familyName: 'CHAN',
                firstName: 'Wai Yuen',
                name: 'Wai Yuen CHAN',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12174,
            sessionSlotId: 2737,
            startDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Application of quantum computing techniques in particle tracking at LHC',
            uniqueId: 'c17238',
            url: '/event/459/contributions/11836/',
          },
          c17239: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11840/attachments/9443/14092/hepcloud-rigetti.pdf',
                  id: 14092,
                  title: 'hepcloud-rigetti.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11840,
            description:
              'The Superconducting Quantum Materials and Systems Center (SQMS) and the Computational Science and AI Directorate (CSAID) at Fermi National Accelerator Laboratory and Rigetti Computing have teamed up to define and deliver a standard pathway for quantum computing at Rigetti from HEPCloud.  HEPCloud now provides common infrastructure and interfacing for managing connectivity and providing access to remote quantum resources using cloud services.  HEPCloud provides the tools necessary for scheduling and running quantum applications that not only require QPU resources, but also a tight coupling with classical algorithms that utilize use QPU resources as co-processors.  With this new interface, quantum resources can be accessed through familiar job submission and monitoring grid infrastructure available at the Fermilab computing facilities.  The system incorporates AWS to handle both the application computational load and the communication link to Rigetti QPUs.  The AWS resources can also be readily used for simulators and testing before actually submission through to Rigetti.  \r\n\r\nWe have demonstrated applications ranging from optimization with QAOA, to experiments measuring qubit fidelity, to qutrit problems using the pulse package Quil-T.   Notebooks developed for interactive use can readily be used with straightforward modifications to command line processing.   Our computing model includes moving data into and out of AWS and Rigetti.   Here we describe use cases driving this development, working example applications, the overall system architecture, and the facilities that are currently used.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 628,
            id: 'c17239',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11840/contribution.pdf',
            presenters: [
              {
                affiliation: 'FNAL',
                displayOrderKey: [3, 'Timm, S.'],
                emailHash: '40848b6b99ba5ca40be682891f2cc812',
                familyName: 'Timm',
                firstName: 'S.',
                name: 'S. Timm',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12174,
            sessionSlotId: 2737,
            startDate: {
              date: '2023-05-09',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Connecting HEPCloud with quantum applications using the Rigetti platform',
            uniqueId: 'c17239',
            url: '/event/459/contributions/11840/',
          },
          c17240: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11838/attachments/9533/13824/chep23.pdf',
                  id: 13824,
                  title: 'chep23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11838,
            description:
              'The study of the decays of $B$ mesons is a key component of modern experiments which probe heavy quark mixing and $CP$ violation, and may lead to concrete deviations from the predictions of the Standard Model [1]. Flavour tagging, the process of determining the quark flavour composition of $B$ mesons created in entangled pairs at particle accelerators, is an essential component of this analysis, enabling the study of asymmetries in the decay rate of neutral $B$ mesons to flavour agnostic $CP$ eigenstates [1] and the explicit violation of $T$ symmetry at the level of fundamental interactions [2].\r\n\r\n  Flavour tagging is a difficult problem, depending in general on subtle correlations between the momenta and particle types of the many decay products emerging from the initial particle collision. Problems which require the detection of faint signals within vast quantities of data fall naturally within the domain of machine learning (ML), and indeed flavour tagging has traditionally been most readily tackled via ML [1].\r\n\r\n  Concurrently, the recent physical realisation of quantum computers has seen significant interest in the prospects of applying quantum machine learning (QML) methods to data intensive problems in particle physics [3]. In this work we employ QML for $B$ meson flavour tagging, investigating the performance of boosted ensembles of continuous variable quantum support vector machines in both the high and low entanglement regimes. We obtain results that are competitive with state-of-the-art classical methods and bode well for the performance of QML algorithms running on the large-scale quantum computers of the future.\r\n\r\n[1] Abudin\u00e9n, F., et al. The European Physical Journal C, 82, (2022)\r\n[2] Lees, J. P., et al. Physical review letters, 109, 211801  (2012)\r\n[3] Heredge, J., et al. Computing and Software for Big Science, 5, (2021)',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 375,
            id: 'c17240',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11838/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'West, Maxwell'],
                emailHash: 'b413c45f1f6e58e8faf1bd2485ebb473',
                familyName: 'West',
                firstName: 'Maxwell',
                name: 'Maxwell West',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12174,
            sessionSlotId: 2737,
            startDate: {
              date: '2023-05-09',
              time: '17:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'B Meson Flavour Tagging via Continuous Variable Quantum Support Vector Machines',
            uniqueId: 'c17240',
            url: '/event/459/contributions/11838/',
          },
          c17289: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11849/attachments/9581/13903/QAG-ValleVaro.pdf',
                  id: 13903,
                  title: 'QAG-ValleVaro.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11849,
            description:
              'The Quantum Angle Generator (QAG) constitutes a new quantum machine learning model designed to generate accurate images on current Noise Intermediate Scale (NISQ) Quantum devices. Variational quan- tum circuits constitute the core of the QAG model, and various circuit architectures are evaluated. In combination with the so-called MERA- upsampling architecture, the QAG model achieves excellent results, which are analyzed and evaluated in detail. To our knowledge, it is the first time that such accurate results are achieved by a quantum model. To explore the noise robustness of the model, an extensive quan- tum noise study is carried out. In this paper it is demonstrated, that the model trained on the quantum device learns the hardware noise behaviour and generates outstanding results with it. It is verified that even a quantum hardware machine calibration change during training of up to 8% can be well tolerated. For demonstration, the model is employed to a crucial high energy physics simulation use case. The sim- ulations are required to measure particle energies and, ultimately, to discover unknown particles at the Large Hadron Collider at CERN.',
            duration: 15.0,
            endDate: {
              date: '2023-05-09',
              time: '17:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 277,
            id: 'c17289',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11849/contribution.pdf',
            presenters: [
              {
                affiliation: 'DESY',
                displayOrderKey: [8, 'Varo, Valle'],
                emailHash: '6bc0ed9e29fb88bb08770f7f55bb0c15',
                familyName: 'Varo',
                firstName: 'Valle',
                name: 'Valle Varo',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12174,
            sessionSlotId: 2737,
            startDate: {
              date: '2023-05-09',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Precise Image Generation on Current Noisy Quantum Devices',
            uniqueId: 'c17289',
            url: '/event/459/contributions/11849/',
          },
        },
        entryType: 'Session',
        friendlyId: 15,
        id: 's12174',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2045/session-timetable.pdf',
        room: 'Marriott Ballroom VII',
        sessionCode: '',
        sessionId: 2045,
        sessionSlotId: 2737,
        slotTitle: 'Quantum Computing',
        startDate: {
          date: '2023-05-09',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffefff',
        title:
          'Track X - Exascale Science, Heterogeneous Computing and Accelerators, and Quantum Computing',
        uniqueId: 's12174',
        url: '/event/459/sessions/2045/',
      },
    },
    '20230510': {
      b12152: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 20.0,
        endDate: {
          date: '2023-05-10',
          time: '10:50:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12152',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom (3rd floor)',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-10',
          time: '10:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'AM Break',
        uniqueId: 'b12152',
      },
      b12154: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#ffec1f',
        conferenceId: 459,
        description: '',
        duration: 355.0,
        endDate: {
          date: '2023-05-10',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12154',
        inheritLoc: true,
        inheritRoom: true,
        location: 'Norfolk Waterside Marriott',
        room: '',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-10',
          time: '12:05:00',
          tz: 'US/Eastern',
        },
        textColor: '#1f1d04',
        title: 'Excursions',
        uniqueId: 'b12154',
      },
      s12136: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfdfdf',
        conferenceId: 459,
        contribDuration: 600.0,
        conveners: [],
        description: '',
        duration: 60.0,
        endDate: {
          date: '2023-05-10',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        entries: {},
        entryType: 'Session',
        friendlyId: 4,
        id: 's12136',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2034/session-timetable.pdf',
        room: '',
        sessionCode: '',
        sessionId: 2034,
        sessionSlotId: 2705,
        slotTitle: 'Registration',
        startDate: {
          date: '2023-05-10',
          time: '08:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#151515',
        title: 'Registration',
        uniqueId: 's12136',
        url: '/event/459/sessions/2034/',
      },
      s12151: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'Lund University',
            displayOrderKey: [0, 'Smirnova, Oxana'],
            emailHash: '0a4e07d041c09a2066c6fc7d96ce8971',
            familyName: 'Smirnova',
            firstName: 'Oxana',
            name: 'Oxana Smirnova',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-10',
          time: '10:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17274: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12432/attachments/9414/14116/alice_o2_gpu_eulisse_rohr.pdf',
                  id: 14116,
                  title: 'alice_o2_gpu_eulisse_rohr.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12432,
            description:
              'ALICE has upgraded many of its detectors for LHC Run 3 to operate in continuous readout mode recording Pb-Pb collisions at 50 kHz interaction rate without trigger.\r\nThis results in the need to process data in real time at rates 50 times higher than during Run 2. In order to tackle such a challenge we introduced O2, a new computing system and the associated infrastructure. Designed and implemented during the long shutdown, O2 is now in production taking care of all the data processing needs of the experiment.\r\nO2 is designed around the message passing paradigm enabling resilient, parallel data processing for both the synchronous (to LHC beam) and asynchronous data taking and processing phases.\r\nThe main purpose of the synchronous online reconstruction is detector calibration and raw data compression. This synchronous processing is dominated by the TPC detector, which produces by far the largest data volume, and TPC reconstruction is fully running on GPUs.\r\nWhen there is no beam in the LHC, the powerful GPU-equipped online computing farm of ALICE is used for the asynchronous reconstruction, which creates the final reconstruction output for analysis from the compressed raw data.\r\nSince the majority of the compute performance of the online farm is in the GPUs, and since the asynchronous processing is not dominated by the TPC in the way the synchronous processing is, there is an ongoing effort to offload a significant amount of compute load from other detectors to the GPU as well.\r\nThe talk will present the experience from running the O2 framework in production during the 2022 ALICE data taking, with particular regard to the GPU usage, an overview of the current state and the plans for the asynchronous reconstruction, and the current performance of synchronous and asynchronous reconstruction with GPUs for pp and Pb-Pb data.',
            duration: 40.0,
            endDate: {
              date: '2023-05-10',
              time: '09:40:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 640,
            id: 'c17274',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12432/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'Eulisse, Giulio'],
                emailHash: '7a6c8f49981d093b41b365a4b522378d',
                familyName: 'Eulisse',
                firstName: 'Giulio',
                name: 'Giulio Eulisse',
              },
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Rohr, David'],
                emailHash: '4837ce48c875dc1fc8c4b80ac62e9ff5',
                familyName: 'Rohr',
                firstName: 'David',
                name: 'David Rohr',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12151,
            sessionSlotId: 2716,
            startDate: {
              date: '2023-05-10',
              time: '09:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title:
              'The O2 software framework and GPU usage in ALICE online and offline reconstruction in Run 3',
            uniqueId: 'c17274',
            url: '/event/459/contributions/12432/',
          },
          c17275: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12494/attachments/9220/13570/2023.05.10.SRO_RemoteCompute.pptx',
                  id: 13570,
                  title: '2023.05.10.SRO_RemoteCompute.pptx',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/12494/attachments/9220/13383/go',
                  id: 13383,
                  title: 'Google slides',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12494,
            description:
              'Streaming Readout Data Acquisition systems coupled with distributed resources spread over vast geographic distances present new challenges to the next generation of experiments. High bandwidth modern network connectivity opens the possibility to utilize large, general-use, HTC systems that are not necessarily located close to the experiment. Near real-time response rates and workflow colocation can provide high reliability solutions to ensure efficient use of beam time. This talk will focus on a few technologies currently being developed at Jefferson Lab and in collaboration with ESnet to support fully streaming DAQ systems.',
            duration: 25.0,
            endDate: {
              date: '2023-05-10',
              time: '10:05:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 648,
            id: 'c17275',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12494/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [0, 'Lawrence, David'],
                emailHash: '762fb4d3bf38cd850e9b1a0a217d217e',
                familyName: 'Lawrence',
                firstName: 'David',
                name: 'David Lawrence',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12151,
            sessionSlotId: 2716,
            startDate: {
              date: '2023-05-10',
              time: '09:40:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Streaming Readout and Remote Compute Systems',
            uniqueId: 'c17275',
            url: '/event/459/contributions/12494/',
          },
          c17276: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12495/attachments/9451/14285/CHEP%202023_%20Data%20Management%20in%20Nuclear%20Physics.pdf',
                  id: 14285,
                  title: 'CHEP 2023_ Data Management in Nuclear Physics.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12495,
            description:
              'As nuclear physics collaborations and experiments increase in size, the data management and software practices in this community have changed as well. Large nuclear physics experiments at Brookhaven National Lab (STAR, PHENIX, sPHENIX), at Jefferson Lab (GlueX, CLAS12, MOLLER), and at the Electron-Ion Collider (ePIC) are taking different approaches to data management, building on existing frameworks or developing new solutions as necessary. In particular, where data analysis patterns are different from high energy physics, the solutions specific to nuclear physics lead to other solutions. Along with this transition to new tools, some collaborations are navigating changes from past practices in smaller efforts. I will give an overview of the approaches that are in use or planned, with a focus on common aspects.',
            duration: 25.0,
            endDate: {
              date: '2023-05-10',
              time: '10:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 649,
            id: 'c17276',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12495/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Manitoba',
                displayOrderKey: [0, 'Deconinck, Wouter'],
                emailHash: '67a661e02b659b03b53d8868b5a09e18',
                familyName: 'Deconinck',
                firstName: 'Wouter',
                name: 'Wouter Deconinck',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12151,
            sessionSlotId: 2716,
            startDate: {
              date: '2023-05-10',
              time: '10:05:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Data Management In Nuclear Physics: Cultural Changes For Larger Collaborations',
            uniqueId: 'c17276',
            url: '/event/459/contributions/12495/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's12151',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2716,
        slotTitle: 'Streaming R/O \u0026 Data Management',
        startDate: {
          date: '2023-05-10',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's12151',
        url: '/event/459/sessions/2024/',
      },
      s12153: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'University of Victoria',
            displayOrderKey: [0, 'Sobie, Randall'],
            emailHash: 'e106556bcded279b116df4aee2de6c31',
            familyName: 'Sobie',
            firstName: 'Randall',
            name: 'Randall Sobie',
          },
        ],
        description: '',
        duration: 75.0,
        endDate: {
          date: '2023-05-10',
          time: '12:05:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17277: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12496/attachments/9574/13893/DE-CompF-CHEP-May13-2023-v2.pdf',
                  id: 13893,
                  title: 'DE-CompF-CHEP-May13-2023-v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12496,
            description:
              'This presentation will cover the content of the report delivered by the Snowmass computational Frontier late in 2022. A description of the frontier organization and various preparatory events, including the Seattle Community Summer Study (CSS), will be followed by a discussion on the evolution of computing hardware and the impact of newly established and emerging technologies, including Artificial Intelligence (AI) and Quantum Computing (QC). The report findings and recommendations will be presented, including the main one on the creation of a Coordinating Panel on Software and Computing (CPSC).',
            duration: 25.0,
            endDate: {
              date: '2023-05-10',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 650,
            id: 'c17277',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12496/contribution.pdf',
            presenters: [
              {
                affiliation: 'FNAL',
                displayOrderKey: [0, 'Elvira, Daniel'],
                emailHash: '3f4418edeaa438ff1c23972fbbeb0e35',
                familyName: 'Elvira',
                firstName: 'Daniel',
                name: 'Daniel Elvira',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12153,
            sessionSlotId: 2717,
            startDate: {
              date: '2023-05-10',
              time: '10:50:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'The Future of HEP Software \u0026 Computing \u2013 The Snowmass Report',
            uniqueId: 'c17277',
            url: '/event/459/contributions/12496/',
          },
          c17278: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12497/attachments/9513/13794/CHEP2023-HaiyanGao.pdf',
                  id: 13794,
                  title: 'CHEP2023-HaiyanGao.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12497,
            description:
              'The U.S. Nuclear Physics community has been conducting long-range planning (LRP) for nuclear science since late 1970s. The process is known as the Nuclear Science Advisory Committee (NSAC) LRP with NSAC being an advisory body jointly appointed by the U.S. Department of Energy and the U.S. National Science Foundation. The last NSAC LRP was completed in 2015 and the current NSAC LRP is ongoing and the LRP report is expected in the fall of 2023. Part of the LRP process is the community driven town meetings led by the Division of Nuclear Physics of the American Physical Society. In this presentation, I will provide some highlights from these town meetings in the context of computing. Brookhaven National Laboratory is supported by the U.S. Department of Energy\u0027s Office of Science.',
            duration: 25.0,
            endDate: {
              date: '2023-05-10',
              time: '11:40:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 651,
            id: 'c17278',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12497/contribution.pdf',
            presenters: [
              {
                affiliation: 'Duke University',
                displayOrderKey: [0, 'Gao, Haiyan'],
                emailHash: '0fa1c63e119a2ecf9b3d48e8710afbe4',
                familyName: 'Gao',
                firstName: 'Haiyan',
                name: 'Haiyan Gao',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12153,
            sessionSlotId: 2717,
            startDate: {
              date: '2023-05-10',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'The Nuclear Science Long Range Plan and Computing',
            uniqueId: 'c17278',
            url: '/event/459/contributions/12497/',
          },
          c17279: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12498/attachments/9498/13766/Ramprakash_CHEP_IRI-ABA_plenary.pdf',
                  id: 13766,
                  title: 'Ramprakash_CHEP_IRI-ABA_plenary.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12498,
            description: '',
            duration: 25.0,
            endDate: {
              date: '2023-05-10',
              time: '12:05:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 652,
            id: 'c17279',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12498/contribution.pdf',
            presenters: [
              {
                affiliation: 'ANL',
                displayOrderKey: [0, 'Ramprakash, Jini'],
                emailHash: '89ca8c82838f0ffe88959d0e2a74b678',
                familyName: 'Ramprakash',
                firstName: 'Jini',
                name: 'Jini Ramprakash',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12153,
            sessionSlotId: 2717,
            startDate: {
              date: '2023-05-10',
              time: '11:40:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Integrated Research Infrastructure - Architecture Blueprint Activity (IRI-ABA)',
            uniqueId: 'c17279',
            url: '/event/459/contributions/12498/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's12153',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2717,
        slotTitle: 'Planning for the Future',
        startDate: {
          date: '2023-05-10',
          time: '10:50:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's12153',
        url: '/event/459/sessions/2024/',
      },
    },
    '20230511': {
      b12143: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 75.0,
        endDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12143',
        inheritLoc: true,
        inheritRoom: true,
        location: 'Norfolk Waterside Marriott',
        room: '',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'Lunch on Own',
        uniqueId: 'b12143',
      },
      b12144: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 30.0,
        endDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12144',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom (3rd floor)',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-11',
          time: '10:45:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'AM Break',
        uniqueId: 'b12144',
      },
      b12148: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#8ec473',
        conferenceId: 459,
        description: '',
        duration: 120.0,
        endDate: {
          date: '2023-05-11',
          time: '21:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12148',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Blue Moon Taphouse',
        room: '',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-11',
          time: '19:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#021f03',
        title: 'Closing Event',
        uniqueId: 'b12148',
      },
      c17485: {
        _fossil: 'contribSchEntryDisplay',
        _type: 'ContribSchEntry',
        attachments: {
          files: null,
          folders: [],
        },
        board_number: '',
        code: '',
        conferenceId: 459,
        contributionId: 12636,
        description: '',
        duration: 10.0,
        endDate: {
          date: '2023-05-11',
          time: '18:10:00',
          tz: 'US/Eastern',
        },
        entryType: 'Contribution',
        friendlyId: 670,
        id: 'c17485',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Norfolk Waterside Hotel',
        pdf: '/event/459/contributions/12636/contribution.pdf',
        presenters: [
          {
            affiliation: 'JLab',
            displayOrderKey: [0, 'Britton, Thomas'],
            emailHash: '821d660cde7c04e24d91f9173ee4f516',
            familyName: 'Britton',
            firstName: 'Thomas',
            name: 'Thomas Britton',
          },
          {
            affiliation: 'Argonne National Laboratory',
            displayOrderKey: [0, 'Childers, Taylor'],
            emailHash: 'f7343705e75f1b3d105ea219714bcad9',
            familyName: 'Childers',
            firstName: 'Taylor',
            name: 'Taylor Childers',
          },
        ],
        references: [],
        room: 'Norfolk Ballroom III-V',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-11',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        title: 'Student Poster Winners!',
        uniqueId: 'c17485',
        url: '/event/459/contributions/12636/',
      },
      s12114: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#5f171a',
        conferenceId: 459,
        contribDuration: 60.0,
        conveners: [
          {
            affiliation: 'Argonne National Laboratory',
            displayOrderKey: [0, 'Childers, Taylor'],
            emailHash: 'f7343705e75f1b3d105ea219714bcad9',
            familyName: 'Childers',
            firstName: 'Taylor',
            name: 'Taylor Childers',
          },
        ],
        description: '',
        duration: 60.0,
        endDate: {
          date: '2023-05-11',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        entries: {},
        entryType: 'Session',
        friendlyId: 3,
        id: 's12114',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2033/session-timetable.pdf',
        room: 'Hampton Roads Ballroom and Foyer Area',
        sessionCode: '',
        sessionId: 2033,
        sessionSlotId: 2685,
        slotTitle: 'Poster Session Thursday',
        startDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffffff',
        title: 'Poster Session',
        uniqueId: 's12114',
        url: '/event/459/sessions/2033/',
      },
      s12137: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfdfdf',
        conferenceId: 459,
        contribDuration: 600.0,
        conveners: [],
        description: '',
        duration: 60.0,
        endDate: {
          date: '2023-05-11',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        entries: {},
        entryType: 'Session',
        friendlyId: 4,
        id: 's12137',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2034/session-timetable.pdf',
        room: '',
        sessionCode: '',
        sessionId: 2034,
        sessionSlotId: 2706,
        slotTitle: 'Registration',
        startDate: {
          date: '2023-05-11',
          time: '08:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#151515',
        title: 'Registration',
        uniqueId: 's12137',
        url: '/event/459/sessions/2034/',
      },
      s12142: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'University of Washington',
            displayOrderKey: [0, 'Watts, Gordon'],
            emailHash: '53faef883b37fb4dc03d02269b5a4401',
            familyName: 'Watts',
            firstName: 'Gordon',
            name: 'Gordon Watts',
          },
        ],
        description: '',
        duration: 105.0,
        endDate: {
          date: '2023-05-11',
          time: '10:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17280: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: null,
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12499,
            description:
              'Today\u0027s students are tomorrow\u0027s leaders in science, technology, engineering and math. To ensure the best minds reach their potential tomorrow, it\u0027s vital to ensure that students not only experience meaningful STEM learning today, but also have the opportunities and support to pursue careers in a STEM environment that is more welcoming, inclusive and just. This panel will feature expertise from across Hampton Roads in building an equitable workforce from a racial justice perspective. The panelists bring experience working with pre-college students (Ivan McKinney, District Manager/Corporate Trainer, Unreasonable Kids College)  and A.K. Schultz, CEO, SVT Robotics), within a university (Dr. Aurelia T.  Williams, Vice Provost for Academic and Faculty Affairs, Norfolk State University) and within a federally funded aeronautical research center (Director Clayton Turner, NASA Langley). The panel will be moderated by Kimberly Weatherly (Assistant Dean and Director, William \u0026 Mary Center for Student Diversity).',
            duration: 45.0,
            endDate: {
              date: '2023-05-11',
              time: '09:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 653,
            id: 'c17280',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12499/contribution.pdf',
            presenters: [
              {
                affiliation: 'William \u0026 Mary',
                displayOrderKey: [0, 'Weatherly, Kimberly'],
                emailHash: null,
                familyName: 'Weatherly',
                firstName: 'Kimberly',
                name: 'Kimberly Weatherly',
              },
              {
                affiliation: 'NASA Langley',
                displayOrderKey: [1, 'Turner, Clayton'],
                emailHash: null,
                familyName: 'Turner',
                firstName: 'Clayton',
                name: 'Clayton Turner',
              },
              {
                affiliation: 'Norfolk State University',
                displayOrderKey: [2, 'Williams, Aurelia'],
                emailHash: null,
                familyName: 'Williams',
                firstName: 'Aurelia',
                name: 'Aurelia Williams',
              },
              {
                affiliation: 'Unreasonable Kids College',
                displayOrderKey: [3, 'McKinney, Ivan'],
                emailHash: null,
                familyName: 'McKinney',
                firstName: 'Ivan',
                name: 'Ivan McKinney',
              },
              {
                affiliation: 'SVT Robotics',
                displayOrderKey: [4, 'Schultz, A.K.'],
                emailHash: null,
                familyName: 'Schultz',
                firstName: 'A.K.',
                name: 'A.K. Schultz',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12142,
            sessionSlotId: 2710,
            startDate: {
              date: '2023-05-11',
              time: '09:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'DEI Roundtable: "Building an equitable STEM workforce for the future"',
            uniqueId: 'c17280',
            url: '/event/459/contributions/12499/',
          },
          c17281: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11499/attachments/9236/14205/WLCGEnergyNeedsCHEP2023.pdf',
                  id: 14205,
                  title: 'WLCGEnergyNeedsCHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 11499,
            description:
              'The WLCG infrastructure provides the compute power and the storage capacity for the computing needs of the experiments at the Large Hadron Collider (LHC) at CERN. The infrastructure is distributed across over 170 data centers in more than 40 countries. The amount of consumed energy in WLCG to support the scientific program of the LHC experiments and its evolution depends on different factors: the luminosity of the LHC and its operating conditions, determining the data volume and the data complexity; the evolution of the computing models and the offline software of the experiments, considering the ongoing R\u0026D program in preparation for the next LHC phase (HL-LHC); the computing hardware technology evolution in the direction of higher energy efficiency; the modernization of the facilities hosting the data centers, improving the Power Usage Efficiency. This contribution presents a study of the WLCG energy needs and their evolution for the continuation of the LHC program, based on the factors mentioned above. Some information is obtained from the CERN experience but can be extrapolated to the whole of WLCG. The study provides therefore a holistic view for the infrastructure rather than a detailed prediction at the level of the single facilities. It presents an accurate view of the trends and offers a model for more refined studies.',
            duration: 30.0,
            endDate: {
              date: '2023-05-11',
              time: '10:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 85,
            id: 'c17281',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11499/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Campana, Simone'],
                emailHash: 'a2dbd3d2df26e3fbc0360f6b3594fd21',
                familyName: 'Campana',
                firstName: 'Simone',
                name: 'Simone Campana',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12142,
            sessionSlotId: 2710,
            startDate: {
              date: '2023-05-11',
              time: '10:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'A holistic study of the WLCG energy needs for the LHC scientific program',
            uniqueId: 'c17281',
            url: '/event/459/contributions/11499/',
          },
          c17282: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12500/attachments/9674/14112/Diversity%20Equity%20Inclusion%20Accessibility%20and%20Particle%20Physics.pdf',
                  id: 14112,
                  title: 'Diversity Equity Inclusion Accessibility and Particle Physics.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12500,
            description:
              'Big science is represented by projects like those in particle physics.  Big engineering is the application of engineering principles to large-scale projects that have a significant impact on society, like popular use of AI/ML (think ChatGPT and Google Bard).  Both big science and big engineering are among the noblest and boldest applications of the human intellect to understanding the universe and our place in it.  Both depend on human collaboration to generate the ingenuity needed to make their impacts positive ones.  Both are marred by evidence of bias, particularly racial bias, that lessens intellectual excellence.  LEIDA - Leadership on Equity, Inclusion, Diversity, and Access is needed to ensure that opportunities lost in the past due to marginalization of particular communities eventually ends and the full breadth of creativity and innovation possible determines the future of our field.',
            duration: 30.0,
            endDate: {
              date: '2023-05-11',
              time: '10:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 654,
            id: 'c17282',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12500/contribution.pdf',
            presenters: [
              {
                affiliation: 'Yale',
                displayOrderKey: [0, 'Gladney, Larry'],
                emailHash: 'f6fae1427c227742e5f14be81e6609a9',
                familyName: 'Gladney',
                firstName: 'Larry',
                name: 'Larry Gladney',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12142,
            sessionSlotId: 2710,
            startDate: {
              date: '2023-05-11',
              time: '09:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title:
              'LEIDA - Ideas for Achieving Excellence in HEP Research By Minimizing Marginalization and Increasing Belonging',
            uniqueId: 'c17282',
            url: '/event/459/contributions/12500/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's12142',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2710,
        slotTitle: 'Diversity \u0026 Bias / Inclusive and Ethical Computing',
        startDate: {
          date: '2023-05-11',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's12142',
        url: '/event/459/sessions/2024/',
      },
      s12145: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#6f390d',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'FNAL',
            displayOrderKey: [0, 'Kirby, Michael'],
            emailHash: '384f25285a1799bb8e2b4ea136bfd945',
            familyName: 'Kirby',
            firstName: 'Michael',
            name: 'Michael Kirby',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Barisits, Martin'],
            emailHash: '46207806844c107a8fefe566f669b206',
            familyName: 'Barisits',
            firstName: 'Martin',
            name: 'Martin Barisits',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16982: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11311/attachments/9651/14229/20230511-NordicDataLakesSuccessStory.pdf',
                  id: 14229,
                  title: '20230511-NordicDataLakesSuccessStory.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11311,
            description:
              'The Data Lake concept has promised increased value to science and more efficient operations for storage compared to the traditional isolated storage deployments. Building on the established distributed dCache serving as the Nordic Tier-1 storage for LHC data, we have also integrated tier-2 pledged storage in Slovenia, Sweden, and Switzerland, resulting in a coherent storage space well above the expected for the funding committed to the Nordic Tier-1 site.\r\n\r\nWe have implemented this with an innovative automated deployment of dCache that scales well over many sites in a distributed federation. This setup optimizes for minimal local site effort, leading to a storage service that delivers increased value to scientists while at the same time reducing the cost of operations.\r\n\r\nARC with caching is used for computing in order to increase performance by up to 50% when reading from geographically distributed storage, as well as reduce bandwidth use.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 322,
            id: 'c16982',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11311/contribution.pdf',
            presenters: [
              {
                affiliation: 'NeIC',
                displayOrderKey: [1, 'Wadenstein, Mattias'],
                emailHash: '72041873a0e7ca5346b0b85105728bae',
                familyName: 'Wadenstein',
                firstName: 'Mattias',
                name: 'Mattias Wadenstein',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12145,
            sessionSlotId: 2711,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Nordic Data Lake Success Story',
            uniqueId: 'c16982',
            url: '/event/459/contributions/11311/',
          },
          c16983: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11292/attachments/9640/14190/The_Implementation_of_DOMAS_V4.pdf',
                  id: 14190,
                  title: 'The_Implementation_of_DOMAS_V4.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11292,
            description:
              'China\u2019s High Energy Photon Source (HEPS), the first national high-energy synchrotron radiation light source and soon one of the world\u2019s brightest fourth-generation synchrotron radiation facilities, is being under intense construction in Beijing\u2019s Huairou District, and will be completed in 2025.\r\n\r\nTo make sure that the huge amount of data collected at HEPS is accurate, available and accessible, we developed an effective data management system that is aimed at automating the organization, transfer, storage, distribution and sharing of the data produced from HEPS experiments. First, the general situation of HEPS and the construction progress of the whole project are introduced. Second, the architecture and data flow of the HEPS DMS are described. Third, key techniques and new function modules implemented in this system are introduced. For example, the process of automatic data tracking when using a hierarchical storage policy is illustrated, and how the DMS deals with the metadata collection when an emergency occurs such as beamline network interruption. Finally, the progress and the effect of the data management and data service system deployed at testbed beamlines of BSRF are given.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 151,
            id: 'c16983',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11292/contribution.pdf',
            presenters: [
              {
                affiliation: 'Institute of High Energy Physics, CAS',
                displayOrderKey: [0, 'Sun, Haokai'],
                emailHash: '6386dfaefc2a2e75fb9b1ae2db8f8588',
                familyName: 'Sun',
                firstName: 'Haokai',
                name: 'Haokai Sun',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12145,
            sessionSlotId: 2711,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'The implementation of Data Management and Data Service for HEPS',
            uniqueId: 'c16983',
            url: '/event/459/contributions/11292/',
          },
          c16984: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11328/attachments/9542/13840/Rubin%20Distributed%20Processing%20-%20CHEP2023%20v1.pdf',
                  id: 13840,
                  title: 'Rubin Distributed Processing - CHEP2023 v1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11328,
            description:
              'The [Vera C. Rubin observatory][1] is preparing for execution of the most ambitious astronomical survey ever attempted, the Legacy Survey of Space and Time (LSST). Currently in its final phase of construction in the Andes mountains in Chile and due to start operations late 2024 for 10 years, its 8.4-meter telescope will nightly scan the southern sky and collect images of the entire visible sky every 4 nights using a 3.2 Gigapixel camera, the largest imaging device ever built for astronomy. Automated detection and classification of celestial objects will be performed by sophisticated algorithms on high-resolution images to progressively produce an astronomical catalog eventually composed of 20 billion galaxies and 17 billion stars and their associated physical properties.\r\n\r\nIn this contribution we will present an overall view of the system currently in construction to perform data distribution and the annual reprocessing campaigns of the entire image dataset collected since the beginning of the survey, using computing and storage resources provided by 3 Rubin data facilities (one in the US and two in Europe). Each year a data release will be produced and delivered to science collaborations for their studies in 4 science areas: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky and mapping the Milky Way.\r\n\r\nWe will present how we leverage some of the practices and tools used for large-scale distributed data processing by other projects in the high energy physics and astronomy communities and how we integrate them with tools developed by the Rubin project for meeting the specific challenges it faces.\r\n\r\n  [1]: https://www.lsst.org',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 397,
            id: 'c16984',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11328/contribution.pdf',
            presenters: [
              {
                affiliation: 'IN2P3 / CNRS computing center',
                displayOrderKey: [1, 'Hernandez, Fabio'],
                emailHash: '4c50bba9c5558ffddf6157ef5a6ad638',
                familyName: 'Hernandez',
                firstName: 'Fabio',
                name: 'Fabio Hernandez',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12145,
            sessionSlotId: 2711,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title:
              'Overview of the distributed image processing infrastructure to produce the Legacy Survey of Space and Time (LSST)',
            uniqueId: 'c16984',
            url: '/event/459/contributions/11328/',
          },
          c16985: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11299/attachments/9621/14192/CHEP%20EPN2EOS%20Data%20Transfer%20System%20-%20May%20-%202023.pdf',
                  id: 14192,
                  title: 'CHEP EPN2EOS Data Transfer System - May - 2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11299,
            description:
              'ALICE is one of the four large experiments at the CERN LHC designed to study the structure and origins of matter in collisions of heavy ions (and protons) at ultra-relativistic energies. The experiment measures the particles produced as a result of collisions in its center so that it can reconstruct and study the evolution of the system produced during these collisions. To perform these measurements, many different sub-detectors combined to form the experimental apparatus, each providing specific information. The ALICE Collaboration is composed of 2,000 members from over 175 physics institutes in 39 countries. Besides, numerous computing resources are available to researchers to collect, process, and analyze data gathered from experiments.\r\n\r\nThe ALICE experiment at CERN started its LHC Run 3 in 2022 with an upgraded detector and an entirely new data acquisition system, capable of collecting 100 times more events than the previous setup. One of the key elements of the new DAQ is the Event Processing Nodes (EPN) farm, which currently comprises 250 servers, each equipped with 8 MI50 ATI GPU accelerators. The role of the EPN is to make a lossy compression of the detector data from approximately 600GB/s to 100GB/s during the heavy-ion data taking period. The 100GB/s stream is written to an 80PB EOS disk buffer for further offline processing. The EPNs handle data streams, called Time Frames, of 10ms duration from the detector independently from each other and write the output, called Compressed Time Frames (CTF), to a local disk. The CTFs must be removed from the buffer as soon as the compression is completed to free the local disk for the next data. In addition to the CTFs, the EPNs process calibration data, which is also written to the local node storage and must be transferred to persistent storage rapidly. The data transfer functions are done by the EPN2EOS system, which in addition to the data copy also registers the CTFs and calibration data in the ALICE Grid catalogue. EPN2EOS is highly optimized to perform the copy and registration functions outside of the EPN data compression times, it is also capable to redirect data streams to an alternative storage system in case of network interruptions or unavailability of the primary storage and has extensive monitoring and messaging system to present the ALICE operations with real-time alerts in case of problems. The system has been in production since November 2021 and in this paper, we describe its architecture, implementation, and analysis of its first year of utilization.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 205,
            id: 'c16985',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11299/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Suiu, Alice Florenta'],
                emailHash: 'b3bf2665bfd28f58487dad9c8f302916',
                familyName: 'Suiu',
                firstName: 'Alice Florenta',
                name: 'Ms Alice Florenta Suiu',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12145,
            sessionSlotId: 2711,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'EPN2EOS Data Transfer System',
            uniqueId: 'c16985',
            url: '/event/459/contributions/11299/',
          },
          c16986: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11313/attachments/9192/13487/CHEP_2023_FTS_Service_Evolution_and_LHC_Run_3_Operations.pdf',
                  id: 13487,
                  title: 'CHEP_2023_FTS_Service_Evolution_and_LHC_Run_3_Operations.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11313,
            description:
              'The File Transfer System (FTS) is a software system responsible for queuing, scheduling, dispatching and retrying file transfer requests, it is used by three of the LHC experiments, namely ATLAS, CMS and LHCb, as well as non LHC experiments including AMS, Dune and NA62.  FTS is critical to the success of many experiments and the service must remain available and performant during the entire LHC Run-3. Experiments use FTS to manage the transfer of their Physics files all over the World or more specifically all over the Worldwide LHC Computing Grid (WLCG).  Since the start of LHC Run-3 (from 5th July 2022 to 5th November 2022), FTS has managed the successful transfer of approximately 400 million files totalling 517 Petabytes of data. \r\n\r\nThis paper describes how the FTS team has evolved the software and the deployment in order to cope with changes in implementation technologies, increase the efficiency of service, streamline its operations, and to meet the ever changing needs of its user community.  We report about the software migration from Python 2 to Python 3, the move from the Pylons web development framework toward Flask and the new database deployment strategy to separate the handling of the critical operations from the long duration monitoring queries. In addition, during 2022 a new HTTP based protocol has been finalised that can now be used between FTS and compatible WLCG tape storage endpoints.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 216,
            id: 'c16986',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11313/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Murray, Steven'],
                emailHash: '060cad0e10f381cb4dad9bd98b9f82b5',
                familyName: 'Murray',
                firstName: 'Steven',
                name: 'Steven Murray',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12145,
            sessionSlotId: 2711,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'FTS Service Evolution and LHC Run-3 Operations',
            uniqueId: 'c16986',
            url: '/event/459/contributions/11313/',
          },
          c16987: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11322/attachments/9666/14216/Integrating_FTS_in_the_Fenix_HPC_Infrastructure.pdf',
                  id: 14216,
                  title: 'Integrating_FTS_in_the_Fenix_HPC_Infrastructure.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11322,
            description:
              'HPC systems are increasingly often used for addressing various challenges in high-energy physics. But often the data infrastructures used in the latter area are not well integrated with infrastructures that include HPC resources. Here we will focus on a specific infrastructure, namely Fenix, which is based on a consortium of 6 leading European supercomputing centres. The Fenix sites are integrated into a common AAI and provide a so-called Archival Data Repository that can be accessed through a Swift API. Fenix was initiated through the Human Brain Project (HBP) but also provides resources to other research communities in Europe.\r\n\r\nIn this talk, we will report on our efforts to enable the support of Swift in FTS3 and its dependencies GFAL2 and DaviX. We will, in particular, discuss how FTS3 has been integrated into the Fenix AAI, which largely follows the architectural principles of the European Open Science Cloud (EOSC). Furthermore, we show how end-users can use this service through a WebFTS service that has been integrated into the science gateway of the HBP, which is also known as the HBP Collaboratory. Finally, we discuss how transfer commands are automatically distributed over several FTS3 instances to optimise transfer between different Fenix sites.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 34,
            id: 'c16987',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11322/contribution.pdf',
            presenters: [
              {
                affiliation: 'Forschungszentrum J\u00fclich',
                displayOrderKey: [1, 'Long, Shiting'],
                emailHash: '8abc4cd1a5a23a61c951da65c0bc509a',
                familyName: 'Long',
                firstName: 'Shiting',
                name: 'Ms Shiting Long',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12145,
            sessionSlotId: 2711,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Integrating FTS in the Fenix HPC infrastructure',
            uniqueId: 'c16987',
            url: '/event/459/contributions/11322/',
          },
        },
        entryType: 'Session',
        friendlyId: 5,
        id: 's12145',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2035/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2035,
        sessionSlotId: 2711,
        slotTitle: 'Data Management',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffeddf',
        title: 'Track 1 - Data and Metadata Organization, Management and Access',
        uniqueId: 's12145',
        url: '/event/459/sessions/2035/',
      },
      s12146: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#6f390d',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'FNAL',
            displayOrderKey: [0, 'Kirby, Michael'],
            emailHash: '384f25285a1799bb8e2b4ea136bfd945',
            familyName: 'Kirby',
            firstName: 'Michael',
            name: 'Michael Kirby',
          },
          {
            affiliation: 'University of California, San Diego',
            displayOrderKey: [0, 'Davila, Diego'],
            emailHash: 'be09e73b4230a27dffdb0d8305b8a1d0',
            familyName: 'Davila',
            firstName: 'Diego',
            name: 'Diego Davila',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16988: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11319/attachments/9678/14127/CHEP2023-QiulanHuang.pdf',
                  id: 14127,
                  title: 'CHEP2023-QiulanHuang.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11319,
            description:
              'The rapid growth of scientific data and the computational needs of BNL-supported science programs will bring the Scientific Data and Computing Center (SDCC) to the Exabyte scale in the next few years. The SDCC Storage team is responsible for the symbiotic development and operations of storage services for all BNL experiment data, in particular for  the data generated by the ATLAS experiment with the largest amount of data. While the steady increase in ATLAS needs for  DISK storage capacity, the cost issue to continue with more than one DISK copies and the updated ATLAS storage environment have brought new challenges to SDCC. In order to overcome the challenges arising from the vast amount of data while enabling efficient and cost-effective data analysis in a large-scale, multi-tiered storage architecture, the Storage team has undertaken a thorough analysis of the ATLAS experiment\u2019s requirements, matching them to the appropriate storage options and strategy, and has explored alternatives to complement/replace our current storage solution. In this paper, we present the main challenges presented by supporting several big data experiments like ATLAS. We describe its requirements and priorities, in particular, what critical storage system characteristics are needed for the high-luminosity run and how the key storage components provided by the Storage team work together: the dCache disk storage system; its archival back-end, HPSS, and its OS-level backend Storage. In particular, we investigate a new solution to integrate Lustre and XRootd. Lustre serves as backend storage and XRootd acts as an access layer frontend to support different grid access protocols. We also describe the validation, commissioning tests, and a comparison between dCache and XRootd in performance. In addition, the performance and cost comparison of OpenZFS and LINUX MDRAID, the evaluation of storage software stacks, and the stress tests to validate Third Party Copy(TPC) will be illustrated.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 191,
            id: 'c16988',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11319/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [0, 'Huang, Qiulan'],
                emailHash: '4a3e0f2c4baae2b0f0b985cb9e0e367a',
                familyName: 'Huang',
                firstName: 'Qiulan',
                name: 'Qiulan Huang',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12146,
            sessionSlotId: 2712,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Exploring Future Storage Options for ATLAS at the BNL/SDCC facility',
            uniqueId: 'c16988',
            url: '/event/459/contributions/11319/',
          },
          c16989: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11302/attachments/9693/14243/Data%20popularity%20for%20the%20Cache%20Eviction%20Algorithms%20using%20Random%20Forests.pdf',
                  id: 14243,
                  title: 'Data popularity for the Cache Eviction Algorithms using Random Forests.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11302/attachments/9693/14244/Data%20popularity%20for%20the%20Cache%20Eviction%20Algorithms%20using%20Random%20Forests.pptx',
                  id: 14244,
                  title:
                    'Data popularity for the Cache Eviction Algorithms using Random Forests.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11302,
            description:
              'In the HEP community, the prediction of Data Popularity is a topic that has been approached for many years. Nonetheless, while facing increasing data storage challenges, especially in the HL-LHC era, we are still in need for better predictive models to answer the questions of whether particular data should be kept, replicated, or deleted.\r\n\r\nThe usage of caches proved to be a convenient technique that partially automates storage management and seems to eliminate some of these questions. While on one hand, we can benefit even from simple caching algorithms like LRU, on the other hand, we show that incorporation of the knowledge about the future access patterns can greatly improve the cache performance. \r\n\r\nIn this paper, we study the data popularity on the file level, where the special relation between files belonging to the same dataset could be used in addition to the standard attributes. We start by analyzing separate features and try to find the relation with the target variable: the reuse distance of the files. After, we turn to Machine Learning algorithms, such as Random Forest, which is well suited to work with Big Data: it can be parallelized, is more lightweight and easier to interpret than Deep Neural Networks. Finally, we compare the results with standard cache retention algorithms and with the theoretical optimum.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 112,
            id: 'c16989',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11302/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN, University of Cote d\u0027Azur',
                displayOrderKey: [1, 'Chuchuk, Olga'],
                emailHash: '4d9fcd18b275b12d46156dda452e66e1',
                familyName: 'Chuchuk',
                firstName: 'Olga',
                name: 'Olga Chuchuk',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12146,
            sessionSlotId: 2712,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Data popularity for the Cache Eviction Algorithms using Random Forests',
            uniqueId: 'c16989',
            url: '/event/459/contributions/11302/',
          },
          c16990: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11331/attachments/9691/14138/CHEP-NewXRootDMonitoring.pdf',
                  id: 14138,
                  title: 'CHEP-NewXRootDMonitoring.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11331,
            description:
              'Complete and reliable monitoring of the WLCG data transfers is an important condition for effective computing operations of the LHC experiments. WLCG data challenges organised in 2021 and 2022 highlighted the need for improvements in the monitoring of data traffic on the WLCG infrastructure. In particular, it concerns the implementation of the monitoring of the remote data access via the XrootD protocol. This contribution describes the new implementation of the XrootD monitoring flow, the overall architecture, the deployment scenario and the integration with the WLCG global monitoring system.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 179,
            id: 'c16990',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11331/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [4, 'Garrido Bear, Borja'],
                emailHash: 'c06a190677dc607e5b2240759236f4e4',
                familyName: 'Garrido Bear',
                firstName: 'Borja',
                name: 'Mr Borja Garrido Bear',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12146,
            sessionSlotId: 2712,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'New XrootD monitoring implementation',
            uniqueId: 'c16990',
            url: '/event/459/contributions/11331/',
          },
          c16991: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11303/attachments/9681/14120/400gbps_benchmark.pdf',
                  id: 14120,
                  title: '400gbps_benchmark.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11303,
            description:
              'Due to the increased demand of network traffic expected during the HL-LHC era, the T2 sites in the USA will be required to have 400Gbps of available bandwidth to their storage solution.\u00a0\r\nWith the above in mind we are pursuing a scale test of XRootD software when used to perform Third Party Copy transfers using the HTTP protocol. Our main objective is to understand the possible limitations in the software stack to achieve the target transfer rate;\u00a0 to that end we have set up a testbed of multiple XRootD servers in both UCSD and Caltech which are connected through a dedicated link capable of 400Gbps end-to-end.\r\nBuilding upon our experience deploying containerized XRootD servers we use docker and kubernetes to easily deploy and test different configurations of our testbed.\u00a0\r\nIn this work we will present our experience doing these tests and the lessons learned.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 614,
            id: 'c16991',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11303/contribution.pdf',
            presenters: [
              {
                affiliation: 'UCSD',
                displayOrderKey: [1, 'Arora, Aashay'],
                emailHash: '856e91e7e872cd93ae46629c5f069ec3',
                familyName: 'Arora',
                firstName: 'Aashay',
                name: 'Aashay Arora',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12146,
            sessionSlotId: 2712,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: '400Gbps benchmark of XRootD HTTP-TPC',
            uniqueId: 'c16991',
            url: '/event/459/contributions/11303/',
          },
          c16992: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11298/attachments/9462/13717/CHEP2023_scale_tests_of_dune_data_pipeline.pdf',
                  id: 13717,
                  title: 'CHEP2023_scale_tests_of_dune_data_pipeline.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11298/attachments/9462/13718/CHEP2023_scale_tests_of_dune_data_pipeline.pptx',
                  id: 13718,
                  title: 'CHEP2023_scale_tests_of_dune_data_pipeline.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11298,
            description:
              'In preparation for the second runs of the ProtoDUNE detectors at CERN (NP02 and NP04), DUNE has established a new data pipeline for bringing the data from the EHN-1 experimental hall at CERN to primary tape storage at Fermilab and CERN, and then spreading it out to a distributed disk data store at many locations around the world.  This system includes a new Ingest Daemon and a new Declaration Daemon.  The Rucio replica catalog, and FTS3 transport are used to transport all files.  All file metadata is declared to the new MetaCat metadata service.  All of these new components have been successfully\r\n tested at a scale equal to the expected output of the detector data acquisition system (~2-4 GB/s), and the expected network bandwidth out of the experimental hall.  We present the procedure that was used to test and the results of the test.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 168,
            id: 'c16992',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11298/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermi National Accelerator Laboratory',
                displayOrderKey: [0, 'Timm, Steven'],
                emailHash: '40848b6b99ba5ca40be682891f2cc812',
                familyName: 'Timm',
                firstName: 'Steven',
                name: 'Steven Timm',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12146,
            sessionSlotId: 2712,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'Scale tests of the new DUNE data pipeline',
            uniqueId: 'c16992',
            url: '/event/459/contributions/11298/',
          },
          c16993: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11290/attachments/9502/14241/CHEP23.pdf',
                  id: 14241,
                  title: 'CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#6f390d',
            conferenceId: 459,
            contributionId: 11290,
            description:
              'The LArSoft/art framework is used at Fermilab\u2019s liquid argon time projection chamber experiments such as ICARUS to run traditional production workflows in a grid environment.  It has become increasingly important to utilize HPC facilities for experimental data processing tasks.  As part of the SciDAC-4 HEP Data Analytics on HPC and HEP Event Reconstruction with Cutting Edge Computing Architectures projects, we have been exploring ways to restructure HEP neutrino workflows to increase resource utilization when running at HPC facilities.   Our explorations focus on taking advantage of distinct architectural features for data services, parallel application scheduling, and high CPU core counts available at these facilities.   In this paper, we introduce changes needed to make use of a new system-wide event store called HEPnOS and efforts to maximize the throughput of newly available multicore algorithms with the available memory on the compute nodes.  Performance results are shown for ALCF Theta using the early signal processing steps within the ICARUS production workflow.\r\n\r\nHEPnOS is a HEP-specific distributed data store built on top of software components from the DOE-ASCR supported Mochi project.  With facility-wide access to HEP event data, we can avoid processing constraints and bottlenecks present in file-based reconstruction workflows.  Data stores such as HEPnOS leverage the high performance networks and memories available on HPC systems, and can help eliminate performance bottlenecks and issues that may appear when using parallel file systems.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 627,
            id: 'c16993',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11290/contribution.pdf',
            presenters: [
              {
                affiliation: 'FNAL',
                displayOrderKey: [7, 'Syed, S.'],
                emailHash: 'f264f9aceaf2afefa5b2ba8257cc3ac3',
                familyName: 'Syed',
                firstName: 'S.',
                name: 'S. Syed',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2035,
            sessionSlotEntryId: 12146,
            sessionSlotId: 2712,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffeddf',
            title: 'ICARUS signal processing with HEPnOS',
            uniqueId: 'c16993',
            url: '/event/459/contributions/11290/',
          },
        },
        entryType: 'Session',
        friendlyId: 5,
        id: 's12146',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2035/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2035,
        sessionSlotId: 2712,
        slotTitle: 'Analytics \u0026 Benchmarks',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffeddf',
        title: 'Track 1 - Data and Metadata Organization, Management and Access',
        uniqueId: 's12146',
        url: '/event/459/sessions/2035/',
      },
      s12147: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'BNL',
            displayOrderKey: [0, 'Wenaus, Torre'],
            emailHash: '849637192af92a0f322682b2abc1e859',
            familyName: 'Wenaus',
            firstName: 'Torre',
            name: 'Torre Wenaus',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '18:00:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17283: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12501/attachments/9704/14261/ME-Monzani-CHEP-11May23.pdf',
                  id: 14261,
                  title: 'ME-Monzani-CHEP-11May23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12501,
            description:
              'The nature and origin of dark matter are among the most compelling mysteries of contemporary science. There is strong evidence for dark matter from its role in shaping the galaxies and galaxy clusters that we observe in the universe. Still, physicists have tried to detect dark matter particles for over three decades with little success.\r\n\r\nThis talk will describe the leading effort in that search, the LUX-ZEPLIN (LZ) detector. LZ is an instrument that is superlative in many ways. It consists of 10 tons of liquified xenon gas, maintained at almost atomic purity and stored in a refrigerated titanium cylinder a mile underground in a former gold mine in Lead, South Dakota.',
            duration: 30.0,
            endDate: {
              date: '2023-05-11',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 655,
            id: 'c17283',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12501/contribution.pdf',
            presenters: [
              {
                affiliation: 'SLAC',
                displayOrderKey: [0, 'Monzani, Maria Elena'],
                emailHash: 'c310d208af05e337c3c929ad3eddc0c6',
                familyName: 'Monzani',
                firstName: 'Maria Elena',
                name: 'Maria Elena Monzani',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12147,
            sessionSlotId: 2713,
            startDate: {
              date: '2023-05-11',
              time: '16:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Data-Intensive search for Dark Matter with LUX-ZEPLIN',
            uniqueId: 'c17283',
            url: '/event/459/contributions/12501/',
          },
          c17284: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/12502/attachments/9617/13990/go',
                  id: 13990,
                  title: 'Massive-scale Data Analytics at the Linac Coherent Light Source',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12502/attachments/9617/14255/Thayer-LCLS-CHEP%202023-nomovie.pdf',
                  id: 14255,
                  title: 'Thayer-LCLS-CHEP 2023-nomovie.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12502/attachments/9617/14162/Thayer-LCLS-CHEP%202023.pptx',
                  id: 14162,
                  title: 'Thayer-LCLS-CHEP 2023.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12502,
            description:
              'The increasing volumes of data produced at light sources such as the Linac Coherent Light Source (LCLS) enable the direct observation of materials and molecular assemblies at the length and timescales of molecular and atomic motion. This exponential increase in the scale and speed of data production is prohibitive to traditional analysis workflows that rely on scientists tuning parameters during live experiments to adapt data collection and analysis. User facilities will increasingly rely on the automated delivery of actionable information in real time for rapid experiment adaptation which presents a considerable challenge for data acquisition, data processing, data management, and workflow orchestration. In addition, the desire from researchers to accelerate science requires rapid analysis, dynamic integration of experiment and theory, the ability to visualize results in near real-time, and the introduction of ML and AI techniques.  We present the LCLS-II Data System architecture which is designed to address these challenges via an adaptable data reduction pipeline (DRP) to reduce data volume on-the-fly, online monitoring analysis software for real-time data visualization and experiment feedback, and the ability to scale to computing needs by utilizing local and remote compute resources, such as the ASCR Leadership Class Facilities, to enable quasi-real-time data analysis in minutes.  We discuss the overall challenges facing LCLS, our ongoing work to develop a system responsive to these challenges, and our vision for future developments.',
            duration: 30.0,
            endDate: {
              date: '2023-05-11',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 656,
            id: 'c17284',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12502/contribution.pdf',
            presenters: [
              {
                affiliation: 'SLAC',
                displayOrderKey: [0, 'Thayer, Jana'],
                emailHash: '8fcd87cd88ada58a12a027c0a0228693',
                familyName: 'Thayer',
                firstName: 'Jana',
                name: 'Jana Thayer',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12147,
            sessionSlotId: 2713,
            startDate: {
              date: '2023-05-11',
              time: '17:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Massive Scale Data Analytics at LCLS-II',
            uniqueId: 'c17284',
            url: '/event/459/contributions/12502/',
          },
          c17285: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12503/attachments/9669/14097/Data%20Management%20in%20Astronomy%20CHEP23%20Bolton.pdf',
                  id: 14097,
                  title: 'Data Management in Astronomy CHEP23 Bolton.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12503,
            description:
              'The astronomy world is moving towards exascale observatories and experiments, with data distribution and access challenges that are comparable with - and on similar timescales to - those of HL-LHC. I will present some of these use cases and show progress towards prototyping work in the Square Kilometre Array (SKA) Regional Centre Network, and present the conceptual architectural view of the global SRCNet that we hope to implement ready for SKA operations from around 2027.',
            duration: 30.0,
            endDate: {
              date: '2023-05-11',
              time: '18:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 657,
            id: 'c17285',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12503/contribution.pdf',
            presenters: [
              {
                affiliation: 'SKA Observatory',
                displayOrderKey: [0, 'Bolton, Rosie'],
                emailHash: '0fec3d9ab76db108cc7142358c67232b',
                familyName: 'Bolton',
                firstName: 'Rosie',
                name: 'Rosie Bolton',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12147,
            sessionSlotId: 2713,
            startDate: {
              date: '2023-05-11',
              time: '17:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title:
              'Global Data Management in Astronomy and the link to HEP: are we collaborators, consumers or competitors?',
            uniqueId: 'c17285',
            url: '/event/459/contributions/12503/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's12147',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2713,
        slotTitle: 'Upcoming Computing Challenges beyond NP/HEP',
        startDate: {
          date: '2023-05-11',
          time: '16:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's12147',
        url: '/event/459/sessions/2024/',
      },
      s12149: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d0c296',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Universite de Geneve (CH)',
            displayOrderKey: [0, 'Antel, Claire'],
            emailHash: '0700a8e2f8e6dd930897159bde503e7d',
            familyName: 'Antel',
            firstName: 'Claire',
            name: 'Claire Antel',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Shahoyan, Ruben'],
            emailHash: '1db06f137fc13354d671d2e50bcdf999',
            familyName: 'Shahoyan',
            firstName: 'Ruben',
            name: 'Ruben Shahoyan',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17023: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11371/attachments/9667/14217/CHEP2023_L0TP.pdf',
                  id: 14217,
                  title: 'CHEP2023_L0TP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11371,
            description:
              'NA62 is a K meson physics experiment based on a decay-in-flight technique and whose Trigger and Data Acquisition system (TDAQ) is multi-level and network based. A reorganization of both the beam line and the detector is foreseen in the next years to complete and extend the physics reach of NA62. One of the challenging aspects of this upgrade is a significant increase (x4) in the event rate which requires a deep revision of the TDAQ system. This revision includes technological and functional aspects. \r\nThe first initiative of the program consists in the upgrade of the hardware trigger processor (L0TP) with a recent platform that offers larger local memory, more computing power and higher transmission bandwidth (L0TP+).  The second action is to increase physics selectivity of the trigger processor using different kinds of online processing, both at front-end and concentrator levels, adopting classical and AI approaches for algorithms design. \r\nL0TP+ is implemented on an FPGA device and is configured and controlled by means of a soft CPU core. Testing has been conducted extensively with a parasitic setup which includes network taps and a commodity server to compare trigger decisions on an event-by-event basis, thus reproducing realistic operation conditions with no impact on the experiment schedule.  In view of the next data taking period, the positive experience of the parasitic mode can be reused as a development setup to explore new algorithms, test additional features, thus speeding up the TDAQ upgrade.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 586,
            id: 'c17023',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11371/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Roma1',
                displayOrderKey: [6, 'Rossi, Cristian'],
                emailHash: 'e33799f3f4b09a7f8818bdd793ca2000',
                familyName: 'Rossi',
                firstName: 'Cristian',
                name: 'Cristian Rossi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12149,
            sessionSlotId: 2714,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Validation of the new hardware trigger processor at NA62 experiment',
            uniqueId: 'c17023',
            url: '/event/459/contributions/11371/',
          },
          c17024: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11404/attachments/9593/14069/CHEP2023-230511-psh.pdf',
                  id: 14069,
                  title: 'CHEP2023-230511-psh.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11404,
            description:
              'The backend of the Belle II data acquisition system consists of a high-level trigger system, online storage, and an express-reconstruction system for online data processing. The high-level trigger system was updated to use the ZeroMQ networking library from the old ring buffer and TCP/IP socket, and the new system is successfully operated. However, the online storage and express-reconstruction system use the old type of data transportation. For future maintainability, we expand the same ZeroMQ library-based system to the online storage and express-reconstruction system. At the same time, we introduce two more updates in the backend system. First, online side raw data output becomes compressed ROOT format which is the official format of the Belle II data. The update helps to reduce the bandwidth of the online to offline data transfer and offline-side computing resource usage for data format conversion and compression. Second, high-level trigger output-based event selection is included in the online storage. The event selection allows more statistics of data quality monitoring from the express-reconstruction system. In the presentation, we show the description and test result of the upgrade before applying it to the beam operation and data taking.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 351,
            id: 'c17024',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11404/contribution.pdf',
            presenters: [
              {
                affiliation: 'KEK',
                displayOrderKey: [1, 'Park, Seokhee'],
                emailHash: 'e0e4220bc531942cf2b7cb5dff908ec0',
                familyName: 'Park',
                firstName: 'Seokhee',
                name: 'Seokhee Park',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12149,
            sessionSlotId: 2714,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'Upgrade of Online Storage and Express-Reconstruction system for the Belle II experiment',
            uniqueId: 'c17024',
            url: '/event/459/contributions/11404/',
          },
          c17025: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11401/attachments/9683/14123/CHEP_2023_P2L1T.pdf',
                  id: 14123,
                  title: 'CHEP_2023_P2L1T.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11401,
            description:
              'The High-Luminosity LHC will open an unprecedented window on the weak-scale nature of the universe, providing high-precision measurements of the standard model as well as searches for new physics beyond the standard model. Such precision measurements and searches require information-rich datasets with a statistical power that matches the high-luminosity provided by the Phase-2 upgrade of the LHC. Efficiently collecting those datasets will be a challenging task, given the harsh environment of 200 proton-proton interactions per LHC bunch crossing. For this purpose, CMS is designing an efficient data-processing hardware trigger (Level-1) that will include tracking information and high-granularity calorimeter information. Trigger data analysis will be performed through sophisticated algorithms such as particle flow reconstruction, including widespread use of Machine Learning. The current conceptual system design is expected to take full advantage of advances in FPGA and link technologies over the coming years, providing a high-performance, low-latency computing platform for large throughput and sophisticated data correlation across diverse sources.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 94,
            id: 'c17025',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11401/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Colorado at Boulder',
                displayOrderKey: [1, 'Savard, Claire'],
                emailHash: 'df3a012c17270586408e8172ca378f4b',
                familyName: 'Savard',
                firstName: 'Claire',
                name: 'Claire Savard',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12149,
            sessionSlotId: 2714,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Overview of the HL-LHC Upgrade for the CMS Level-1 Trigger',
            uniqueId: 'c17025',
            url: '/event/459/contributions/11401/',
          },
          c17026: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11378/attachments/9715/14186/chep2023_fusy.pptx',
                  id: 14186,
                  title: 'chep2023_fusy.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11378,
            description:
              'High Energy Photon Source\uff08HEPS\uff09is expected to generate a huge amount of data, which puts extreme pressure on data I/O in computing tasks. Meanwhile, inefficient data I/O significantly affect computing performance.\r\n\r\nFirstly, the data reading mode and limitations of computing resources are taken into account\uff0cand we propose a method for automatic tuning of storage parameters, such as data block size, for optimizing the data reading speed.\r\n\r\nSecondly, we designed a data processing pipeline scheme for reducing I/O latency and maximizing IO bandwidth utilization while processing high-throughput data. The computing task is split into multiple steps, i.e., data loading, data preprocessing, data processing and data writing, which are executed asynchronously and in parallel.\r\n\r\nFinally, due to the limited storage, the lossless compression methods are applied for further optimizing the I/O speed. However, it will incur additional performance overhead. Thus, we put forward an intelligent lossless compression method, which judges whether compression is beneficial to data I/O, and compresses the suitable data to reduce I/O footprint and required storage resources.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 290,
            id: 'c17026',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11378/contribution.pdf',
            presenters: [
              {
                affiliation: 'IHEP',
                displayOrderKey: [1, 'Fu, Shiyuan'],
                emailHash: '80da714a95d7c08e7a56734aac46b067',
                familyName: 'Fu',
                firstName: 'Shiyuan',
                name: 'Shiyuan Fu',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12149,
            sessionSlotId: 2714,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'A High-Speed Asynchronous Data I/O Method for HEPS',
            uniqueId: 'c17026',
            url: '/event/459/contributions/11378/',
          },
          c17027: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11363/attachments/9630/14203/SimeleviciusCHEP2023.pdf',
                  id: 14203,
                  title: 'SimeleviciusCHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11363,
            description:
              'The CMS data acquisition (DAQ) is implemented as a service-oriented architecture where DAQ applications, as well as general applications such as monitoring and error reporting, are run as self-contained services. The task of deployment and operation of services is achieved by using several heterogeneous facilities, custom configuration data and scripts in several languages. Deployment of all software is carried out by installation of rpms through Puppet management system on physical and virtual machines in computer network. Two main approaches are used to operate and control the life cycle of the different services: short-lived services, such as event building and read-out, are managed using a custom-built infrastructure, while auxiliary, long-running services are managed using systemd. In this work, we restructure the existing system into a homogeneous, scalable cloud architecture adopting a uniform paradigm where all applications are orchestrated in a uniform environment with standardized facilities. In this new paradigm DAQ applications are organized as groups of containers and the required software is packaged into container images. Automation of all aspects of coordinating and managing containers is provided by the Kubernetes environment, where a set of physical and virtual machines is unified in a single pool of compute resources. As opposed to the current system, different versions of the software, including operating system, libraries, and their dependencies, can coexist within the same network host, and can be installed in container images prepared at build time with no need of applying software changes on target machines. In this work we demonstrate that a container-based cloud architecture provides an across-the-board solution that can be applied for DAQ in CMS. We show strengths and advantages of running DAQ applications in a container infrastructure as compared to a traditional application model.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 563,
            id: 'c17027',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11363/contribution.pdf',
            presenters: [
              {
                affiliation: 'Vilnius University/CERN',
                displayOrderKey: [1, 'Simelevicius, Dainius'],
                emailHash: '4050d0120e49830c45c32a4b6d1c38a5',
                familyName: 'Simelevicius',
                firstName: 'Dainius',
                name: 'Dainius Simelevicius',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12149,
            sessionSlotId: 2714,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Towards a container-based architecture for CMS data acquisition',
            uniqueId: 'c17027',
            url: '/event/459/contributions/11363/',
          },
          c17028: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11392/attachments/9648/14182/CERN%20CHEP2023%20MiniDAQ-3-1.pdf',
                  id: 14182,
                  title: 'CERN CHEP2023 MiniDAQ-3-1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11392,
            description:
              'The CMS experiment data acquisition (DAQ) collects data for events accepted by the Level-1 trigger from the different detector systems and assembles them in an event builder prior to making them available for further selection in the High Level Trigger, and finally storing the selected ones for offline analysis. In addition to the central DAQ providing global acquisition functionality, several separate, so-called \u201cMiniDAQ\u201d setups allow operating independent data acquisition runs using an arbitrary subset of the CMS subdetectors.\r\n\r\nDuring Run 2 of the LHC, MiniDAQ setups were running their event builder and high level trigger applications on dedicated resources, separate from those used for the central DAQ. This cleanly separated MiniDAQ setups from the central DAQ system, but also meant limited throughput and a fixed number of possible MiniDAQ setups. In Run 3, MiniDAQ-3 setups share production resources with the new central DAQ system, allowing each setup to operate at the maximum Level-1 rate thanks to the reuse of the resources and network bandwidth.\r\n\r\nThe configuration management tool defines the assignment of shared resources to subdetectors and provides functionality to evolve it, for example when hardware becomes unavailable, minimizing changes to unaffected parts of the system, in such a way as to not disturb ongoing independent runs. A system has been implemented to automatically synchronize MiniDAQ configurations to that of the central DAQ in order to minimize required operator and expert interventions in case of re-assignment of resources. The configuration management tool further provides expert features needed during the commissioning of the new DAQ system, to enable for example performance tests of most of the resources, concurrently to providing MiniDAQ for the commissioning of selected subdetectors.\r\n\r\nWe report on the new configuration management features and on the first year of operational experience with the new MiniDAQ-3 system.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 519,
            id: 'c17028',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11392/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Brummer, Philipp'],
                emailHash: '52e1999ca97e7b12bea71338fa5adc92',
                familyName: 'Brummer',
                firstName: 'Philipp',
                name: 'Philipp Brummer',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12149,
            sessionSlotId: 2714,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'MiniDAQ-3: providing concurrent independent subdetector data-taking on CMS production DAQ resources',
            uniqueId: 'c17028',
            url: '/event/459/contributions/11392/',
          },
        },
        entryType: 'Session',
        friendlyId: 6,
        id: 's12149',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2036/session-timetable.pdf',
        room: 'Marriott Ballroom V-VI',
        sessionCode: '',
        sessionId: 2036,
        sessionSlotId: 2714,
        slotTitle: 'Network \u0026 Infrastructure (Part 2)',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#000000',
        title: 'Track 2 - Online Computing',
        uniqueId: 's12149',
        url: '/event/459/sessions/2036/',
      },
      s12150: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d9dfc3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Fermilab',
            displayOrderKey: [0, 'Yang, Tingjun'],
            emailHash: '521815c4f74228afa6e1c602c0d8d4a7',
            familyName: 'Yang',
            firstName: 'Tingjun',
            name: 'Tingjun Yang',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Espinal, Xavier'],
            emailHash: 'bce5c14595f76db054a5c96d090c8d11',
            familyName: 'Espinal',
            firstName: 'Xavier',
            name: 'Xavier Espinal',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17064: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11459/attachments/9646/14147/REve-RCore-CHEP-2023.pdf',
                  id: 14147,
                  title: 'REve-RCore-CHEP-2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11459,
            description:
              'REve, the new generation of the ROOT event-display module, uses a web server-client model to guarantee exact data translation from the experiments\u0027 data analysis frameworks to users\u0027 browsers. Data is then displayed in various views, including high-precision 2D and 3D graphics views, currently driven by THREE.js rendering engine based on WebGL technology.\r\n\r\nRenderCore, a computer graphics research oriented rendering engine, has been integrated into REve to optimize rendering performance as well as to enable the use of state-of-the-art techniques for object highlighting and object selection. It also allowed for an implementation of optimized instanced rendering through usage of custom shaders and rendering pipeline modifications.\r\n\r\nTo further the impact of this investment and ensure long-term viability of REve, RenderCore has been refactored on top of WebGPU, the next generation GPU interface for browsers that supports compute shaders and introduces significant improvements in GPU utilization. This leads to optimization of interchange data formats, decreases server-client traffic, and improves offloading of data visualization algorithms to the GPU.\r\n\r\nFireworksWeb, physics analysis oriented event-display of the CMS experiment, will be used to demonstrate the results, focusing on optimized visualization of particle trajectories, and high-granularity calorimeters and targeting high data-volume events of heavy-ion collisions and high-luminosity LHC.\r\n\r\nNext steps and directions will be discussed, such as porting parts of RenderCore and client-side REve code to C++ and compiling them into WebAssembly using Emscripten to further optimize the CPU performance of the rendering engine.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 371,
            id: 'c17064',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11459/contribution.pdf',
            presenters: [
              {
                affiliation: 'UCSD',
                displayOrderKey: [0, 'Tadel, Matevz'],
                emailHash: 'b6910c94580994105a7b03af3c3b2c33',
                familyName: 'Tadel',
                firstName: 'Matevz',
                name: 'Matevz Tadel',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12150,
            sessionSlotId: 2715,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'RenderCore \u2013 a new WebGPU-based rendering engine for ROOT-EVE',
            uniqueId: 'c17064',
            url: '/event/459/contributions/11459/',
          },
          c17071: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11422/attachments/9190/13431/go',
                  id: 13431,
                  title: 'CDS record',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11422/attachments/9190/13430/CHEP_EventAugmentation_vanGemmeren.pdf',
                  id: 13430,
                  title: 'CHEP_EventAugmentation_vanGemmeren.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11422,
            description:
              'For HEP event processing, data is typically stored in column-wise synchronized containers, such as most prominently ROOT\u2019s TTree, which have been used for several decades to store by now over 1 exabyte. These containers can combine row-wise association capabilities needed by most HEP event processing frameworks (e.g. Athena for ATLAS) with column-wise storage, which typically results in better compression and more efficient support for many analysis use-cases.\r\nOne disadvantage is that these containers, TTree in the HEP use-case, require to contain the same attributes for each entry/row (representing events), which can make extending the list of attributes very costly in storage, even if those are only required for a small subsample of events.\r\nSince the initial design, the ATLAS software framework features powerful navigational infrastructure to allow storing custom data extensions for subsample of events in separate, but synchronized containers. This allows adding event augmentations to ATLAS standard data products (such as DAOD-PHYS or PHYSLITE) avoiding duplication of those core data products, while limiting their size increase. For this functionality, the framework does not rely on any associations made by the I/O technology (i.e. ROOT), however it supports TTree friends and builds the associated index to allow for analysis outside of the ATLAS framework.\r\nA prototype based on the Long-Lived Particle search is implemented and preliminary results with this prototype will be presented. At this point, augmented data are stored within the same file as the core data. Storing them in separate files will be investigated in future, as this could provide more flexibility, e.g. certain sites may only want a subset of several augmentations or augmentations can be archived to disk once their analysis is complete.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 403,
            id: 'c17071',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11422/contribution.pdf',
            presenters: [
              {
                affiliation: 'Argonne National Laboratory',
                displayOrderKey: [7, 'van Gemmeren, Peter'],
                emailHash: '7f06d3b5b8794957c7a1d77974b84841',
                familyName: 'van Gemmeren',
                firstName: 'Peter',
                name: 'Peter van Gemmeren',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12150,
            sessionSlotId: 2715,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Framework for custom event sample augmentations for ATLAS analysis data',
            uniqueId: 'c17071',
            url: '/event/459/contributions/11422/',
          },
          c17072: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11429/attachments/9562/13876/Marcon_Optimizing_ATLAS_Storage_ATL-SOFT-SLIDE-2023-111.pdf',
                  id: 13876,
                  title: 'Marcon_Optimizing_ATLAS_Storage_ATL-SOFT-SLIDE-2023-111.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11429,
            description:
              'The increased footprint foreseen for Run-3 and HL-LHC data will soon expose\r\nthe limits of currently available storage and CPU resources. Data formats\r\nare already optimized according to the processing chain for which they are\r\ndesigned. ATLAS events are stored in ROOT-based reconstruction output files\r\ncalled Analysis Object Data (AOD), which are then processed within the\r\nderivation framework to produce Derived AOD (DAOD) files.\r\n\r\nNumerous DAOD formats, tailored for specific physics and performance groups,\r\nhave been in use throughout the ATLAS Run-2 phase. In view of Run-3, ATLAS\r\nhas changed its Analysis Model, which entailed a significant reduction of\r\nthe existing DAOD flavors. Two new, unfiltered and skimmable on read,\r\nformats have been proposed as replacements: DAOD_PHYS, designed to meet the\r\nrequirements of the majority of the analysis workflows, and DAOD_PHYSLITE, a\r\nsmaller format containing already calibrated physics objects. As ROOT-based\r\nformats, they natively support four lossless compression algorithms: Lzma,\r\nLz4, Zlib and Zstd.\r\n\r\nIn this study, the effects of different compression settings on file size,\r\ncompression time, compression factor and reading speed are investigated\r\nconsidering both DAOD_PHYS and DAOD_PHYSLITE formats. Total as well as\r\npartial event reading strategies have been tested. Moreover, the impact of\r\nAutoFlush and SplitLevel, two parameters controlling how in-memory data\r\nstructures are serialized to ROOT files, has been evaluated.\r\n\r\nThis study yields quantitative results that can serve as a paradigm on how\r\nto make compression decisions for different ATLAS\u0027 use cases. As an example,\r\nfor both DAOD_PHYS and DAOD_PHYSLITE, the Lz4 library exhibits the fastest\r\nreading speed, but results in the largest files, whereas the Lzma algorithm\r\nprovides larger compression factors at the cost of significantly slower\r\nreading speeds. In addition, guidelines for setting appropriate AutoFlush\r\nand SplitLevel values are outlined.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 419,
            id: 'c17072',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11429/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Milano',
                displayOrderKey: [1, 'Marcon, Caterina'],
                emailHash: 'f8397c5411a38b255202a584253df615',
                familyName: 'Marcon',
                firstName: 'Caterina',
                name: 'Dr Caterina Marcon',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12150,
            sessionSlotId: 2715,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Optimizing ATLAS data storage: the impact of compression algorithms on ATLAS physics analysis data formats',
            uniqueId: 'c17072',
            url: '/event/459/contributions/11429/',
          },
          c17073: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11444/attachments/9627/14011/CHEP2023-esseiva.pdf',
                  id: 14011,
                  title: 'CHEP2023-esseiva.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11444,
            description:
              'With the increased data volumes expected to be delivered by the HL-LHC, it becomes critical for the ATLAS experiment to maximize the utilization of available computing resources ranging from conventional GRID clusters to supercomputers and cloud computing platforms. To be able to run its data processing applications on these resources, the ATLAS software framework must be capable of efficiently executing data processing tasks in heterogeneous distributed computing environments. Today with the use of Gaudi Avalanche Scheduler, a central component of the multithreaded Athena framework whose implementation is based on Intel TBB, we can efficiently schedule Athena algorithms to multiple threads within a single compute node. Our goal is to develop a new framework scheduler capable of supporting distributed heterogeneous environments, based on technologies like HPX and Ray. After the initial evaluation phase of these technologies, we began the actual development of prototype distributed task schedulers and their integration with the Athena framework. This contribution will describe these prototype schedulers , as well as the preliminary results of performance studies of these prototypes within ATLAS data processing applications.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 429,
            id: 'c17073',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11444/contribution.pdf',
            presenters: [
              {
                affiliation: 'Lawrence Berkeley National Laboratory',
                displayOrderKey: [1, 'Esseiva, Julien'],
                emailHash: '5af68c71ad7de9b0d949cefc2fb88c7a',
                familyName: 'Esseiva',
                firstName: 'Julien',
                name: 'Julien Esseiva',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12150,
            sessionSlotId: 2715,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Towards a distributed heterogeneous task scheduler for the ATLAS offline software framework',
            uniqueId: 'c17073',
            url: '/event/459/contributions/11444/',
          },
          c17074: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11435/attachments/9699/14153/chep2023-ddossett.pdf',
                  id: 14153,
                  title: 'chep2023-ddossett.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11435,
            description:
              'Since March 2019 the Belle II detector has collected data from e+ e- collisions at the SuperKEKB collider. For Belle II analyses to be competitive it is crucial that calibration constants are calculated promptly so that the reconstructed datasets can be provided to analysts. A subset of calibration constants also benefit by being re-derived during yearly recalibration campaigns to give analysts the best possible reconstructed datasets for their final publications.\r\n\r\nAt the Belle II experiment a Python package, b2cal, was developed to automate the running of Calibration and Alignment Framework (CAF) processes for prompt calibration at Brookhaven National Laboratory (BNL). This uses the open-source Apache Airflow workflow platform to schedule, run, and monitor the calibration procedures by describing them as Directed Acyclic Graphs (DAGs). This has resulted in a successful reduction of both the time taken to produce constants and the human intervention required. In 2022 the recalibration of older data at the Deutsches Elektronen-Synchrotron Laboratory (DESY) was also performed in parallel with the continuing prompt calibration at BNL. The scope of the system has now expanded to include the organisation of creating calibration constants for run-dependent Monte Carlo data and development of the b2cal package now focuses on incorporating more of the post-calibration data processing tasks. The current structure of the automated Belle II calibration system and these new developments will be described.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 506,
            id: 'c17074',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11435/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Melbourne',
                displayOrderKey: [1, 'Dossett, David'],
                emailHash: 'aed94b337fbeb7232b1e9234e5b5e828',
                familyName: 'Dossett',
                firstName: 'David',
                name: 'David Dossett',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12150,
            sessionSlotId: 2715,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Current Status and Future Developments for Automation of Data Calibration and Processing Procedures at the Belle II Experiment',
            uniqueId: 'c17074',
            url: '/event/459/contributions/11435/',
          },
          c17293: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11460/attachments/9713/14178/CHEP2023_ACTS_opti_RBG.pdf',
                  id: 14178,
                  title: 'CHEP2023_ACTS_opti_RBG.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11460,
            description:
              'Particle tracking is among the most sophisticated and complex part of the full event reconstruction chain. A number of reconstruction algorithms work in a sequence to build these trajectories from detector hits. Each of these algorithms use many configuration parameters that need to be fine-tuned to properly account for the detector/experimental setup, the available CPU budget and the desired physics performance. Few examples of such parameters include the cut values limiting the search space of the algorithm, the approximations accounting for complex phenomena or the parameters controlling algorithm performance. The most popular method to tune these parameters is hand-tuning using brute-force techniques. These techniques can be inefficient and raise issues for the long-term maintainability of such algorithms. The open-source track reconstruction software framework known as \u201cA Common Tracking Framework (ACTS)\u201d offers an alternative solution to these parameter tuning techniques through the use of automatic parameter optimization algorithms. ACTS come equipped with an auto-tuning suite that provides necessary setup for performing optimization of input parameters belonging to track reconstruction algorithms. The user can choose the tunable parameters in a flexible way and define a cost/benefit function for optimizing the full reconstruction chain. The fast execution speed of ACTS allows the user to run several iterations of optimization within a reasonable time bracket. The performance of these optimizers has been demonstrated on different track reconstruction algorithms such as trajectory seed reconstruction and selection, particle vertex reconstruction and generation of simplified material map, and on different detector geometries such as Generic Detector and Open Data Detector (ODD). We aim to bring this approach to all aspects of trajectory reconstruction by having a more flexible integration of tunable parameters within ACTS.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 229,
            id: 'c17293',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11460/contribution.pdf',
            presenters: [
              {
                affiliation: 'Stanford University',
                displayOrderKey: [6, 'Garg, rocky'],
                emailHash: 'b1cf93eba33cde4c13e539a0ae6e9d24',
                familyName: 'Garg',
                firstName: 'rocky',
                name: 'rocky Garg',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12150,
            sessionSlotId: 2715,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title:
              'Potentiality of automatic parameter tuning suite available in ACTS track reconstruction software framework',
            uniqueId: 'c17293',
            url: '/event/459/contributions/11460/',
          },
        },
        entryType: 'Session',
        friendlyId: 7,
        id: 's12150',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2037/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VI',
        sessionCode: '',
        sessionId: 2037,
        sessionSlotId: 2715,
        slotTitle: 'Data Preparation (part 2)',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#272f09',
        title: 'Track 3 - Offline Computing',
        uniqueId: 's12150',
        url: '/event/459/sessions/2037/',
      },
      s12176: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#efebc2',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'KEK/IPNS',
            displayOrderKey: [0, 'Miyake, Hideki'],
            emailHash: '7ae6773a40c49ae5776d96621f5c7ab5',
            familyName: 'Miyake',
            firstName: 'Hideki',
            name: 'Hideki Miyake',
          },
          {
            affiliation: 'SKAO',
            displayOrderKey: [0, 'Joshi, Rohini'],
            emailHash: '7fd55f634d18a43003ce815c039c70ab',
            familyName: 'Joshi',
            firstName: 'Rohini',
            name: 'Rohini Joshi',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16866: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11463/attachments/9717/14188/CHEP_2023_EIC_Ceph_V3.pdf',
                  id: 14188,
                  title: 'CHEP_2023_EIC_Ceph_V3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11463,
            description:
              'The Electron Ion Collider (EIC) collaboration and future experiment is a unique scientific ecosystem within Nuclear Physics as the experiment starts right off as a cross-collaboration from Brookhaven National Lab (BNL) \u0026 Jefferson Lab (JLab). As a result, this muti-lab computing model tries at best to provide services accessible from anywhere by anyone who is part of the collaboration. While the computing model for the EIC is not finalized, it is anticipated that the computational and storage resources will be made accessible to a wide range of collaborators across the world. The use of federated ID seems to be a critical element to the strategy of providing such services, allowing seamless access to each lab site computing resources. However, providing Federated access to a Federated storage is not a trivial matter and has its share of technical challenges.\r\n\r\nIn this contribution, we will focus on the steps we took towards the deployment of a distributed object storage system that integrates with Amazon S3 and Federated ID. We will first cover for and explain the first stage storage solutions provided to the EIC during the detector design phase. Our initial test deployment consisted of Lustre storage using MinIO, hence providing an S3 interface. High Availability load balancers were added later to provide the initial scalability it lacked. Performance of that system will be shown. While this embryonic solution worked well, it had many limitations. Looking ahead, the Ceph object storage is considered a top-of-the-line solution in the storage community - since the Ceph Object Gateway is compatible with the Amazon S3 API out of the box, our next phase will use a native S3 storage. Our Ceph deployment will consist of erasure coded storage nodes to maximize storage potential along with multiple Ceph Object Gateways for redundant access. We will compare performance of our next stage implementations. Finally, we will present how to leverage OpenID Connect with the Ceph Object Gateway\u2019s to enable Federated ID access. \r\n\r\nWe hope this contribution will serve the community needs as we move forward with cross-lab collaborations and the need for Federated ID access to distributed compute facilities.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 575,
            id: 'c16866',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11463/contribution.pdf',
            presenters: [
              {
                affiliation: 'BNL',
                displayOrderKey: [1, 'Poat, Michael'],
                emailHash: 'dcfa0e82c6dcd12fb4fce0404b1fdfbd',
                familyName: 'Poat',
                firstName: 'Michael',
                name: 'Michael Poat',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12176,
            sessionSlotId: 2739,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Federated Access to Distributed Storage in the EIC Computing Era',
            uniqueId: 'c16866',
            url: '/event/459/contributions/11463/',
          },
          c16867: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11476/attachments/9479/13740/chep_2023_anil.pdf',
                  id: 13740,
                  title: 'chep_2023_anil.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11476,
            description:
              'Rucio, the data management software initially developed for ATLAS, has been in use at Belle II since January 2021. After the transition to Rucio, new features and functionality were implemented  in Belle II grid  tools based on Rucio, to improve the experience of grid users. The container structure in the Rucio File Catalog enabled us to define collections of arbitrary datasets, allowing the simple definition of official datasets for physics analyses that result in a reduction of potential mistakes and pre-analysis work for users. Other features, including asynchronous replication, deletion, and direct resolution of file level path information, have improved the experience of analysis on the grid at Belle II. We will describe how the implementation in the Belle II computing architecture was performed to exploit the features from Rucio and how it has enhanced the user experience for analysts.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 103,
            id: 'c16867',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11476/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Mississippi',
                displayOrderKey: [1, 'Panta, Anil'],
                emailHash: 'c8c5a88fc8f62d8afa2848a0cc50daf3',
                familyName: 'Panta',
                firstName: 'Anil',
                name: 'Anil Panta',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12176,
            sessionSlotId: 2739,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Improvement in user experience with Rucio integration into grid tools at Belle II',
            uniqueId: 'c16867',
            url: '/event/459/contributions/11476/',
          },
          c16868: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11484/attachments/9686/14130/Justas%20Balcas%20CPU%20Performance%20for%20MINIAOD%20Reads.pdf',
                  id: 14130,
                  title: 'Justas Balcas CPU Performance for MINIAOD Reads.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11484,
            description:
              'A critical challenge of performing data transfers or remote reads is to be fast and efficient as possible while, at the same time, keeping the usage of system resources as low as possible. Ideally, the software that manages these data transfers should be able to organize them so that one can have them run up to the hardware limits. Significant portions of LHC analysis use the same datasets, running over each file or dataset multiple times. By utilizing "on-demand" based regional caches, we can improve CPU Efficiency and reduce the wide area network usage. Speeding up user analysis and reducing network usage (and hiding latency from jobs by caching most essential files on demand) are significant challenges for HL-LHC, where the data volume increases to an exabyte level. In this paper, we will describe our journey and tests with the CMS XCache project (SoCal Cache), which will compare job performance and CPU efficiency using different storage solutions (Hadoop, Ceph, Local Disk, Named Data Networking). It will also provide insights into our tests over a wide area network and possible storage and network usage savings.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 59,
            id: 'c16868',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11484/contribution.pdf',
            presenters: [
              {
                affiliation: 'California Institute of Technology',
                displayOrderKey: [1, 'Balcas, Justas'],
                emailHash: '7613c8c60c21277df334265b3852ce87',
                familyName: 'Balcas',
                firstName: 'Justas',
                name: 'Justas Balcas',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12176,
            sessionSlotId: 2739,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Job CPU Performance comparison based on MINIAOD reading options: local versus remote',
            uniqueId: 'c16868',
            url: '/event/459/contributions/11484/',
          },
          c16869: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11482/attachments/9247/13415/CHEP23%20PanDA%20(1).pdf',
                  id: 13415,
                  title: 'CHEP23 PanDA.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11482,
            description:
              'In recent years, advanced and complex analysis workflows have gained increasing importance in the ATLAS experiment at CERN, one of the large scientific experiments at the Large Hadron Collider (LHC). Support for such workflows has allowed users to exploit remote computing resources and service providers distributed worldwide, overcoming limitations on local resources and services. The spectrum of computing options keeps increasing across WLCG resources, volunteer computing, high-performance and leadership computing facilities, commercial clouds, and emerging service levels like Platform-as-a-Service (PaaS), Container-as-a-Service (CaaS) and Function-as-a-Service (FaaS), each one providing new advantages and constraints. Users can significantly benefit from these providers, but at the same time, it is cumbersome to deal with multiple providers even in a single analysis workflow with fine-grained requirements coming from their applications\u0027 nature and characteristics.\r\nIn this presentation we will first highlight issues in distributed heterogeneous computing, such as the insulation of users from the complexities of distributed heterogeneous providers, complex resource provisioning for CPU and GPU hybrid applications, integration of PaaS, CaaS, and FaaS providers, smart workload routing, automatic data placement, seamless execution of complex workflows, interoperability between pledged and user resources, and on-demand data production. We will then present solutions developed in ATLAS with the Production and Distributed Analysis system (PanDA system) and future challenges for LHC Run4.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 232,
            id: 'c16869',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11482/contribution.pdf',
            presenters: [
              {
                affiliation: 'Unive',
                displayOrderKey: [1, 'Barreiro Megino, Fernando'],
                emailHash: 'f04de6eee7b8dcc20dedefcb9fe659da',
                familyName: 'Barreiro Megino',
                firstName: 'Fernando',
                name: 'Fernando Barreiro Megino',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12176,
            sessionSlotId: 2739,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Utilizing Distributed Heterogeneous Computing with PanDA in ATLAS',
            uniqueId: 'c16869',
            url: '/event/459/contributions/11482/',
          },
          c16870: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11466/attachments/9305/13502/20230511%20Heterogenous%20resources%20CMS%20CHEP23.pdf',
                  id: 13502,
                  title: '20230511 Heterogenous resources CMS CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11466,
            description:
              'The computing resources supporting the LHC experiments research programmes are still dominated by x86 processors deployed at WLCG sites. This will however evolve in the coming years, as a growing number of HPC and Cloud facilities will be employed by the collaborations in order to process the vast amounts of data to be collected in the LHC Run 3 and into the HL-LHC phase. Compute power in these facilities typically includes a significant (or even dominant) fraction of non-x86 components, such as alternative CPU architectures (ARM, Power) and a variety of GPU specifications. Using these heterogeneous resources efficiently will be therefore essential for the LHC collaborations reaching their scientific goals. The Submission Infrastructure (SI) is a central element in the CMS Offline Computing model, enabling resource acquisition and exploitation by CMS data processing, simulation and analysis tasks. The SI is implemented as a set of federated HTCondor dynamic pools, which must therefore be adapted to ensure access and optimal usage of alternative processors and coprocessors such as GPUs. Resource provisioning and workload management tools and strategies in use by the CMS SI team must take into account questions such as the optimal level of granularity in the description of the resources and how to prioritize CMS diversity of workflows in relation to the new resource mix. Some steps in this evolution towards profiting from this higher resource heterogeneity have been already taken. For example, CMS is already opportunistically using a pool of GPU slots provided mainly at the CMS WLCG sites. Additionally, Power processors have been validated for CMS production at the Marconi100 cluster at CINECA. This contribution will describe the updated capabilities of the SI to continue ensuring the efficient allocation and use of computing resources by CMS, despite their increasing diversity. The next steps towards a full integration and support of heterogeneous resources according to CMS needs will also be reported.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 162,
            id: 'c16870',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11466/contribution.pdf',
            presenters: [
              {
                affiliation: 'CIEMAT - PIC',
                displayOrderKey: [7, 'P\u00e9rez-Calero Yzquierdo, Antonio'],
                emailHash: '01a84001ee4bc4352b98e880033d1fe7',
                familyName: 'P\u00e9rez-Calero Yzquierdo',
                firstName: 'Antonio',
                name: 'Dr Antonio P\u00e9rez-Calero Yzquierdo',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12176,
            sessionSlotId: 2739,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'The integration of heterogeneous resources in the CMS Submission Infrastructure for the LHC Run 3 and beyond',
            uniqueId: 'c16870',
            url: '/event/459/contributions/11466/',
          },
          c16871: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11481/attachments/9688/14133/CHEP_CSV2.pdf',
                  id: 14133,
                  title: 'CHEP_CSV2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11481,
            description:
              'Cloudscheduler is a system to manage resources of local and remote compute clouds and makes those resources available to HTCondor pools. It examines the resource needs of idle jobs, then starts virtual machines (VMs) sized to suit those resource needs on allowed clouds with available resources. Using yaml files, cloudscheduler then provisions the VMs during the boot process with all necessary tools needed to register with HTCondor and run the experiment\u0027s jobs. Although we have run cloudscheduler in its first version for ATLAS and Belle-II workloads successfully for more than 10 years, we developed cloudscheduler version 2 (CSV2), a complete overhaul and modernization of cloudscheduler. We published the technical design of CSV2 in 2019, however, many features have been added since then and the system is used successfully in production for Belle-II, ATLAS, DUNE, and BaBar. In addition to using CSV2 as a WLCG grid site, we also run it as a service for other WLCG grid sites, and the Canadian Advanced Network for Astronomical Research (CANFAR) group uses its own instance of CSV2 for their astronomy workload. In this talk, we report on our experience in operating CSV2 for the different experiment\u0027s jobs from a user\u0027s and administrator\u0027s point of view, running on up to 10,000 cores across all experiments and clouds in North America, Australia, and Europe. We will also report on how to correctly account for the resource usage in the APEL system. CSV2 can be used with its own HTCondor system, but it can also extend an existing HTCondor system with cloud resources, for example in times of high demand of batch computing resources. We will detail how projects can be created and integrated with an existing or new HTCondor system, and how the monitoring works. We will also report on the integration of different clouds, as well as using the integrated opportunistic system. CSV2\u2019s integrated opportunistic system allows the use of the same cloud for different experiments, giving one experiment the preferred usage and others an opportunity to make temporary use of idle resources. In addition, we report on how we worked with different cloud administrators to allow opportunistic use of idle cloud resources, managed by the cloud administrators through cloud metadata.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 84,
            id: 'c16871',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11481/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Victoria',
                displayOrderKey: [2, 'Ebert, Marcus'],
                emailHash: 'd2e0cef05ea1a024f0c2163a3aec8619',
                familyName: 'Ebert',
                firstName: 'Marcus',
                name: 'Marcus Ebert',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12176,
            sessionSlotId: 2739,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Managing remote cloud resources for multiple HEP VO\u2019s with cloudscheduler',
            uniqueId: 'c16871',
            url: '/event/459/contributions/11481/',
          },
        },
        entryType: 'Session',
        friendlyId: 8,
        id: 's12176',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2038/session-timetable.pdf',
        room: 'Marriott Ballroom II-III',
        sessionCode: '',
        sessionId: 2038,
        sessionSlotId: 2739,
        slotTitle: 'Distributed Storage and Computing Resources',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 4 - Distributed Computing',
        uniqueId: 's12176',
        url: '/event/459/sessions/2038/',
      },
      s12177: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfe555',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: '',
            displayOrderKey: [0, 'Eulisse, Giulio'],
            emailHash: '7a6c8f49981d093b41b365a4b522378d',
            familyName: 'Eulisse',
            firstName: 'Giulio',
            name: 'Giulio Eulisse',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Sailer, Andre'],
            emailHash: '84f3f721370dba12b47a611b1a36081e',
            familyName: 'Sailer',
            firstName: 'Andre',
            name: 'Andre Sailer',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16901: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11548/attachments/9706/14189/JJAHAN-%20CHEP_2023_v1.pdf',
                  id: 14189,
                  title: 'JJAHAN- CHEP_2023_v1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11548,
            description:
              'EPOS 4 is the last version of the high-energy collision event generator EPOS, released publicly in 2022. It was delivered with improvements on several aspects, whether about the theoretical bases on which it relies, how they are handled technically, or regarding user\u0027s interface and data compatibility.\r\n\r\nThis last point is especially important, as part of a commitment to provide the widest possible use. In this regard, a new output data format have been implemented, based on the HepMC standard libraries. This feature enables in particular the analysis of EPOS simulations with RIVET, an analysis and validation toolkit for Monte Carlo event generators, with recent major upgrades on concerning heavy-ion analysis methods. In order to take advantage of this, the use of RIVET has been implemented directly in the EPOS analysis machinery, ensuring an easy and fast solution for comparison with experimental data, beneficial for both developers and users. We will hence present in this talk the details of this implementation and the results obtained thanks to it.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 91,
            id: 'c16901',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11548/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Houston',
                displayOrderKey: [1, 'Jahan, Johannes'],
                emailHash: 'a0f093159fa6622d641c96fad27b2c2a',
                familyName: 'Jahan',
                firstName: 'Johannes',
                name: 'Johannes Jahan',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12177,
            sessionSlotId: 2740,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Integrating the RIVET analysis tool into EPOS 4',
            uniqueId: 'c16901',
            url: '/event/459/contributions/11548/',
          },
          c16902: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11550/attachments/9718/14193/chep_2023.pdf',
                  id: 14193,
                  title: 'chep_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11550,
            description:
              'A mechanism to store in databases all the parameters needed to simulate the detectors response to physics interactions is presented. This includes geometry, materials, magnetic field, electronics.\r\n\r\nGEMC includes a python API to populate the databases, and the software to run the Monte-Carlo simulation. The engine is written in C++ and uses Geant4 for the passage of particles through matter.\r\n\r\nAn overview of the software, and its usage will be shown, with examples on how to build geometry, handle geometry variations, and provide realistic electronic response. \r\n\r\nThe usage of GEMC at Jefferson Lab in the CLAS12 experimental program will be showcased.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 490,
            id: 'c16902',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11550/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [1, 'Ungaro, Maurizio'],
                emailHash: '3dc68517c3fb05003595e8b36595398c',
                familyName: 'Ungaro',
                firstName: 'Maurizio',
                name: 'Maurizio Ungaro',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12177,
            sessionSlotId: 2740,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'GEMC: a database driven Monte Carlo simulation program',
            uniqueId: 'c16902',
            url: '/event/459/contributions/11550/',
          },
          c16903: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11549/attachments/9697/14149/CHEP_2023__The_LHCb_simulation_software_Gauss_and_its_Gaussino_core_framework_v2.pdf',
                  id: 14149,
                  title:
                    'CHEP_2023__The_LHCb_simulation_software_Gauss_and_its_Gaussino_core_framework_v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11549,
            description:
              'The LHCb software has undergone a major upgrade in view of data taking with higher luminosity in Run3 of the LHC at CERN. \r\nThe LHCb simulation framework, Gauss, had to be adapted to follow the changes in modern technologies of the underlying experiment core software and to introduce new simulation techniques to cope with the increase of the required amount of simulated data. Additional constraints come from the fact that Gauss also relies on external simulation libraries. \r\nThe new version of Gauss is based on a newly developed experiment agnostic simulation framework, called Gaussino. This new software is based on the Gaudi data processing framework and encapsulates generic simulation components. Gaussino provides components and interfaces for High Energy Physics generators, e.g. Pythia. It relies on the Geant4 toolkit for detector simulation and provides a fast simulation interface to replace the Geant4 physics processes in given sub-detectors with parametric models, including deep learning based options. Geometry layouts can be provided through DD4Hep or experiment-specific detector description tools. A built-in mechanism to define simple volumes at configuration time is also available. Gaussino ensures a consistent multi-threaded execution between the various components and the underlying Gaudi infrastructure. \r\nFollowing an overview of the structure and functionality of Gaussino we will describe how the new version of Gauss exploits the Gaussino infrastructure to provide what required for the simulation(s) of LHCb experiment. \r\nFinally we will show recent developments and performance of the new software as well as first experience of using it to generate simulated samples in the LHCb production system.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 577,
            id: 'c16903',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11549/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Mazurek, Micha\u0142'],
                emailHash: '653f820861fb90b1c4568a092a08702b',
                familyName: 'Mazurek',
                firstName: 'Micha\u0142',
                name: 'Micha\u0142 Mazurek',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12177,
            sessionSlotId: 2740,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'The LHCb simulation software Gauss and its Gaussino core framework',
            uniqueId: 'c16903',
            url: '/event/459/contributions/11549/',
          },
          c16904: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11528/attachments/9698/14150/CHEP_2023__From_prototypes_to_large_scale_detectors_v2.pdf',
                  id: 14150,
                  title: 'CHEP_2023__From_prototypes_to_large_scale_detectors_v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11528,
            description:
              'Gaussino is a new simulation experiment-independent framework based on the Gaudi data processing framework. It provides generic core components and interfaces to build a complete simulation application: generation, detector simulation, geometry, monitoring, and saving of the simulated data. Thanks to its highly configurable and extendable components Gaussino can be used both as a toolkit and a stand-alone application. It provides implementations for software components widely used by the High Energy Physics community, e.g. Pythia and Geant4. Geometry layouts can be provided through DD4Hep or experiment-specific software. A built-in mechanism is available to define simple volumes at configuration time and ease the development cycle. Inspections of the geometry and simulated data can be performed through Geant4 visualization driver accessible in Gaussino. It is also possible to save objects for visualising them a posteriori with Phoenix. We will show how Gaussino can be first used to try out new detector ideas and increasing the complexity of the geometry and physics processes can provide the foundation for a complete experiment simulation where the same detector can be used and its physics performance evaluated. The possibility of retrieving custom information from any place in the detector also allows to obtain samples for proposed additions to experimental setup as well as training datasets for studies involving machine learning, such as fast simulation models, for which use Gaussino provides a dedicated interface.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 580,
            id: 'c16904',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11528/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Mazurek, Micha\u0142'],
                emailHash: '653f820861fb90b1c4568a092a08702b',
                familyName: 'Mazurek',
                firstName: 'Micha\u0142',
                name: 'Micha\u0142 Mazurek',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12177,
            sessionSlotId: 2740,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'From prototypes to large scale detectors: how to exploit the Gaussino simulation framework for detectors studies, with a detour into machine learning',
            uniqueId: 'c16904',
            url: '/event/459/contributions/11528/',
          },
          c16905: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11536/attachments/9701/14155/20230511_CHEP2023_LCIO_Turns20_Graf.pdf',
                  id: 14155,
                  title: '20230511_CHEP2023_LCIO_Turns20_Graf.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11536,
            description:
              'LCIO is a persistency framework and event data model originally developed to foster closer collaboration among the international groups conducting simulation studies for future linear colliders. In the twenty years since its introduction at CHEP 2003 it has formed the backbone for ILC and CLIC physics and detector studies. It has also been successfully employed to study and develop other collider and fixed-target experiments, as well has having been adopted by several detector R\u0026D groups such as CALICE. It has been a remarkably successful collaborative development that has served a number of disparate communities. We intend to discuss the history of the development of LCIO and show how it is being used in simulation studies, testbeam campaigns and running experiments today, a full two decades after its introduction.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 467,
            id: 'c16905',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11536/contribution.pdf',
            presenters: [
              {
                affiliation: 'SLAC National Accelerator Laboratory',
                displayOrderKey: [1, 'Graf, Norman'],
                emailHash: '3e84f64e37bceead0577ddf1db41bb2a',
                familyName: 'Graf',
                firstName: 'Norman',
                name: 'Norman Graf',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12177,
            sessionSlotId: 2740,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'LCIO Turns 20',
            uniqueId: 'c16905',
            url: '/event/459/contributions/11536/',
          },
          c16906: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11532/attachments/9530/14236/Analyzing%20HEP%20workflow%20I_O%20behavior%20with%20Darshan.pdf',
                  id: 14236,
                  title: 'Analyzing HEP workflow I_O behavior with Darshan.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11532,
            description:
              'Modern HEP workflows must manage increasingly large and complex data collections. HPC facilities may be employed to help meet these workflows\u0027 growing data processing needs. However, a better understanding of the I/O patterns and underlying bottlenecks of these workflows is necessary to meet the performance expectations of HPC systems.\r\nDarshan is a lightweight I/O characterization tool that captures concise views of HPC application I/O behavior. It intercepts application I/O calls at runtime,  records file access statistics for each process, and generates log files detailing application I/O access patterns.\r\nTypical HEP workflows include event generation, detector simulation, event reconstruction, and subsequent analysis stages. A study of the I/O behavior of the ATLAS simulation and DAOD_PHY/DAOD_PHYSLITE production, CMS simulation, and DUNE analysis workload using Darshan are presented. Characterization of the various stages at scale would guide the further tuning of the I/O patterns with real HEP workloads to better inform storage capabilities requirements at facilities, uncover the  I/O bottlenecks in current workflows when deployed at scale, and provide recommendations for data format and access patterns for future HEP workloads.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 461,
            id: 'c16906',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11532/contribution.pdf',
            presenters: [
              {
                affiliation: 'Argonne national laboratory',
                displayOrderKey: [1, 'Wang, Rui'],
                emailHash: '442ce6794cf5be46cfae4c74ec5d3186',
                familyName: 'Wang',
                firstName: 'Rui',
                name: 'Rui Wang',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12177,
            sessionSlotId: 2740,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Darshan for HEP application',
            uniqueId: 'c16906',
            url: '/event/459/contributions/11532/',
          },
        },
        entryType: 'Session',
        friendlyId: 9,
        id: 's12177',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2039/session-timetable.pdf',
        room: 'Chesapeake Meeting Room',
        sessionCode: '',
        sessionId: 2039,
        sessionSlotId: 2740,
        slotTitle: 'MISC: MonteCarlos, Infrastructure and Simulation',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 5 - Sustainable and Collaborative Software Engineering',
        uniqueId: 's12177',
        url: '/event/459/sessions/2039/',
      },
      s12178: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#feffbf',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Hageboeck, Stephan'],
            emailHash: '64d6d832b57aa9e7597d4011228b0da0',
            familyName: 'Hageboeck',
            firstName: 'Stephan',
            name: 'Stephan Hageboeck',
          },
          {
            affiliation: 'CNU',
            displayOrderKey: [0, 'Heddle, Dave'],
            emailHash: 'a9a8498d1e59570298c4ee6af05530e7',
            familyName: 'Heddle',
            firstName: 'Dave',
            name: 'Dave Heddle',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16938: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11579/attachments/9714/14260/The_New_Awkward_Ecosystem-Ioana_Ifrim-CHEP2023.pdf',
                  id: 14260,
                  title: 'The_New_Awkward_Ecosystem-Ioana_Ifrim-CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11579,
            description:
              'In particle physics, data analysis frequently needs variable-length, nested data structures such as arbitrary numbers of particles per event and combinatorial operations to search for particle decay. Arrays of these data types are provided by the Awkward Array library.\r\n \r\nThe previous version of this library was implemented in C++, but this impeded its ability to grow. Thus, driven by this limitation, Awkward Array has been deeply restructured to enable its integration with other libraries while preserving its existing high-level API and C++ performance-critical algorithms. In the latest 2.0 release, 50k LoC of C++ have been converted to 20 kLoC of Python.\r\n \r\nIn this talk, we present the design and features of Awkward Array 2.0 and showcase the full ecosystem that developed as a result of the library\u2019s restructuring work. First, this endeavour has laid the groundwork for full CUDA integration (Awkward Arrays can be copied to a GPU). Second, conversion facilities are now available between Awkward Arrays and ROOT\u2019s RDataFrame, Arrow and Parquet. Finally, multiple libraries have been integrated:\r\n\r\n - Dask : Awkward Array calculations can now be delayed and distributed for parallel processing;\r\n - JAX : enables differentiation of functions involving Awkward Arrays;\r\n - Pandas : DataFrame(s) containing Awkward structures.\r\n \r\nAwkward Array 2.0 was released at the end of 2022 and is available for physics research now.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 362,
            id: 'c16938',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11579/contribution.pdf',
            presenters: [
              {
                affiliation: 'Princeton University',
                displayOrderKey: [1, 'Ifrim, Ioana'],
                emailHash: 'f127069cda9e32442fad4d2991bbf21c',
                familyName: 'Ifrim',
                firstName: 'Ioana',
                name: 'Ioana Ifrim',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12178,
            sessionSlotId: 2741,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'The New Awkward Ecosystem',
            uniqueId: 'c16938',
            url: '/event/459/contributions/11579/',
          },
          c16946: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11560/attachments/9692/14160/High-performance%20end-user%20analysis%20with%20Julia%20language.pdf',
                  id: 14160,
                  title: 'High-performance end-user analysis with Julia language.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11560,
            description:
              'We present tools for high-performance analysis written in pure Julia, a just-in-time (JIT) compiled dynamic programming language with a high-level syntax and performance. The packages we present center around UnROOT.jl, a pure Julia ROOT file I/O package that is optimized for speed, lazy reading, flexibility, and thread safety.\r\n\r\nWe discuss what affects performance in Julia, the challenges, and their solutions during the development of UnROOT.jl. We highlight type stability as a challenge and discuss its implication whenever any \u201ccompilation\u201d happens (incl. Numba, Jax, C++) as well as Julia\u2019s specific ones.\r\n\r\nWe demonstrate the performance and \u201ceasy to use\u201d claim by comparing UnROOT.jl against popular alternatives (RDataFrame, Uproot, etc.) in medium-size realistic benchmarks, comparing both performance and code complexity.\r\n\r\nFinally, we also showcase real ATLAS analysis workflows both locally and on an HPC system, highlighting the composability of UnROOT.jl with multi-thread/process and out-of-core distributed computing libraries.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 86,
            id: 'c16946',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11560/contribution.pdf',
            presenters: [
              {
                affiliation: 'Harvard University',
                displayOrderKey: [1, 'Ling, Jerry'],
                emailHash: 'd64465c7f8222f6afd02156c0ab46a00',
                familyName: 'Ling',
                firstName: 'Jerry',
                name: 'Jerry Ling',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12178,
            sessionSlotId: 2741,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'High-performance end-user analysis in pure Julia programming language',
            uniqueId: 'c16946',
            url: '/event/459/contributions/11560/',
          },
          c16947: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11566/attachments/9611/14168/ServiceX-BenGalewsky-CHEP2023.pptx',
                  id: 14168,
                  title: 'ServiceX-BenGalewsky-CHEP2023.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11566,
            description:
              'We will describe how ServiceX, an IRIS-HEP project,  generates C++ or python code from user queries and orchestrates thousands of experiment-provided docker containers to filter and select event data. The source datafiles are identified using Rucio. We will show how the service encapsulates best practice for using Rucio and helps inexperienced analysers get up to speed quickly. The data is returned as flat root or parquet files in an object store. We will show how ServiceX is deployed into a modern analysis facility as part of a scalable analysis workflow. Recent work includes support for CMS MiniAOD files, robust failover across file replicas, CERN OpenData support, Autoscaling of worker pods.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 223,
            id: 'c16947',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11566/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Illinois',
                displayOrderKey: [1, 'Galewsky, Ben'],
                emailHash: '7194392db4aaf9500f726c3d0090d86d',
                familyName: 'Galewsky',
                firstName: 'Ben',
                name: 'Ben Galewsky',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12178,
            sessionSlotId: 2741,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Extracting Columnar Event Data From Experiment Specific Data Formats At Scale',
            uniqueId: 'c16947',
            url: '/event/459/contributions/11566/',
          },
          c16948: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11557/attachments/9454/15237/CHEP2023_Awkward_JIT_Compilation.pdf',
                  id: 15237,
                  title: 'CHEP2023_Awkward_JIT_Compilation.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11557/attachments/9454/15599/CHEP_2023__Awkward_Just_In_Time__JIT__Compilation__A_Developer_s_Experience-5.pdf',
                  id: 15599,
                  title:
                    'CHEP_2023__Awkward_Just_In_Time__JIT__Compilation__A_Developer_s_Experience-5.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11557/attachments/9454/15234/publication_right_form-4.pdf',
                  id: 15234,
                  title: 'publication_right_form-4.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11557,
            description:
              'Awkward Arrays is a library for performing NumPy-like computations on nested, variable-sized data, enabling array-oriented programming on arbitrary data structures in Python. However, imperative (procedural) solutions can sometimes be easier to write or faster to run. Performant imperative programming requires compilation; JIT-compilation makes it convenient to compile in an interactive Python environment.\r\n\r\nSeveral functions in Awkward Arrays JIT-compile a user\u2019s code into executable machine code. They use several different techniques, but reuse parts of each others\u2019 implementations.\r\n\r\nWe discuss the techniques used to achieve the Awkward Arrays acceleration with JIT-compilation, focusing on RDataFrame, cppyy, and Numba, particularly Numba on GPUs:\r\n\r\n - Conversions of Awkward Arrays to and from RDataFrame (stable).\r\n - Standalone cppyy (in development).\r\n - Passing Awkward Arrays to and from Python functions compiled by Numba (stable).\r\n - Passing Awkward Arrays to Python functions compiled for GPUs by Numba (in development).\r\n - Header-only libraries for populating Awkward Arrays from C++ without\r\n   any Python dependencies (stable).',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 338,
            id: 'c16948',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11557/contribution.pdf',
            presenters: [
              {
                affiliation: 'Princeton University',
                displayOrderKey: [0, 'Schreiner, Henry'],
                emailHash: 'd650b9966acbbc9dbb5005deb583a407',
                familyName: 'Schreiner',
                firstName: 'Henry',
                name: 'Henry Schreiner',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12178,
            sessionSlotId: 2741,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Awkward Just-In-Time (JIT) Compilation: A Developer\u2019s Experience',
            uniqueId: 'c16948',
            url: '/event/459/contributions/11557/',
          },
          c16956: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11568/attachments/9720/14195/KyungEon_CHEP2023.pdf',
                  id: 14195,
                  title: 'KyungEon_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11568,
            description:
              'Recent developments of HEP software allow novel approaches to physics analysis workflows. The novel data delivery system, ServiceX, can be very effective when accessing a fraction of large datasets at remote grid sites. ServiceX can deliver user-selected columns with filtering and run at scale. We will introduce the ServiceX data management package, ServiceX DataBinder, for easy manipulations of ServiceX delivery requests and delivered data using a single configuration file. We will show various practical use cases within analysis pipelines that range from a data delivery of a few columns for machine learning study to a data delivery for full-scale physics analysis.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 465,
            id: 'c16956',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11568/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Texas at Austin',
                displayOrderKey: [1, 'Choi, KyungEon'],
                emailHash: '8792fb7edf495a947bada9ba0ba5e8bf',
                familyName: 'Choi',
                firstName: 'KyungEon',
                name: 'KyungEon Choi',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12178,
            sessionSlotId: 2741,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title:
              'Data Management Package for the novel data delivery system, ServiceX, and Applications to various physics analysis workflows',
            uniqueId: 'c16956',
            url: '/event/459/contributions/11568/',
          },
          c16957: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11563/attachments/9696/14191/CHEP_2023_cling_Interpreting_C__20.pdf',
                  id: 14191,
                  title: 'CHEP_2023_cling_Interpreting_C__20.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11563,
            description:
              'Cling is a clang/LLVM-based, high-performance C++ interpreter originating from HEP. In ROOT, cling is used as the basis for language interoperability, it provides reflection data to ROOT\u0027s I/O system, and enables RDataFrame\u0027s dynamic type-safe interfaces.\r\n\r\nCling regularely moves to more recent LLVM versions to bring new features and support for new language standards. The recent LLVM 13 upgrade introduces support for C++20, including new language features such as C++ concepts. We describe what else the new LLVM infrastructure has to offer, including updated CUDA support and a more reliable and customizable JIT facility. In addition, we describe the recently-added Cling tooling support for performance tracing and debugging, making interpreted code a good citizen of the C++ ecosystem. Given Cling\u0027s sensitivity to bugs in the system stacks (compilers, ABI, standard libraries), we provide some insights on the challenges faced on the way to support Apple\u0027s ARM architecture. Finally, we share details on how to get the fastest interpreted code out of Cling, be it analysis or reconstruction code.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 469,
            id: 'c16957',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11563/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [4, 'Canal, Philippe'],
                emailHash: '6d86e8b49397c11a44f4a1d750a5d65a',
                familyName: 'Canal',
                firstName: 'Philippe',
                name: 'Philippe Canal',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12178,
            sessionSlotId: 2741,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Interpreting C++20 and CUDA, with profiling and debugging',
            uniqueId: 'c16957',
            url: '/event/459/contributions/11563/',
          },
        },
        entryType: 'Session',
        friendlyId: 10,
        id: 's12178',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2040/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VIII',
        sessionCode: '',
        sessionId: 2040,
        sessionSlotId: 2741,
        slotTitle: 'AM Parallel',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#1f1f02',
        title: 'Track 6 - Physics Analysis Tools',
        uniqueId: 's12178',
        url: '/event/459/sessions/2040/',
      },
      s12179: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#1a3f14',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'KEK',
            displayOrderKey: [0, 'Kishimoto, Tomoe'],
            emailHash: '1c2a9b3c698c4de9040fbe367da6851b',
            familyName: 'Kishimoto',
            firstName: 'Tomoe',
            name: 'Tomoe Kishimoto',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Wiebalck, Arne'],
            emailHash: 'a7f74ac17e363dd0ab1ffe2342ba5c5f',
            familyName: 'Wiebalck',
            firstName: 'Arne',
            name: 'Arne Wiebalck',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17111: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11645/attachments/9482/14204/20230511_Applications_of_Tianhe2_for_Bes3.pdf',
                  id: 14204,
                  title: '20230511_Applications_of_Tianhe2_for_Bes3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11645,
            description:
              'High energy physics experiments are pushing forward the precision measurements and searching for new physics beyond standard model. It is urgent to simulate and generate mass data to meet requirements from physics. It is one of the most popular areas to make good use of existing power of supercomputers for high energy physics computing. Taking the BESIII experiment as an illustration, we deploy the offline software BOSS into the top-tier supercomputer "Tianhe-II" with the help of Singularity. With very limited internet connection bandwidth and without root privilege, we synchronize and maintain the simulation software up to date through CVMFS successfully, and an acceleration rate in a comparison of HPC and HTC is realized for the same large-scale task. There are two creative ideas to be shared in the community: on one hand, common users constantly meet problems in the real-time internet connection and the conflict of loading locker. We solve these two problems by a deployment of a squid server and using fuse in memory in each computing node. On the other hand, we provide a MPI python interface for high throughput parallel computation in Tianhe-II. Meanwhile, the program to deal with data output is also specially aligned so that there is no queue issue in the I/O task. The acceleration rate in simulation reaches 80% so far, as we have done the simulation tests up to 15 K processes in parallel.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 52,
            id: 'c17111',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11645/contribution.pdf',
            presenters: [
              {
                affiliation: 'Sun Yat-sen University',
                displayOrderKey: [1, 'Hu, Biying'],
                emailHash: '223a8e91d79094c3181e7e0a3bfe525b',
                familyName: 'Hu',
                firstName: 'Biying',
                name: 'Biying Hu',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12179,
            sessionSlotId: 2742,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Application of a supercomputer Tianhe-II in BESIII',
            uniqueId: 'c17111',
            url: '/event/459/contributions/11645/',
          },
          c17112: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11634/attachments/9680/14119/20230511_CMS-BSC-integration%20(1).pdf',
                  id: 14119,
                  title: '20230511_CMS-BSC-integration (1).pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11634,
            description:
              'The CMS experiment is working to integrate an increasing number of High Performance Computing (HPC) resources into its distributed computing infrastructure. The case of the Barcelona Supercomputing Center (BSC) is particularly challenging as severe network restrictions prevent the use of CMS standard computing solutions. The CIEMAT CMS group has performed significant work in order to overcome these constraints and make BSC resources available to CMS. The developments include adapting the workload management tools, replicating the CMS software repository to BSC storage, providing an alternative access to detector conditions data, and setting up a service to transfer produced output data to a nearby storage facility. In this work, we discuss the current status of this integration activity, and present recent developments, such as a front-end service to improve slot usage efficiency, and an enhanced transfer service that supports the staging of input data for workflows at BSC. Moreover, significant efforts have been devoted to improving the scalability of the deployed solution, automating its operation, and simplifying the matchmaking of CMS workflows that are suitable for execution at BSC.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 156,
            id: 'c17112',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11634/contribution.pdf',
            presenters: [
              {
                affiliation: 'CIEMAT - PIC',
                displayOrderKey: [2, 'Josep, Flix'],
                emailHash: '216979a87592f83986535d8be6697c2f',
                familyName: 'Josep',
                firstName: 'Flix',
                name: 'Dr Flix Josep',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12179,
            sessionSlotId: 2742,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Integration of the Barcelona Supercomputing Center for CMS computing: towards large scale production',
            uniqueId: 'c17112',
            url: '/event/459/contributions/11634/',
          },
          c17113: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11615/attachments/9650/14233/hufnagel_us_hpc_cms_reco.pdf',
                  id: 14233,
                  title: 'hufnagel_us_hpc_cms_reco.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11615,
            description:
              'With advances in the CMS CMSSW framework to support GPU, culminating with the deployment of GPU in the Run-3 HLT, CMS is also starting to look at integrating GPU resources into CMS Offline Computing as well. At US HPC Facilities a number of very large HPC with GPU have either just become available or will become available soon, offering opportunities for CMS if we would be able to make use of them. We are reporting our experience with commissioning of GPU resources at OLCF Summit and NERSC Perlmutter and using them for GPU-enabled CMS data reconstruction.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 64,
            id: 'c17113',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11615/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Hufnagel, Dirk'],
                emailHash: '068b9018b69ea40467e02d92d6a35844',
                familyName: 'Hufnagel',
                firstName: 'Dirk',
                name: 'Dirk Hufnagel',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12179,
            sessionSlotId: 2742,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Commissioning of US HPC GPU resources for CMS Data Reconstrucion',
            uniqueId: 'c17113',
            url: '/event/459/contributions/11615/',
          },
          c17114: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11619/attachments/9728/14219/SCAILFIN_CHEP_2023.pdf',
                  id: 14219,
                  title: 'SCAILFIN_CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11619,
            description:
              'The NSF-funded Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN) project has developed and deployed artificial intelligence (AI) and likelihood-free inference (LFI) techniques and software using scalable cyberinfrastructure (CI) built on top of existing CI elements.  Specifically, the project has extended the CERN-based REANA framework, a cloud-based data analysis platform deployed on top of Kubernetes clusters that was originally designed to enable analysis reusability and reproducibility. REANA is capable of orchestrating extremely complicated multi-step workflows, and uses Kubernetes clusters both for scheduling and distributing container-based workloads across a cluster of available machines, as well as instantiating and monitoring the concrete workloads themselves. This work describes the the components that were developed in order to enable large scale deployment on High Performance Computing (HPC) resources.  Scaling and performance results using large-scale MadMiner AI/LFI training workflows on a variety of large HPC sites will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 541,
            id: 'c17114',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11619/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Notre Dame',
                displayOrderKey: [2, 'Hurtado Anampa, Kenyi'],
                emailHash: '7943facafd939b2fdef4acd9f444ae8a',
                familyName: 'Hurtado Anampa',
                firstName: 'Kenyi',
                name: 'Dr Kenyi Hurtado Anampa',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12179,
            sessionSlotId: 2742,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Results from Large-scale HPC deployment of Scalable CyberInfrastructure for Artificial Intelligence and Likelihood Free Inference (SCAILFIN)',
            uniqueId: 'c17114',
            url: '/event/459/contributions/11619/',
          },
          c17115: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11647/attachments/9672/14109/Cifra%20P%20-%20Lifecycle%20Management,%20Business%20Continuity%20and%20Disaster%20Recovery%20Planning%20for%20the%20LHCb%20Experiment%20Control%20System%20Infrastructure.pdf',
                  id: 14109,
                  title:
                    ' Cifra P - Lifecycle Management, Business Continuity and Disaster Recovery Planning for the LHCb Experiment Control System Infrastructure.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11647,
            description:
              'LHCb (Large Hadron Collider beauty) is one of the four large particle physics experiments aimed at studying differences between particles and anti-particles and very rare decays in the charm and beauty sector of the standard model at the LHC. The Experiment Control System (ECS) is in charge of the configuration, control, and monitoring of the various subdetectors as well as all areas of the online system, and it is built on top of hundreds of Linux virtual machines (VM) running on a Red Hat Enterprise Virtualization cluster. For such a mission-critical project, it is essential to keep the system operational; it is not possible to run the LHCb\u2019s Data Acquisition without the ECS, and a failure would likely mean the loss of valuable data. In the event of a disruptive fault, it is important to recover as quickly as possible in order to restore normal operations. In addition, the VM\u2019s lifecycle management is a complex task that needs to be simplified, automated, and validated in all of its aspects, with particular focus on deployment, provisioning, and monitoring. The paper describes the LHCb\u2019s approach to this challenge, including the methods, solutions, technology, and architecture adopted. We also show limitations and problems encountered, and we present the results of tests performed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 243,
            id: 'c17115',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11647/contribution.pdf',
            presenters: [
              {
                affiliation: 'Nikhef National institute for subatomic physics (PL) and CERN',
                displayOrderKey: [1, 'Cifra, Pierfrancesco'],
                emailHash: 'ef5712f1fb99bff427b9ba79d1807673',
                familyName: 'Cifra',
                firstName: 'Pierfrancesco',
                name: 'Pierfrancesco Cifra',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12179,
            sessionSlotId: 2742,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Lifecycle Management, Business Continuity and Disaster Recovery Planning for the LHCb Experiment Control System Infrastructure',
            uniqueId: 'c17115',
            url: '/event/459/contributions/11647/',
          },
          c17116: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11623/attachments/9647/14052/ATL-SOFT-SLIDE-2023-150.pdf',
                  id: 14052,
                  title: 'ATL-SOFT-SLIDE-2023-150.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11623,
            description:
              'The ATLAS Trigger and Data Acquisition (TDAQ) High Level Trigger (HLT) computing farm contains 120,000 cores. These resources are critical for online selection and collection of collision data in the ATLAS experiment during LHC operation. Since 2013, during longer period of LHC inactivity these resources are being used for offline event simulation via the "Simulation at Point One" project (Sim@P1). With the recent start of LHC Run 3 and the flat computing budget expected in the near future, finding ways to maximize the resource utilization efficiency is of paramount importance. Recent improvements in the ATLAS software stack can potentially allow the utilization of the Sim@P1 even during LHC operation for the duration of the LHC inter-fill gaps. While previous papers on the Sim@P1 project were emphasizing the technical implementation details, the current contribution is presenting results of a variety of tests that led to the optimal configuration of the job submission infrastructure which would allow the use of Sim@P1 during LHC Run 3.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 339,
            id: 'c17116',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11623/contribution.pdf',
            presenters: [
              {
                affiliation: 'The University of Texas at Arlington',
                displayOrderKey: [1, 'Glushkov, Ivan'],
                emailHash: 'fa4b98ff2babf0afd4594226775ea675',
                familyName: 'Glushkov',
                firstName: 'Ivan',
                name: 'Ivan Glushkov',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12179,
            sessionSlotId: 2742,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Optimization of opportunistic utilization of the ATLAS High-Level Trigger farm for LHC Run3',
            uniqueId: 'c17116',
            url: '/event/459/contributions/11623/',
          },
        },
        entryType: 'Session',
        friendlyId: 11,
        id: 's12179',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2041/session-timetable.pdf',
        room: 'Marriott Ballroom IV',
        sessionCode: '',
        sessionId: 2041,
        sessionSlotId: 2742,
        slotTitle: 'HPC and Deployment',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#f1ffef',
        title: 'Track 7 - Facilities and Virtualization',
        uniqueId: 's12179',
        url: '/event/459/sessions/2041/',
      },
      s12180: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#e3f2d3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Jefferson Lab',
            displayOrderKey: [0, 'Diefenthaler, Markus'],
            emailHash: 'af834e0ec8ce37c7bc6e53f554561c99',
            familyName: 'Diefenthaler',
            firstName: 'Markus',
            name: 'Markus Diefenthaler',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Gazzarrini, Elena'],
            emailHash: '39921bb8f8e49b6f44e880111530f493',
            familyName: 'Gazzarrini',
            firstName: 'Elena',
            name: 'Elena Gazzarrini',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17152: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11703/attachments/9656/14159/ATLAS_communication_CHEP2023.pdf',
                  id: 14159,
                  title: 'ATLAS_communication_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11703,
            description:
              'Communicating the science and achievements of the ATLAS Experiment is a core objective of the ATLAS Collaboration. This talk will explore the range of communication strategies adopted in ATLAS communications, with particular focus on how these have been impacted by the COVID-19 pandemic. In particular, an overview of ATLAS\u2019 digital communication platforms will be given \u2013 with focus on social media, YouTube and Virtual Visits \u2013 and the effect on target audiences evaluated with best practices are shared.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 182,
            id: 'c17152',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11703/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'Le Boulicaut, Elise'],
                emailHash: 'b6df0fa5336cd917de8ffb9db205aae1',
                familyName: 'Le Boulicaut',
                firstName: 'Elise',
                name: 'Elise Le Boulicaut',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12180,
            sessionSlotId: 2743,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Sharing ATLAS Science: communicating to the public',
            uniqueId: 'c17152',
            url: '/event/459/contributions/11703/',
          },
          c17153: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11684/attachments/9729/14223/YACOOB-IPPOG-CHEP-20230511.pptx',
                  id: 14223,
                  title: 'YACOOB-IPPOG-CHEP-20230511.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11684,
            description:
              'The International Particle Physics Outreach Group (IPPOG) is a network of scientists, science educators and communication specialists working across the globe in informal science education and public engagement for particle physics. The primary methodology adopted by IPPOG includes the direct participation of scientists active in current research with education and communication specialists, in order to effectively develop and share best practices in outreach. IPPOG member activities include the International Particle Physics Masterclass programme, International Day of Women and Girls in Science, Worldwide Data Day, International Muon Week and International Cosmic Day organisation, and participation in activities ranging from public talks, festivals, exhibitions, teacher training, student competitions, and open days at local institutes. These independent activities, often carried out in a variety of languages to public with a variety of backgrounds, all serve to gain the public trust and to improve worldwide understanding and support of science. We present our vision of IPPOG as a strategic pillar of particle physics, fundamental research and evidence-based decision-making around the world.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 347,
            id: 'c17153',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11684/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Cape Town / CERN',
                displayOrderKey: [3, 'Yacoob, Sahal'],
                emailHash: 'f5674a312cd5a35e74051775a2095b43',
                familyName: 'Yacoob',
                firstName: 'Sahal',
                name: 'Sahal Yacoob',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12180,
            sessionSlotId: 2743,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title:
              'The International Particle Physics Outreach Group - Engaging the world with science',
            uniqueId: 'c17153',
            url: '/event/459/contributions/11684/',
          },
          c17154: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11704/attachments/9690/14135/Software%20Training%20Outreach%20in%20IRISHEP-QuarkNet.pdf',
                  id: 14135,
                  title: 'Software Training Outreach in  IRISHEP-QuarkNet.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11704,
            description:
              'Sudhir Malik, Peter Elmer, Adam LaMee, Ken Cecire\r\n\r\nThe NSF-funded IRIS-HEP "Training, Education \u0026 Outreach" program and QuarkNet are partnering to enable and expand software training for the high school teachers with a goal to tap, grow and diversify the talent pipeline from K-12 students for future cyberinfrastructure. IRIS-HEP (https://iris-hep.org/) is a software institute that aims to develop the state-of-the-art software cyberinfrastructure for the High Luminosity Large Hadron Collider (HL-LHC) at CERN and other planned HEP experiments of the 2020\u2019s. QuarkNet (https://quarknet.org/) provides professional development to K-12 physics teachers in particle physics content and methods. The two projects have recently built a collaborative relationship where a well-established community of QuarkNet K-12 teachers has access to a wide training on software tools via its Data and Coding Camps supported by IRIS-HEP. The talk highlights the synergistic efforts and future plans.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 330,
            id: 'c17154',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11704/contribution.pdf',
            presenters: [
              {
                affiliation: 'CROEM High School (Departamento de Educaci\u00f3n de Puerto Rico)',
                displayOrderKey: [3, 'Cordero, Danelix'],
                emailHash: 'ddbf8ed30a5fc32230fa63b427e23972',
                familyName: 'Cordero',
                firstName: 'Danelix',
                name: 'Danelix Cordero',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12180,
            sessionSlotId: 2743,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'Software Training Outreach In HEP',
            uniqueId: 'c17154',
            url: '/event/459/contributions/11704/',
          },
          c17155: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11689/attachments/9536/14080/CHEP2023-Strategy-and-Evaluation-PE-.pptx',
                  id: 14080,
                  title: 'CHEP2023-Strategy-and-Evaluation-PE-.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11689,
            description:
              'UKRI/STFC\u2019s Scientific Computing Department (SCD) runs a vibrant range of computing related public engagement activities. We benefit form the work done by the National Labs public engagement team to develop a well articulated PE strategy, and an accompanying evaluation framework, including the idea of defining formal generic learning outcomes (GLOs).\r\n\r\nThis paper presents how this combination has supported better decision making when applying limited human and financial resources. The evaluation framework helps us ensure our activities are effective and GLOs help ensure activities communicate what we intend.\r\n\r\nFurther these all combined to enable a rapid pivot to fully online PE during the pandemic, improving reach in many areas. As we return to face-to-face PE activities we also have a new repertoire of remote activities that are accessible to populations unable to travel to our facilities.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 515,
            id: 'c17155',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11689/contribution.pdf',
            presenters: [
              {
                affiliation: 'UKRI-STFC',
                displayOrderKey: [1, 'Collier, Ian'],
                emailHash: '84c6ace0bbe762b8ebd2bc3870870a69',
                familyName: 'Collier',
                firstName: 'Ian',
                name: 'Ian Collier',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12180,
            sessionSlotId: 2743,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title:
              'The role of strategy and evaluation in delivering effective and innovative public engagement',
            uniqueId: 'c17155',
            url: '/event/459/contributions/11689/',
          },
          c17156: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11693/attachments/9655/14072/ATLAS_VV_CHEP2023.pdf',
                  id: 14072,
                  title: 'ATLAS_VV_CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#e3f2d3',
            conferenceId: 459,
            contributionId: 11693,
            description:
              'The Virtual Visit service run by the ATLAS Collaboration has been active since 2010. The ATLAS Collaboration has used this popular and effective method to bring the excitement of scientific exploration and discovery into classrooms and other public places around the world. The programme, which uses a combination of video conferencing, webcasts, and video recording to communicate with remote audiences, has already reached tens of thousands of viewers, in a large number of languages, from tens of countries across all continents. We present a summary of the ATLAS Virtual Visit service that is currently in use \u2013 including a new booking system and hand-held video conference setup from the ATLAS cavern \u2013 and present a new system that is being installed in the ATLAS Visitors Centre. In addition, we show the reach of the programme over the last few years.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 187,
            id: 'c17156',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11693/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'Le Boulicaut, Elise'],
                emailHash: 'b6df0fa5336cd917de8ffb9db205aae1',
                familyName: 'Le Boulicaut',
                firstName: 'Elise',
                name: 'Elise Le Boulicaut',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2042,
            sessionSlotEntryId: 12180,
            sessionSlotId: 2743,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#253f08',
            title: 'ATLAS Virtual Visits: Bringing the world to our detector',
            uniqueId: 'c17156',
            url: '/event/459/contributions/11693/',
          },
        },
        entryType: 'Session',
        friendlyId: 12,
        id: 's12180',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2042/session-timetable.pdf',
        room: 'Marriott Ballroom I',
        sessionCode: '',
        sessionId: 2042,
        sessionSlotId: 2743,
        slotTitle: 'Outreach and Public Engagement',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#253f08',
        title: 'Track 8 - Collaboration, Reinterpretation, Outreach and Education',
        uniqueId: 's12180',
        url: '/event/459/sessions/2042/',
      },
      s12181: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#92b6db',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'INFN-CNAF',
            displayOrderKey: [0, 'Dal Pra, Stefano'],
            emailHash: '4bb37fa7bb6103869e0d7cdb7c104ea6',
            familyName: 'Dal Pra',
            firstName: 'Stefano',
            name: 'Stefano Dal Pra',
          },
          {
            affiliation: 'University of Washington (US)',
            displayOrderKey: [0, 'Schaarschmidt, Jana'],
            emailHash: '6f556891cf4ef30c552a4ac4aa1c87dc',
            familyName: 'Schaarschmidt',
            firstName: 'Jana',
            name: 'Jana Schaarschmidt',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17188: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11754/attachments/9719/14231/JetNet_CHEP.pdf',
                  id: 14231,
                  title: 'JetNet_CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11754,
            description:
              'Machine learning (ML) has become ubiquitous in high energy physics (HEP) for many tasks, including classification, regression, reconstruction, and simulations. To facilitate development in this area, and to make such research more accessible, and reproducible, we require standard, easy-to-access, datasets and metrics. To this end, we develop the open source Python JetNet library with easily accessible, standardised interfaces for particle cloud datasets, implementations for HEP evaluation and loss metrics, and more useful tools for ML in HEP. While still in the development stage, JetNet has already been widely used for several ML projects at the LHC, averaging 2,000 downloads per month, and being prominently featured at recent conferences such as ML4Jets, illustrating its significant contribution to making ML in HEP research more FAIR.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 154,
            id: 'c17188',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11754/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [3, 'Pareja, Carlos'],
                emailHash: 'e4109b23dfebd3d9a9c6a7367159d2b5',
                familyName: 'Pareja',
                firstName: 'Carlos',
                name: 'Carlos Pareja',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12181,
            sessionSlotId: 2744,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'JetNet library for machine learning in high energy physics',
            uniqueId: 'c17188',
            url: '/event/459/contributions/11754/',
          },
          c17189: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11720/attachments/9721/14198/CHEP_2023_CF_ePIC.pdf',
                  id: 14198,
                  title: 'CHEP_2023_CF_ePIC.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11720,
            description:
              'The newly formed EPIC Collaboration has recently laid the foundations of its software infrastructure. Noticeably, several forward-looking aspects of the software are favorable for Artificial Intelligence (AI) and Machine Learning (ML) applications and utilization of heterogeneous resources. EPIC has a unique opportunity to integrate AI/ML from the beginning: the number of AI/ML activities is anticipated to grow in the coming months (spanning from, e.g., design and other automated procedures to simulation, reconstruction, and particle identification); in the long-term, AI/ML will likely permeate and contribute to multiple aspects of near real-time analyses. This talk will provide an overview of all ongoing activities regarding AI/ML for EPIC and will present plans and steps forward for future implementations',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 616,
            id: 'c17189',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11720/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Fanelli, Cristiano'],
                emailHash: 'aa7084c4c6bfd33cb8e030c293d332fc',
                familyName: 'Fanelli',
                firstName: 'Cristiano',
                name: 'Cristiano Fanelli',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12181,
            sessionSlotId: 2744,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Artificial Intelligence and Machine Learning for EPIC: an Overview',
            uniqueId: 'c17189',
            url: '/event/459/contributions/11720/',
          },
          c17190: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11746/attachments/9716/14215/TMVA_SOFIE_%20CHEP23.pdf',
                  id: 14215,
                  title: 'TMVA_SOFIE_ CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11746,
            description:
              'The recent developments in ROOT/TMVA focus on fast machine learning inference, which enables analysts to deploy their machine learning models rapidly on large scale datasets. A new tool has been recently developed, SOFIE, allowing for generating C++ code for evaluation of deep learning models, which are trained from external tools such as Tensorflow or PyTorch.  \r\nWhile Python-based deep learning frameworks for training models in GPU environments develop and mature, SOFIE is a  good solution that allows easy integration of inference of trained models into conventional C++ and CPU-based scientific computing workflows. Using this new tool, SOFIE, it is easier to integrate Machine Learning model evaluation in HEP data analysis and in particular when using tools such as  RDataFrame.\r\nWe  will present the recent developments of SOFIE,  notably one of  the latest features, the support for Graph Neural Networks. In the previous CHEP conference we have introduced SOFIE showing the support for some basic Deep learning operators. Now we have extended the support for parsing and generating C++ code for  several deep learning operators commonly  used in HEP and represented by the ONNX standard.  Other types of architectures typically used in HEP are the Graph Neural Networks. These networks cannot easily and efficiently be represented with ONNX operators. Therefore we have developed a set of C++ classes that can represent message passing GNN architectures, which are created with common tools used in HEP such as PyTorch geometric and the Graph Nets library from DeepMind.  From these classes it is then possible to generate efficient C++ code for inference of GNN\u2019s which can be easily integrated in CPU based workflows. \r\nWe demonstrate these current capabilities of SOFIE with benchmarks in evaluating some machine leaning models used by LHC experiments.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 478,
            id: 'c17190',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11746/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Moneta, Lorenzo'],
                emailHash: 'f93807550591a4b3698b7ee42329e540',
                familyName: 'Moneta',
                firstName: 'Lorenzo',
                name: 'Lorenzo Moneta',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12181,
            sessionSlotId: 2744,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'New developments of TMVA/SOFIE: Code Generation and Fast Inference for Graph Neural Networks',
            uniqueId: 'c17190',
            url: '/event/459/contributions/11746/',
          },
          c17191: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11735/attachments/9694/14143/2023-05-11%20CHEP%20CL-ALTMIN.pdf',
                  id: 14143,
                  title: '2023-05-11 CHEP CL-ALTMIN.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11735,
            description:
              'Neural Networks (NN) are often trained offline on large datasets and deployed on specialized hardware for inference, with a strict separation between training and inference. However, in many realistic applications the training environment differs from the real world or data arrive in a streaming fashion and are continuously changing. In these scenarios, the ability to continuously train and update NN models is desirable.\r\n\r\nContinual learning (CL) algorithms allow training of models over a stream of data. CL algorithms are often designed to work in constrained settings,  such as limited memory and computational power, or limitations on the ability to store past data (e.g., due to privacy concerns or memory requirements). The most basic online learning suffers from \u201ccatastrophic forgetting\u201d, where knowledge from initial or previous training is lost. CL aims to mitigate this effect through the use of different learning algorithms. \r\n\r\nHigh-energy physics experiments are developing intelligent detectors, with algorithms running on computer systems located close to the detector to meet the challenges of increased data rates and occupancies. The use of NN algorithms in this context is limited by changing detector conditions, such as degradation over time or failure of an input signal which might cause the NNs to lose accuracy leading, in the worst case, to loss of interesting events.\r\n\r\nCL has the potential to solve this issue, using large amounts of continuously streaming data to allow the network to recognize changes to learn and adapt to detector conditions. It has the potential to outperform traditional NN training techniques as not all possible scenarios can be predicted and modeled in static training data samples.\r\n\r\nHowever, NN training is computationally expensive and when combined with the strict timing requirements of embedded processors deployed close to the detector, current state-of-the-art offline approaches cannot be directly applied in real-time systems. Alternatives to typical backpropagation-based training that can be deployed on FPGAs for real-time data processing are presented, and their computational and accuracy characteristics are discussed in the context of HL-LHC.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 166,
            id: 'c17191',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11735/contribution.pdf',
            presenters: [
              {
                affiliation: 'Imperial College London',
                displayOrderKey: [0, 'Barbone, Marco'],
                emailHash: '67f6e235a1ea8baeb03e2fbe5fd43fd9',
                familyName: 'Barbone',
                firstName: 'Marco',
                name: 'Marco Barbone',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12181,
            sessionSlotId: 2744,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Embedded Continual Learning for HEP',
            uniqueId: 'c17191',
            url: '/event/459/contributions/11735/',
          },
          c17192: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11760/attachments/9724/14202/CHEP_FAIR4HEP_05112023.pdf',
                  id: 14202,
                  title: 'CHEP_FAIR4HEP_05112023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11760,
            description:
              'The findable, accessible, interoperable, and reusable (FAIR) data principles have provided a framework for examining, evaluating, and improving how we share data with the aim of facilitating scientific discovery. Efforts have been made to generalize these principles to research software and other digital products. Artificial intelligence (AI) models---algorithms that have been trained on data rather than explicitly programmed---are an important target for this because of the ever-increasing pace with which AI is transforming scientific and engineering domains. \r\n\r\nWe propose a practical definition of FAIR principles for AI models, create a FAIR AI project template that promotes adherence to these principles, and introduce a framework to quantify whether an AI model is FAIR. We demonstrate how to implement these principles using a concrete example from experimental high energy physics: a graph neural network for identifying Higgs bosons decaying to bottom quarks. We study the robustness of these FAIR AI models and their portability across hardware architectures and software frameworks and report new insights on the interpretability of AI predictions by studying the interplay between FAIR datasets and AI models. Enabled by publishing FAIR AI models, these studies pave the way toward reliable and automated AI-driven scientific discovery.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 82,
            id: 'c17192',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11760/contribution.pdf',
            presenters: [
              {
                affiliation: 'UCSD',
                displayOrderKey: [8, 'Li, Haoyang'],
                emailHash: 'c4d3589c74be2a751d68393e579b9c94',
                familyName: 'Li',
                firstName: 'Haoyang',
                name: 'Haoyang Li',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12181,
            sessionSlotId: 2744,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'FAIR AI Models in High Energy Physics',
            uniqueId: 'c17192',
            url: '/event/459/contributions/11760/',
          },
          c17193: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11740/attachments/9705/14164/hls4sr_chep.pdf',
                  id: 14164,
                  title: 'hls4sr_chep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11740,
            description:
              'The high-energy physics community is investigating the feasibility of deploying more machine-learning-based solutions on FPGAs to meet modern physics experiments\u0027 sensitivity and latency demands. In this contribution, we introduce a novel end-to-end procedure that utilises a forgotten method in machine learning, i.e. symbolic regression (SR). It searches equation space to discover algebraic relations approximating a dataset. We use PySR (software for uncovering these expressions based on evolutionary algorithms) and extend the functionality of hls4ml (a package for machine learning inference in FPGAs) to support PySR-generated expressions for resource-constrained production environments. Deep learning models often optimise the top metric by pinning the network size because vast hyperparameter space prevents extensive neural architecture search. Conversely, SR selects a set of models on the Pareto front, which allows for optimising the performance-resource tradeoff directly. By embedding symbolic forms, our implementation can dramatically reduce the computational resources needed to perform critical tasks. We validate our procedure on multiple physics benchmarks as an alternative to deep learning and decision tree models.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 532,
            id: 'c17193',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11740/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Wisconsin Madison',
                displayOrderKey: [1, 'Tsoi, Ho Fung'],
                emailHash: 'b31fba1b4ba5392019d5e8445e762417',
                familyName: 'Tsoi',
                firstName: 'Ho Fung',
                name: 'Mr Ho Fung Tsoi',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12181,
            sessionSlotId: 2744,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Symbolic Regression on FPGAs for Fast Machine Learning Inference',
            uniqueId: 'c17193',
            url: '/event/459/contributions/11740/',
          },
        },
        entryType: 'Session',
        friendlyId: 13,
        id: 's12181',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2043/session-timetable.pdf',
        room: 'Hampton Roads VII',
        sessionCode: '',
        sessionId: 2043,
        sessionSlotId: 2744,
        slotTitle: 'General Methods and Tools',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#03070f',
        title: 'Track 9 - Artificial Intelligence and Machine Learning',
        uniqueId: 's12181',
        url: '/event/459/sessions/2043/',
      },
      s12183: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#4f144e',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'Nikhef National institute  for subatomic physics (NL)',
            displayOrderKey: [0, 'Aaij, Roel'],
            emailHash: '6fecef0aff50325ef5ea685927dec4a5',
            familyName: 'Aaij',
            firstName: 'Roel',
            name: 'Roel Aaij',
          },
          {
            affiliation: 'Fermi National Accelerator Laboratory',
            displayOrderKey: [0, 'Timm, Steven'],
            emailHash: '40848b6b99ba5ca40be682891f2cc812',
            familyName: 'Timm',
            firstName: 'Steven',
            name: 'Steven Timm',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17241: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11804/attachments/9642/14209/EJFAT-chep2023.3.pptx',
                  id: 14209,
                  title: 'EJFAT-chep2023.3.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11804,
            description:
              'To increase the science rate for high data rates/volumes, Thomas Jefferson National Accelerator Facility (JLab) has partnered with Energy Sciences Network (ESnet) to define an edge to data center traffic shaping/steering transport capability featuring data event-aware network shaping and forwarding.\r\n\r\nThe keystone of this ESnet JLab FPGA Accelerated Transport (EJFAT) is the joint development of a dynamic compute work Load Balancer (LB) of UDP streamed data. The LB\u0027s centerpiece is a Field Programmable Gate Array (FPGA). The FPGA executes a dynamically configurable, low fixed latency, LB data plane featuring real-time packet redirection at high throughput. It also executes a control plane running on its host computer that monitors network and compute farm telemetry in order to make dynamic AI/ML guided decisions. These decisions include determining destination compute host redirection / load balancing.\r\n\r\nThe LB provides for three forms of scaling. It provides horizontal scale by adding more FPGAs for increased bandwidth. Second it sets the number of core compute hosts independent of the number of source DAQs. Thirdly it allows for a flexible number of CPUs and threads per host, treating each receiving thread as an independent LB destination. The LB provides seamless integration of edge / core computing to support direct experimental data processing.Immediate use will be in JLab science programs and others such as the EIC (Electron Ion Collider). Data centers of the future will need high throughput and low latency for both live streamed and recorded data for running experiment data acquisition analysis and data center use cases.\r\n\r\nEJFAT is in development for production use within DOE. When completed, it will have an operational impact for integrated research infrastructure as called for in planning documents for Exascale, Nuclear Physics, and Scientific Computing. It demonstrates a new load balancing architecture.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 314,
            id: 'c17241',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11804/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Goodrich, Michael'],
                emailHash: '0314d1a929136f81bb9e49ef1d9802b1',
                familyName: 'Goodrich',
                firstName: 'Michael',
                name: 'Michael Goodrich',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12183,
            sessionSlotId: 2746,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'EJFAT: Accelerated Intelligent Compute Destination Load Balancing',
            uniqueId: 'c17241',
            url: '/event/459/contributions/11804/',
          },
          c17242: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11826/attachments/9660/14078/CHEP-FPGA%20(3).pdf',
                  id: 14078,
                  title: 'CHEP-FPGA.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11826,
            description:
              'Field Programmable Gate Arrays (FPGAs) are playing an increasingly important role in the sampling and data processing industry due to their intrinsically highly parallel architecture, low power consumption, and flexibility to execute custom algorithms. In particular, the use of FPGAs to perform machine learning inference is increasingly growing thanks to the development of high-level synthesis projects that abstract the complexity of HDL programming.\r\nIn this presentation we will describe our experience extending KServe predictors, an emerging standard for ML (Machine Learning) model inference as a service on kubernetes. This project will support a custom workflow capable of loading and serving models on-demand on top of FPGAs. A key aspect is that the proposed approach makes the firmware generation transparent, often an obstacle to a widespread FPGA adoption. We will detail how the proposed system automates both the synthesis of the HDL code and the generation of the firmware, starting from a high-level language and user-friendly machine learning libraries. The ecosystem is then completed with the adoption of a common language for sharing user models and firmwares, that is based on a dedicated Open Container Initiative artifact definition, thus leveraging all the well established practices on managing resources on a container registry.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 152,
            id: 'c17242',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11826/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Perugia',
                displayOrderKey: [1, 'Ciangottini, Diego'],
                emailHash: '77ca614431d9bb97101b37318d192959',
                familyName: 'Ciangottini',
                firstName: 'Diego',
                name: 'Diego Ciangottini',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12183,
            sessionSlotId: 2746,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'KServe inference extension for a FPGA vendor-free ecosystem',
            uniqueId: 'c17242',
            url: '/event/459/contributions/11826/',
          },
          c17243: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11816/attachments/9537/14083/May_11_SONIC_CHEP.pdf',
                  id: 14083,
                  title: 'May_11_SONIC_CHEP.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11816,
            description:
              'Computing demands for large scientific experiments, such as the CMS experiment at CERN, will increase dramatically in the next decades. To complement the future performance increases of software running on CPUs, explorations of coprocessor usage in data processing hold great potential and interest. We explore the novel approach of Services for Optimized Network Inference on Coprocessors (SONIC) and study the deployment of this as-a-Service approach in large-scale data processing. In this setup, the main CMS Mini-AOD creation workflow is executed on CPUs, while several machine learning (ML) inference tasks are offloaded onto (remote) coprocessors, such as GPUs. With experiments performed at Google Cloud, the Purdue Tier-2 computing center, and combinations of the two, we demonstrate the acceleration of these ML algorithms individually on coprocessors and the corresponding throughput improvement for the entire workflow. We also show that this approach can be easily generalized to different types of coprocessors, and even deployed on local CPUs without performance decrease. We emphasize that SONIC enables high coprocessor usage and brings the portability to run workflows on different types of coprocessors.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 247,
            id: 'c17243',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11816/contribution.pdf',
            presenters: [
              {
                affiliation: 'MIT',
                displayOrderKey: [1, 'McCormack, William'],
                emailHash: 'b911a3dad48aadbd1ec32bbf738bd8c2',
                familyName: 'McCormack',
                firstName: 'William',
                name: 'William McCormack',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12183,
            sessionSlotId: 2746,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Portable Acceleration of CMS Mini-AOD Production with Coprocessors as a Service',
            uniqueId: 'c17243',
            url: '/event/459/contributions/11816/',
          },
          c17244: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11824/attachments/9281/14171/20230511-CHEP23_CMSPortability.pdf',
                  id: 14171,
                  title: '20230511-CHEP23_CMSPortability.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11824,
            description:
              'In the past years the landscape of tools for expressing parallel algorithms in a portable way across various compute accelerators has continued to evolve significantly. There are many technologies on the market that provide portability between CPU, GPUs from several vendors, and in some cases even FPGAs. These technologies include C++ libraries such as Alpaka and Kokkos, compiler directives such as OpenMP, the SYCL open specification that can be implemented as a library or in a compiler, and standard C++  where the compiler is solely responsible for the offloading. Given this developing landscape, users have to choose the technology that best fits their applications and constraints. For example, in the CMS experiment the experience so far in heterogeneous reconstruction algorithms suggests that the full application contains a large number of relatively short computational kernels and memory transfer operations. In this work we use a stand-alone version of the CMS heterogeneous pixel reconstruction code as a realistic use case of HEP reconstruction software that is capable of leveraging GPUs effectively. We summarize the experience of porting this code base from CUDA to Alpaka, Kokkos, SYCL, std::par, and OpenMP offloading. We compare the event processing throughput achieved by each version on NVIDIA, AMD, and Intel GPUs as well as on a CPU, and compare those to what a native version of the code achieves on each platform.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 80,
            id: 'c17244',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11824/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermi National Accelerator Laboratory',
                displayOrderKey: [1, 'Kortelainen, Matti'],
                emailHash: '256b1809c77f751f332ca80c677a9e31',
                familyName: 'Kortelainen',
                firstName: 'Matti',
                name: 'Dr Matti Kortelainen',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12183,
            sessionSlotId: 2746,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'Evaluating Performance Portability with the CMS Heterogeneous Pixel Reconstruction code',
            uniqueId: 'c17244',
            url: '/event/459/contributions/11824/',
          },
          c17245: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11812/attachments/9206/14093/CHEP2023-clustering-final.pdf',
                  id: 14093,
                  title: 'CHEP2023-clustering-final.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11812,
            description:
              'We report the implementation details, commissioning results, and physics performances of a two-dimensional cluster finder for reconstructing hit positions in the new vertex pixel detector (VELO) that is part of the LHCb Upgrade. The associated custom VHDL firmware has been deployed to the existing FPGA cards that perform the readout of the VELO and fully commissioned during the start of LHCb Run 3 data taking. This work represents a further enhancement of the DAQ system, reconstructing VELO hits coordinates on-the-fly, in real time, at the LHC collision rate, and it is part of a wider effort aimed at boosting the real-time processing capability of HEP experiments by delegating intensive tasks to dedicated computing accelerators deployed at the earliest stages of the data acquisition chain. The end result is a DAQ throughput increase in excess of 11%, together with a corresponding drop in electrical power consumption, as the FPGA implementation requires O(50x) less power with respect to the GPU implementation. The tracking performance of this novel system being indistinguishable from a full-fledged software implementation, allows the raw pixel data to be dropped immediately at the readout level, yielding the additional benefit of a 14% reduction in data flow.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 522,
            id: 'c17245',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11812/contribution.pdf',
            presenters: [
              {
                affiliation: 'SNS and INFN Pisa',
                displayOrderKey: [1, 'Bassi, Giovanni'],
                emailHash: '47013f62479e027171e7145e940fc6b6',
                familyName: 'Bassi',
                firstName: 'Giovanni',
                name: 'Giovanni Bassi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12183,
            sessionSlotId: 2746,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'FPGA-based real-time cluster finding for the LHCb silicon pixel detector',
            uniqueId: 'c17245',
            url: '/event/459/contributions/11812/',
          },
          c17246: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11827/attachments/9668/14279/CHEP2023_APEIRON_INFN.pdf',
                  id: 14279,
                  title: 'CHEP2023_APEIRON_INFN.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11827,
            description:
              'High Energy Physics (HEP) Trigger and Data Acquisition systems (TDAQs) need ever increasing throughput and real-time data analytics capabilities either to improve particle identification accuracy and further suppress background events in trigger systems or to perform an efficient online data reduction for trigger-less ones.\r\nAs for the requirements imposed by HEP TDAQs applications in the class of real-time dataflow processing, FPGA devices are a good fit in as much they can provide not only adequate computing, memory and I/O resources but also a smooth programming experience thanks to the availability of High-Level Synthesis (HLS) tools.\r\nThe main motivation for the design and development of the APEIRON framework is that the currently available HLS tools do not natively support the deployment of applications over multiple FPGA devices, which severely chokes the scalability of problems that this approach could tackle. To overcome this limitation, we envisioned APEIRON as an extension of the Xilinx Vitis framework able to support a network of FPGA devices interconnected by a low-latency direct network as the reference execution platform.\r\nDevelopers can define scalable applications, using a dataflow programming model inspired by Kahn Process Networks, that can be efficiently deployed on a multi-FPGAs system: the APEIRON communication IPs allow low-latency communication between processing tasks deployed on FPGAs, even if they are hosted on different computing nodes. Thanks to the use of HLS tools in the workflow, processing tasks are described in C++ as HLS kernels, while communication between tasks is expressed through a lightweight C++ API based on non-blocking *send()* and blocking *receive()* operations',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 500,
            id: 'c17246',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11827/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Roma1',
                displayOrderKey: [4, 'Rossi, Cristian'],
                emailHash: 'e33799f3f4b09a7f8818bdd793ca2000',
                familyName: 'Rossi',
                firstName: 'Cristian',
                name: 'Cristian Rossi',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12183,
            sessionSlotId: 2746,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'APEIRON: a Framework for High Level Programming of Dataflow Applications on Multi-FPGA Systems',
            uniqueId: 'c17246',
            url: '/event/459/contributions/11827/',
          },
        },
        entryType: 'Session',
        friendlyId: 15,
        id: 's12183',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2045/session-timetable.pdf',
        room: 'Marriott Ballroom VII',
        sessionCode: '',
        sessionId: 2045,
        sessionSlotId: 2746,
        slotTitle: 'FPGA and Inference Servers',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffefff',
        title:
          'Track X - Exascale Science, Heterogeneous Computing and Accelerators, and Quantum Computing',
        uniqueId: 's12183',
        url: '/event/459/sessions/2045/',
      },
      s12185: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d0c296',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Shahoyan, Ruben'],
            emailHash: '1db06f137fc13354d671d2e50bcdf999',
            familyName: 'Shahoyan',
            firstName: 'Ruben',
            name: 'Ruben Shahoyan',
          },
          {
            affiliation: 'KEK',
            displayOrderKey: [0, 'Yamada, Satoru'],
            emailHash: 'f2ae8a15fd4a3fafb55ff71831b9c00c',
            familyName: 'Yamada',
            firstName: 'Satoru',
            name: 'Satoru Yamada',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17029: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11391/attachments/9641/14232/CHEP.pptx',
                  id: 14232,
                  title: 'CHEP.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11391,
            description:
              'The Liquid Argon Calorimeters are employed by ATLAS for all electromagnetic calorimetry in the pseudo-rapidity region |\u03b7| \u003c 3.2, and for hadronic and forward calorimetry in the region from |\u03b7| = 1.5 to |\u03b7| = 4.9. They also provide inputs to the first level of the ATLAS trigger. After successful period of data taking during the LHC Run-2 between 2015 and 2018 the ATLAS detector entered into the a long period of shutdown. In 2022 the LHC has restarted a new data taking period the Run-3 period should see an increase of luminosity and pile-up up to 80 interaction per bunch crossing.\r\n\r\nTo cope with this harsher conditions, a new trigger readout path has been installed during the long shutdown. This new path should improve significantly the triggering performances on electromagnetic objects. This will be achieved by increasing the granularity of the objects available at trigger level by up to a factor of ten.\r\n\r\nThe installation of this new trigger readout chain required also the update of the legacy system. More than 1500 boards of the precision readout have been extracted from the ATLAS pit, refurbished and re-installed. The legacy analog trigger readout that will remain during the LHC Run-3 as a backup of the new digital trigger system has also been updated.\r\n\r\nFor the new system 124 new on-detector boards have been added. Those boards that are operating in a radiative environment are digitizing the calorimeter trigger signals at 40MHz. The digital signal is sent to the off-detector system and processed online to provide the measured energy value for each unit of readout. In total up to 31Tbps are analyzed by the processing system and more than 62Tbps are generated for downstream reconstruction. To minimize the triggering latency the processing system had to be installed underground. The limited available space imposed a very compact hardware structure. To achieve a compact system, large FPGAs with high throughput have been mounted on ATCA mezzanines cards. In total no more than 3 ATCA shelves are used to process the signal from approximately 34000 channels.\r\n\r\nGiven that modern technologies have been used compared to the previous system, all the monitoring and control infrastructure is being adapted and commissioned as well.\r\n\r\nThis contribution will present the challenges of the installation, the commissioning and the milestones still to be completed towards the full operation of both the legacy and the new readout paths for the LHC Run-3.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 46,
            id: 'c17029',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11391/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Tokyo',
                displayOrderKey: [1, 'Furukawa, Marin'],
                emailHash: '7420248a7cfaa453d09db9c12a26e667',
                familyName: 'Furukawa',
                firstName: 'Marin',
                name: 'Marin Furukawa',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12185,
            sessionSlotId: 2748,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'ATLAS LAr Calorimeter Commissioning for LHC Run-3',
            uniqueId: 'c17029',
            url: '/event/459/contributions/11391/',
          },
          c17030: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11380/attachments/9661/14085/20230511_chep_alice_readout_membuf.mp4',
                  id: 14085,
                  title: '20230511_chep_alice_readout_membuf.mp4',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11380/attachments/9661/14082/20230511_chep_alice_readout.pdf',
                  id: 14082,
                  title: '20230511_chep_alice_readout.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11380,
            description:
              'ALICE (A Large Ion Collider Experiment) is a heavy-ion detector studying the physics of strongly interacting matter and the quark-gluon plasma at the CERN LHC (Large Hadron Collider). During the second long shut-down of the LHC, the ALICE detector was upgraded to cope with an interaction rate of 50 kHz in Pb-Pb collisions, producing in the online computing system (O2) a sustained input throughput of 3 TB/s. \r\n \r\n  In the past years, the O2/FLP project built the new data-acquisition system capable of handling this load. It consists of 200 readout nodes, collecting the data transferred from over 8000 detector links to PCs memory by dedicated PCI boards. The readout software manages the hardware and software memory buffers used for DMA and inter-process communication. It initiates the data flow, performs on-the-fly consistency checks, formats the data, reports performance, and finally pushes the data to the local processing pipeline. The output is then sent by the data distribution software over 100Gb/s links to a dedicated event processing farm.\r\n\r\n  The readout software modular design allowed to address the manifold needs faced during the prototyping, installation and commissioning phases, which proved essential from the lab tests to physics production, like file replay and recording, or online multi-threaded LZ4 compression.\r\n\r\n  We will describe the hardware and software implementation of the O2 readout system, and review the challenges met during the commissioning and first months of operation with LHC collisions in 2022.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 51,
            id: 'c17030',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11380/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Chapeland, Sylvain'],
                emailHash: 'b6a438aa14369eee8583315267b7df18',
                familyName: 'Chapeland',
                firstName: 'Sylvain',
                name: 'Sylvain Chapeland',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12185,
            sessionSlotId: 2748,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Commissioning of the ALICE readout software for LHC Run 3',
            uniqueId: 'c17030',
            url: '/event/459/contributions/11380/',
          },
          c17031: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11369/attachments/9652/14235/CHEP23_SalvadorVentura.pdf',
                  id: 14235,
                  title: 'CHEP23_SalvadorVentura.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11369,
            description:
              'A new era of hadron collisions will start around 2029 with the High-Luminosity LHC which will allow to collect ten times more data than what has been collected during 10 years of operation at LHC. This will be achieved by higher instantaneous luminosity at the price of higher number of collisions per bunch crossing.\r\n\r\nIn order to withstand the high expected radiation doses and the harsher data taking conditions, the ATLAS Liquid Argon Calorimeter readout electronics will be upgraded.\r\n\r\nThe electronic readout chain is composed of four main components.\r\n1: New front-end boards will allow to amplify, shape and digitise the calorimeter\u2019s ionisation signal on two gains over a dynamic range of 16 bits and 11 bit precision. Low noise below Minimum Ionising Particle (MIP), i.e. below 120 nA for 45 ns peaking time, and maximum non-linearity of two per mille are required. Custom preamplifiers and shapers are being developed to meet these requirements using 65 nm and 130 nm CMOS technologies. They shall be stable under irradiation until 1.4kGy (TID) and 4.1x10^13 new/cm^2 (NIEL). Two concurrent preamp-shaper ASICs were developed and, \u201cALFE\u201d, the best one has been chosen. The test results of the latest version of this ASIC will be presented. \u201cCOLUTA\u201d, a new ADC chip is also being designed. A production test setup is being prepared and integration tests of the different components (including lpGBT links developed by CERN) on a 32-channels front-end board are ongoing, and results of this integration will be shown.\r\n2: New calibration boards will allow the precise calibration of all 182468 channels of the calorimeter over a 16 bits dynamic range. A non-linearity of one per mille and non-uniformity between channels of 0.25% with a pulse rise time smaller than 1ns shall be achieved. In addition, the custom calibration ASICs shall be stable under irradiation with same levels as preamp-shaper and ADC chips. The HV SOI CMOS XFAB 180nm technology is used for the pulser ASIC, \u201cCLAROC\u201d, while the TSMC 130 nm technology is used for the DAC part, \u201cLADOC\u201d. The latest versions of those 2 ASICs which recently passed the production readiness review (PDR) with their respective performances will be presented.\r\n3: New ATCA compliant signal processing boards (\u201cLASP\u201d) will receive the detector data at 40 MHz where FPGAs connected through lpGBT high-speed links will perform energy and time reconstruction. In total, the off-detector electronics receive 345 Tbps of data via 33000 links at 10 Gbps. For the first time, online machine learning techniques are considered to be used in these FPGAs. A subset of the original data is sent with low latency to the hardware trigger system, while the full data are buffered until the reception of trigger accept signals. The latest development status of the board as well as the firmware will be shown.\r\n4: A new timing and control system, \u201cLATS\u201d, will synchronise to the aforementioned components. Its current design status will also be shown.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 47,
            id: 'c17031',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11369/contribution.pdf',
            presenters: [
              {
                affiliation: 'CEA-SACLAY',
                displayOrderKey: [1, 'Ventura Gonzalez, Salvador'],
                emailHash: '41b951f980f1dcb0e3fbbce330a7e54c',
                familyName: 'Ventura Gonzalez',
                firstName: 'Salvador',
                name: 'Salvador Ventura Gonzalez',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12185,
            sessionSlotId: 2748,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'Development of the ATLAS Liquid Argon Calorimeter Readout Electronics for the HL-LHC',
            uniqueId: 'c17031',
            url: '/event/459/contributions/11369/',
          },
          c17032: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11362/attachments/9702/14158/JHoya_FELIX_CHEP2023_Final.pdf',
                  id: 14158,
                  title: 'JHoya_FELIX_CHEP2023_Final.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11362,
            description:
              'Over the next decade, the ATLAS detector will be required to operate in an increasingly harsh collision environment. To maintain physics performance, the detector will undergo a series of upgrades during major shutdowns. A key goal of these upgrades is to improve the capacity and flexibility of the detector readout system. To this end, the Front-End Link eXchange (FELIX) system was developed as the new interface between the data acquisition; detector control and TTC (Timing, Trigger and Control) systems; and new or updated trigger and detector front-end electronics. FELIX functions as a router between custom serial links from front end ASICs and FPGAs to data collection and processing components via a commodity switched network. The serial links may aggregate many slower links or be a single high bandwidth link. FELIX also forwards the LHC bunch-crossing clock, fixed latency trigger accepts and resets received from the TTC system to front-end electronics. FELIX uses commodity server technology in combination with FPGA-based PCIe I/O cards. FELIX servers run a software routing platform serving data to network clients performing a number of data preparation, monitoring and control functions. \r\n\r\nThis presentation covers the design of FELIX as well as the first operational experience gained during the Run 3 starting, including the challenges faced commissioning the system for each ATLAS sub-detector. Finally, the planned evolution of FELIX for High-Luminosity LHC will be described, including architectural changes and status of early integration with detector development projects.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 303,
            id: 'c17032',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11362/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Hoya, Joaquin'],
                emailHash: '892346529690f591a6a05c28be5e7521',
                familyName: 'Hoya',
                firstName: 'Joaquin',
                name: 'Joaquin Hoya',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12185,
            sessionSlotId: 2748,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'FELIX: first operational experience with the new ATLAS readout system and perspectives for HL-LHC',
            uniqueId: 'c17032',
            url: '/event/459/contributions/11362/',
          },
          c17033: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11397/attachments/9679/14118/ersap-chep-23.pptx',
                  id: 14118,
                  title: 'ersap-chep-23.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11397,
            description:
              'The volume and complexity of data produced at HEP and NP research facilities have grown exponentially. There is an increased demand for new approaches to process data in near-real-time. In addition, existing data processing architectures need to be reevaluated to see if they are adaptable to new technologies. A unified programming model for event processing and distribution that can exploit parallelism and heterogeneity in the computing environment still needs to be developed. This paper investigates the benefits of blending Flow-Based Programming with the Reactive Actor Model for building distributed, reactive, and high-performance data stream processing applications. In this paper, we present the design concepts of the ERSAP framework for building such applications. The results of using ERSAP in the recent beam test of the EIC prototype calorimeter at DESY and a GEM detector at JLAB will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 317,
            id: 'c17033',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11397/contribution.pdf',
            presenters: [
              {
                affiliation: 'Jefferson Lab',
                displayOrderKey: [1, 'Gyurjyan, Vardan'],
                emailHash: 'd96010739664021f1bf2e72848130026',
                familyName: 'Gyurjyan',
                firstName: 'Vardan',
                name: 'Vardan Gyurjyan',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12185,
            sessionSlotId: 2748,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title: 'Streaming Readout and Data-Stream Processing With ERSAP',
            uniqueId: 'c17033',
            url: '/event/459/contributions/11397/',
          },
          c17286: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11381/attachments/9612/14157/2023_05_11%20Dinardo.pdf',
                  id: 14157,
                  title: '2023_05_11 Dinardo.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d0c296',
            conferenceId: 459,
            contributionId: 11381,
            description:
              'To improve the potential for discoveries at the LHC, a significant luminosity increase of the accelerator (HL-LHC) is foreseen in the late 2020s, to achieve a peak luminosity of 7.5x10^34 cm^-2 s^-1, in the ultimate performance scenario. HL-LHC is expected to run with a bunch-crossing separation of 25 ns and a maximum average of 200 events (pile-up) per crossing. To maintain or even improve the performance of CMS in this harsh environment, the detector will undergo several upgrades in the next years. In particular, the Inner Tracker is being completely redesigned featuring a frontend chip with a data readout speed of 1.28 Gbps, and a downlink for clock, trigger, and commands of 160 Mbps. The communication between the frontend and the backend electronics occurs through an optical link based on a custom Low-power Gigabit Transceiver which sends data at 10 and 2.5 Gbps on the uplink and downlink, respectively. The number of pixels has been increased by x6 with respect to the present detector, covering a larger pseudorapidity region up to 4, resulting in an unprecedented number of channels of about two billion. This represents a challenging requirement for the data acquisition system since it needs to efficiently configure, monitor, and calibrate them. A dedicated data acquisition system, written in C++ and based on a custom micro Data, Trigger, and Control board, equipped with an FPGA, was developed to fully test and characterize the pixel modules both on a bench and with beam tests. In this note, we will describe the system architecture and its scalability to the final system which will be based on custom back-end boards equipped with FPGAs and CPUs.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 19,
            id: 'c17286',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11381/contribution.pdf',
            presenters: [
              {
                affiliation: 'Universit\u00e0 degli Studi di Milano - Bicocca',
                displayOrderKey: [1, 'Dinardo, Mauro'],
                emailHash: '08d7f3ad1ef8d0a37492d5b96b5b888e',
                familyName: 'Dinardo',
                firstName: 'Mauro',
                name: 'Mauro Dinardo',
              },
            ],
            references: [],
            room: 'Marriott Ballroom V-VI',
            sessionCode: '',
            sessionId: 2036,
            sessionSlotEntryId: 12185,
            sessionSlotId: 2748,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#000000',
            title:
              'The CMS Inner Tracker DAQ system for the High Luminosity upgrade of LHC: from single-chip testing, to large-scale assembly qualification',
            uniqueId: 'c17286',
            url: '/event/459/contributions/11381/',
          },
        },
        entryType: 'Session',
        friendlyId: 6,
        id: 's12185',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2036/session-timetable.pdf',
        room: 'Marriott Ballroom V-VI',
        sessionCode: '',
        sessionId: 2036,
        sessionSlotId: 2748,
        slotTitle: 'Readout \u0026 Data Processing',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#000000',
        title: 'Track 2 - Online Computing',
        uniqueId: 's12185',
        url: '/event/459/sessions/2036/',
      },
      s12186: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#d9dfc3',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Pittsburgh',
            displayOrderKey: [0, 'Bandieramonte, Marilena'],
            emailHash: 'f4b45dbfcfef5c90c1b484b28e777c71',
            familyName: 'Bandieramonte',
            firstName: 'Marilena',
            name: 'Marilena Bandieramonte',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'VALLECORSA, SOFIA'],
            emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
            familyName: 'VALLECORSA',
            firstName: 'SOFIA',
            name: 'SOFIA VALLECORSA',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '12:45:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17076: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11433/attachments/9659/14076/hrivnacova-chep23-final.pdf',
                  id: 14076,
                  title: 'hrivnacova-chep23-final.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11433,
            description:
              'The analysis category was introduced in Geant4 almost ten years ago (in 2014) with the aim to provide users with a lightweight analysis tool, available as part of the Geant4 installation without the need to link to an external analysis package. It helps capture statistical data in the form of histograms and n-tuples and store these in files in four various formats. It was already presented at CHEP multiple times, the last time five years ago. In this presentation we will give an update on its evolution since then.\r\n\r\nWe will present the major redesign in the past two years that allowed introducing a new Generic analysis manager. In particular, we will discuss the advantages of our design choice based on the so- called Non Virtual Interface pattern: the code robustness and stability in the context of the code evolution over almost ten years.\r\n\r\nWe will also report on new functionalities: a new factory class, Generic analysis manager, that provides more flexibility in the selection of the output file type, saving data in multiple formats from the same simulation run, then on the connection of the analysis to visualization or new support for data object cycles in the upcoming version Geant4 11.1. Finally, we will present the continuous code improvements using static code analysis and sanitizer tools.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 364,
            id: 'c17076',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11433/contribution.pdf',
            presenters: [
              {
                affiliation: 'IJCLab, IN2P3/CNRS',
                displayOrderKey: [1, 'HRIVNACOVA, Ivana'],
                emailHash: '362423a9c8c56740feb7cef9d347d9e0',
                familyName: 'HRIVNACOVA',
                firstName: 'Ivana',
                name: 'Ivana HRIVNACOVA',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12186,
            sessionSlotId: 2749,
            startDate: {
              date: '2023-05-11',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Analysis Tools in Geant4',
            uniqueId: 'c17076',
            url: '/event/459/contributions/11433/',
          },
          c17077: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11458/attachments/9560/13870/sawkey_chep2023_2.pdf',
                  id: 13870,
                  title: 'sawkey_chep2023_2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11458,
            description:
              'For the new Geant4 series 11.X electromagnetic (EM) physics sub-libraries were revised and reorganized in view of requirements for simulation of Phase-2 LHC experiments. EM physics simulation software takes a significant part of CPU during massive production of Monte Carlo events for LHC experiments.  We present recent evolution of Geant4 EM sub-libraries for simulation of gamma, electron, and positron transport. Updates of other components of EM physics are also discussed. These developments are included into the new Geant4 version 11.1 (December 2022). The most important modifications concern reorganization of initialization of EM physics and introduction of alternative tracking software. These modifications affect CPU efficiency of any simulation, CPU saving depends on geometry and physics configuration for concrete experimental setup. We will discuss several methods: gamma general process, Woodcock tracking, transportation with multiple scattering process, alternative tracking manager, and new G4HepEm library. These developments provide a ground for implementation of EM particle transport at co-processors and GPU. We also will present very recent updates in physics processes and in configuration of EM physics.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 445,
            id: 'c17077',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11458/contribution.pdf',
            presenters: [
              {
                affiliation: 'Varian Medical Systems',
                displayOrderKey: [4, 'Sawkey, Daren'],
                emailHash: '923c75fc698e208cc9b06e7c92c809af',
                familyName: 'Sawkey',
                firstName: 'Daren',
                name: 'Dr Daren Sawkey',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12186,
            sessionSlotId: 2749,
            startDate: {
              date: '2023-05-11',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Geant4 electromagnetic physics for Run3 and Phase2 LHC',
            uniqueId: 'c17077',
            url: '/event/459/contributions/11458/',
          },
          c17078: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11428/attachments/9613/13980/CHEP2023_DC_20230509.pdf',
                  id: 13980,
                  title: 'CHEP2023_DC_20230509.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11428,
            description:
              'The Circular Electron Positron Collider (CEPC) [1] is one of the future experiments aiming to study the Higgs boson\u2019s properties precisely. For this purpose, excellent track reconstruction and particle identification (PID) performance are required. Such as the tracking efficiency should be around 100%, the momentum resolution should be less than 0.1%, and the Kaon and pion should have 2 sigma separation power for momentum below 20 GeV. To fulfill these requirements, a tracking system combining a silicon tracker and a drift chamber is proposed for the CEPC experiment. Here the drift chamber is not only used for improving track reconstruction performance but also used for excellent PID with cluster counting method[2]. To evaluate the performance of this design carefully, the simulation should be close to the real situation as much as possible. \r\n\r\nThis contribution presents a refined drift chamber simulation by combining the Geant4 and the Garfield++ [3] simulation in the CEPCSW [4]. The Geant4 is used for traditional particle transportation and interaction simulation in the detector. The Garfield++ aims for detailed simulation in each drift chamber cell including precise ionization simulation, pulse simulation as well as waveform simulation. Due to the extremely time-consuming avalanche process simulation in Garfield++. It is not feasible to simulate the waveform of the whole drift chamber using Garfield++. To solve the barrier, a fast pulse simulation method based on the Normalizing Flow technology [5] is developed which can simulate the pulse\u2019s time and amplitude according to the local position of an ionized electron. The result shows the fast simulation has very high fidelity and more than 2 magnitude speed up can be achieved. To further validate this method, simulating drift time is performed using real data from the BESIII experiment [6]. It shows the simulated drift time distribution is consistent with real data. Last but not least, the track reconstruction performance is shown by using this more realistic drift chamber simulation.\r\n\r\nReference:\r\n[1] CEPC Study Group Collaboration, M. Dong et al., *CEPC Conceptual Design Report: Volume 2 - Physics \u0026 Detector*, arXiv:1811.10545 \r\n[2] Jean-Fran\u00b8cois Caron, et al., *Improved Particle Identification Using Cluster Counting in a Full-Length Drift Chamber Prototype*, 10.1016/j.nima.2013.09.028\r\n[3] Garfield++ Team, https://gitlab.cern.ch/garfield/garfieldpp, 2021. GitLab repository (2021).\r\n[4] CEPCSW Team, *CEPCSW prototype repository*, https://github.com/cepc/CEPCSW, 2021. GitHub repository (2021)\r\n[5] I. Kobyzev, S. J. D. Prince and M. A. Brubaker, \u201cNormalizing Flows: An Introduction and Review of Current Methods,\u201d in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 3964-3979, 1 Nov. 2021, doi: 10.1109/TPAMI.2020.2992934.\r\n[6] BESIII Collaboration, Design and Construction of the BESIII Detector. Nucl.Instrum.Meth.A614:345-399,2010',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 135,
            id: 'c17078',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11428/contribution.pdf',
            presenters: [
              {
                affiliation: 'IHEP',
                displayOrderKey: [1, 'Fang, Wenxing'],
                emailHash: '0ba49dd7ed9971d06f903d4576945778',
                familyName: 'Fang',
                firstName: 'Wenxing',
                name: 'Dr Wenxing Fang',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12186,
            sessionSlotId: 2749,
            startDate: {
              date: '2023-05-11',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Refined drift chamber simulation in the CEPC experiment',
            uniqueId: 'c17078',
            url: '/event/459/contributions/11428/',
          },
          c17079: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11451/attachments/9687/14227/Simulation%20of%20the%20MoEDAL-MAPP%20experiment%20at%20the%20LHC.pdf',
                  id: 14227,
                  title: 'Simulation of the MoEDAL-MAPP experiment at the LHC.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11451,
            description:
              'MoEDAL (the Monopole and Exotics Detector at the LHC) searches directly magnetic monopoles at the Interaction Point 8 of the Large Hadron Collider (LHC). As an upgrade of the experiment an addition, MAPP (MoEDAL Apparatus for Penetrating Particles) detector extends the physics reach by providing sensitivity to milli-charged and long-lived exotic particles. The MAPP detectors are scintillator detectors, and they are planned, or already installed in service tunnels of the LHC, locating from 50 to 100 meters from the IP8 [1].\r\n\r\nTo study and to support the data analysis of the detectors, a complete simulation model of the detector regions was developed. This Geant4 [2] based model consists of all tunnel and accelerator components between the detectors and the IP8, and the material budget, about 100 meters thick ground layer above the tunnels for cosmic background studies. In addition, new physics models describing the interactions of exotic particles, such as millicharged particles, has been implemented in the model.\r\n\r\nThe geometry description of the model utilises CAD to GDML conversion and parser of Geant4. This allowed to construct modular elements which can be changed without the need of recompiling the software code. In addition, the physics processes are handled in stacks which allows to limit the required computing resources. Various physics generators, such as Pythia8 [3], are used as a primary input.\r\n\r\nIn this contribution I will discuss about the development of the geometry models and present the new physics models and interactions that are used in the simulations. I will also show some examples of the detector response for various types of physics models.\r\n\r\n**References:**\r\n[1] B. Acharya et al, MoEDAL-MAPP, an LHC Dedicated Detector Search Facility, in Proceedings of 2022 Snowmass Summer Study (2022) arXiv:2209.03988 [hep-ph].\r\n[2] J. Allison et al., Recent developments in Geant4, Nucl. Instr. Meth. A 835 (2016) p. 186-225.\r\n[3] T. Sjostrand, The Pythia event generator: Past, present and future, Comput. Phys. Comm. 246 (2020) 106910 arXiv:1907.09874v1 [hep-ph].',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 344,
            id: 'c17079',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11451/contribution.pdf',
            presenters: [
              {
                affiliation: 'Helsinki Institute of Physics',
                displayOrderKey: [1, 'Kalliokoski, Matti'],
                emailHash: '344a346dab3f478e45568463c7396fc7',
                familyName: 'Kalliokoski',
                firstName: 'Matti',
                name: 'Matti Kalliokoski',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12186,
            sessionSlotId: 2749,
            startDate: {
              date: '2023-05-11',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Simulation of the MoEDAL-MAPP experiment at the LHC',
            uniqueId: 'c17079',
            url: '/event/459/contributions/11451/',
          },
          c17080: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11446/attachments/9675/14113/CHEP2023.key',
                  id: 14113,
                  title: 'CHEP2023.key',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11446/attachments/9675/14239/CHEP2023.pdf',
                  id: 14239,
                  title: 'CHEP2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11446,
            description:
              'FullSimLight is a lightweight, Geant4-based command line\r\nsimulation utility intended for studies of simulation performance. It\r\nis part of the GeoModel toolkit (geomodel.web.cern.ch) which has been\r\nstable for more than one year. The FullSimLight component\r\nhas recently undergone renewed development aimed at extending its\r\nfunctionality. It has been endowed with a GUI for fast, transparent,\r\nand foolproof configuration and with a plugin mechanism allowing users\r\nand developers with diverse goals to extend and customize the\r\nsimulation. Geometry and event input can be easily specified on the\r\nfly, allowing rapid evaluation of different geometry options and their\r\neffect on simulation performance. User actions and sensitive detectors\r\ncan also be loaded through the new plugin mechanism, allowing for\r\ncustomization of Geant4 processing and hit production. The geometry\r\nexplorer (gmex), in a parallel development, has been enhanced with\r\nthe capability of visualizing FullSimLight track and hit output.\r\nFullSimLight, brought to you by the ATLAS collaboration, is an\r\nexperiment independent software tool.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 428,
            id: 'c17080',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11446/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Pittsburgh',
                displayOrderKey: [1, 'Khan, Raees'],
                emailHash: '52d627c2e463bac0b1fece0bd0da7482',
                familyName: 'Khan',
                firstName: 'Raees',
                name: 'Mr Raees Khan',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12186,
            sessionSlotId: 2749,
            startDate: {
              date: '2023-05-11',
              time: '12:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Recent Developments in the FullSimLight Simulation Tool from ATLAS',
            uniqueId: 'c17080',
            url: '/event/459/contributions/11446/',
          },
          c17081: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11421/attachments/9375/14156/PhatS-CHEP2023-opt.pdf',
                  id: 14156,
                  title: 'PhatS-CHEP2023-opt.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#d9dfc3',
            conferenceId: 459,
            contributionId: 11421,
            description:
              'In this contribution we report status of the CMS Geant4 simulation and the prospects for Run-3 and Phase-2. \r\nFirstly, we report about our experience during the start of Run-3 with Geant4 10.7.2, the common software package DD4hep for geometry description, and VecGeom run time geometry library. In addition, FTFP_BERT_EMM Physics List and CMS configuration for tracking in magnetic field have been utilized. For the first time, for the Grid mass production of Monte Carlo, this combination of components is used.\r\nFurther simulation improvements are under development targeting Run-3 such as the switch to the new Geant4 11.1 in production, that provides several features important for the optimization of simulation, for example the new transportation process with built-in multiple scattering, neutron general process, custom tracking manager, G4HepEm sub-library, and others. \r\nWe will present evaluation of various options, validation results, and the final choice of simulation configuration for 2023 production and beyond. The performance of the CMS full simulation for Run-2 and Run-3 will also be discussed. \r\nCMS development plan for the Phase-2 Geant4 based simulation is very ambitious, and it includes a new geometry description, physics, and simulation configurations. The progress on new detector descriptions and full simulation will be presented as well as the R\u0026D in progress to reduce compute capacity needs.\r\nFinally, the status of the R\u0026D for using Celeritas and Adept GPU prototypes in CMSSW will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '12:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 438,
            id: 'c17081',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11421/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [0, 'Srimanobhas, Phat'],
                emailHash: '0645a80ac553e03e11b305b0095578fb',
                familyName: 'Srimanobhas',
                firstName: 'Phat',
                name: 'Phat Srimanobhas',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VI',
            sessionCode: '',
            sessionId: 2037,
            sessionSlotEntryId: 12186,
            sessionSlotId: 2749,
            startDate: {
              date: '2023-05-11',
              time: '12:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#272f09',
            title: 'Full Simulation of CMS for Run-3 and Phase-2',
            uniqueId: 'c17081',
            url: '/event/459/contributions/11421/',
          },
        },
        entryType: 'Session',
        friendlyId: 7,
        id: 's12186',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2037/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VI',
        sessionCode: '',
        sessionId: 2037,
        sessionSlotId: 2749,
        slotTitle: 'Simulation (part 3)',
        startDate: {
          date: '2023-05-11',
          time: '11:15:00',
          tz: 'US/Eastern',
        },
        textColor: '#272f09',
        title: 'Track 3 - Offline Computing',
        uniqueId: 's12186',
        url: '/event/459/sessions/2037/',
      },
      s12187: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#efebc2',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'SKAO',
            displayOrderKey: [0, 'Joshi, Rohini'],
            emailHash: '7fd55f634d18a43003ce815c039c70ab',
            familyName: 'Joshi',
            firstName: 'Rohini',
            name: 'Rohini Joshi',
          },
          {
            affiliation: 'STFC-RAL',
            displayOrderKey: [0, 'Ellis, Katy'],
            emailHash: 'e4c8a78292759108e5511297386cac3a',
            familyName: 'Ellis',
            firstName: 'Katy',
            name: 'Katy Ellis',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16872: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11488/attachments/9725/14207/EKaravakis-PanDARubin-CHEP2023-1.pdf',
                  id: 14207,
                  title: 'EKaravakis-PanDARubin-CHEP2023-1.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11488,
            description:
              'The Vera C. Rubin Observatory will produce an unprecedented astronomical data set for studies of the deep and dynamic universe. Its Legacy Survey of Space and Time (LSST) will image the entire southern sky every three days and produce tens of petabytes of raw image data and associated calibration data. More than 20 terabytes of data must be processed and stored every night for ten years.\r\n\r\nThe Production and Distributed Analysis (PanDA) system was evaluated by the Vera C. Rubin Observatory Data Management team and selected to serve the observatory\u2019s needs due to its demonstrated scalability and flexibility over the years, for its Directed Acyclic Graph (DAG) support, it\u2019s support for multi-site processing, and its highly scalable complex workflows via the intelligent Data Delivery Service (iDDS). PanDA is also being evaluated for prompt processing where data must be processed and alerts issued within 60 seconds.\r\n\r\nThis presentation will briefly describe the Vera C. Rubin Data Management system and its use at both the Interim Data Facility (IDF) hosted on the Google Cloud Platform (GCP) and the United States Data Facility (USDF) hosted at the SLAC Shared Scientific Data Facility (S3DF). Finally, it will describe in depth the work performed in order to integrate the PanDA system with the Vera Rubin Observatory to be able to run the Rubin Science Pipelines using PanDA.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 30,
            id: 'c16872',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11488/contribution.pdf',
            presenters: [
              {
                affiliation: 'Brookhaven National Laboratory',
                displayOrderKey: [0, 'Karavakis, Edward'],
                emailHash: '4a66eba8bd02dcd2372a3cd6255f9fc5',
                familyName: 'Karavakis',
                firstName: 'Edward',
                name: 'Dr Edward Karavakis',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12187,
            sessionSlotId: 2750,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'Integrating the PanDA Workload Management System with the Vera C. Rubin Observatory',
            uniqueId: 'c16872',
            url: '/event/459/contributions/11488/',
          },
          c16873: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11504/attachments/9541/13843/Rubin_processing_CC-IN2P3_CHEP%20v3.pdf',
                  id: 13843,
                  title: 'Rubin_processing_CC-IN2P3_CHEP v3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11504,
            description:
              'The Vera C. Rubin Observatory, currently in construction in Chile, will start performing the Large Survey of Space and Time (LSST) late 2024 for 10 years. Its 8.4-meter telescope will survey the southern sky in less than 4 nights in six optical bands, and repeatedly generate about 2000 exposures per night, corresponding to a data volume of about 20 TB every night. Three data facilities are preparing to contribute to the production of the annual data releases: the US Data Facility (USDF) will process 25% of the raw data, the UK data facility (UKDF)  will process 25% of the raw data and the French data facility (FrDF), operated by CC-IN2P3, that will locally process the remaining 50% of the raw data.\r\n\r\nIn the context of the Data Preview 0.2 (DP0.2), the Data Release Production (DRP) pipelines have been executed on the DC-2 simulated dataset (generated by the DESC collaboration, DESC). This dataset includes 20 000 simulated exposures, representing 300 square degrees of Rubin images with a typical depth of 5 years.\r\n\r\nDP0.2 ran at the interim data facility (based on Google cloud), and the full exercise was replicated at CC-IN2P3. During this exercise, 3 PiB of data and more than 200 million files have been produced. In this contribution we will present a detailed description of the system that we set up to perform this processing campaign using CC-IN2P3\u0027s computing and storage infrastructure. Several topics will be addressed: workflow generation and execution, batch job submission, memory and I/O requirements, operations, etc. We will focus on the issues that arose during this campaign and how they have been addressed and will present the lessons learnt from this exercise.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 521,
            id: 'c16873',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11504/contribution.pdf',
            presenters: [
              {
                affiliation: 'IN2P3 / CNRS computing center',
                displayOrderKey: [1, 'Hernandez, Fabio'],
                emailHash: '4c50bba9c5558ffddf6157ef5a6ad638',
                familyName: 'Hernandez',
                firstName: 'Fabio',
                name: 'Fabio Hernandez',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12187,
            sessionSlotId: 2750,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title:
              'The Rubin Observatory\u2019s Legacy Survey of Space and Time DP0.2 processing campaign at CC-IN2P3',
            uniqueId: 'c16873',
            url: '/event/459/contributions/11504/',
          },
          c16874: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11471/attachments/9689/14134/light%20weith%20distributing%20system%20for%20lhaaso.pdf',
                  id: 14134,
                  title: 'light weith distributing system for lhaaso.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11471,
            description:
              'The Large High Altitude Air Shower Observatory (LHAASO) is a large-scale astrophysics experiment led by China. The offline data processing was highly dependent on the Institute of High Energy Physics(IHEP) local cluster and the local file system. \r\nAs the LHAASO experimental cooperation groups\u2019 resources are located geographically and most of them have the characteristics of limited scale, low stability, and lack of human support, it is difficult to integrate them via Grid. We designed and developed a lightweight distributed computing system for LHAASO offline data processing. Unlike the grid model, the system keeps the IHEP cluster as the main cluster and extends the cluster to the worker nodes of the remote site. LHAASO jobs are submitted to the IHEP cluster and are dispatched to the remote worker node in the system.\r\nTokens are the authentication and authorization solution in the whole cluster, LHAASO computing tasks are classified into several types. Each type of job is wrapped by a dedicated script which helps the job have no direct access to the IHEP file system. The system draws on the idea of \u201cstartd automatic cluster joining\u201d of GlideinWMS but abandons the grid certificate authentication. \r\nAbout 125 worker nodes with 4k CPU cores at the remote site have been joined into IHEP LHAASO cluster by the distributed computing system and provided LHAASO job to produce 700TB simulation data in 6 months.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 268,
            id: 'c16874',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11471/contribution.pdf',
            presenters: [
              {
                affiliation: 'IHEP, CAS',
                displayOrderKey: [4, 'CHENG, Yaodong'],
                emailHash: '6ee3ac44f5bc2a44a90d7c47d76bd92a',
                familyName: 'CHENG',
                firstName: 'Yaodong',
                name: 'Yaodong CHENG',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12187,
            sessionSlotId: 2750,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Lightweight Distributed Computing System Oriented to LHHAASO Data Processing',
            uniqueId: 'c16874',
            url: '/event/459/contributions/11471/',
          },
          c16875: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11491/attachments/9412/13653/CTADIRAC-CHEP-Alice-Faure.pdf',
                  id: 13653,
                  title: 'CTADIRAC-CHEP-Alice-Faure.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11491,
            description:
              'The Cherenkov Telescope Array Observatory (CTAO) is the next generation ground-based observatory for gamma-ray astronomy at very high energies. It will consist of tens of Cherenkov telescopes, spread between two array sites: one in the Northern hemisphere in La Palma (Spain), and one in the Southern hemisphere in Paranal (Chile). Currently under construction, CTAO will start scientific operations in the next years for about 30 years. CTAO is expected to produce 2PB of raw data each year and to manage a global data volume which will grow through the years to reach around 100 PB. In addition, CTAO will require a high computing capacity for data processing and Monte Carlo simulations, of the order of hundreds of millions of CPU HS06 hours per year. To meet these requirements, CTAO will have a distributed computing model using 4 academic data centers, and use the DIRAC framework as its workload management system. In the past ten years, to optimize the instrument design and study its performances, CTAO has used the EGI grid infrastructure to run massive Monte Carlo campaigns. For these campaigns, CTAO has developed a production system prototype, based on DIRAC, to automatize the simulation and data processing workflows. This production system uses meta-data to link the different steps of a workflow. Recently, we have developed an interface to this system allowing for the configuration and submission of complex workflows.\r\n\r\nIn this contribution we present the CTAO production system and its use during the latest Monte Carlo campaigns as well as its recent interface development.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 367,
            id: 'c16875',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11491/contribution.pdf',
            presenters: [
              {
                affiliation: 'LUPM',
                displayOrderKey: [0, 'Faure, Alice'],
                emailHash: 'abdfd7a514ef6d2d7de64cf09d575159',
                familyName: 'Faure',
                firstName: 'Alice',
                name: 'Alice Faure',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12187,
            sessionSlotId: 2750,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'The Cherenkov Telescope Array Observatory workflow management system',
            uniqueId: 'c16875',
            url: '/event/459/contributions/11491/',
          },
          c16876: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11473/attachments/9433/14185/ALICE_Grid_CHEP2023_2.pdf',
                  id: 14185,
                  title: 'ALICE_Grid_CHEP2023_2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11473,
            description:
              'In preparation for LHC Run 3 and 4, the ALICE Collaboration has moved to a new Grid middleware, JAliEn, and workflow management system. The migration was dictated by the substantially higher requirements on the Grid infrastructure in terms of payload complexity, increased number of jobs and managed data volume, all of which required a complete rewrite of the middleware using modern software languages and technologies. Through containerization, self-contained binaries, managed by the JAliEn middleware, we provide a uniform execution environment across sites and various architectures, including accelerators. The model and implementation have proven their scalability and can be easily deployed across sites with minimal intervention.\r\n\r\nThis contribution outlines the architecture of the new Grid workflow as deployed in production and the workflow process. Specifically shown is how core components are moved and bootstrapped through CVMFS, enabling the middleware to run anywhere fully independent of the host system. Furthermore, it will examine how new middleware releases, containers and their runtimes are centrally maintained and easily deployed across the Grid, also by the means of a common build system.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 498,
            id: 'c16876',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11473/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Storetvedt, Maksim'],
                emailHash: 'b2cd2820aaf0912102f62b1adbc98aa2',
                familyName: 'Storetvedt',
                firstName: 'Maksim',
                name: 'Maksim Storetvedt',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12187,
            sessionSlotId: 2750,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'The ALICE Grid Workflow for LHC Run 3',
            uniqueId: 'c16876',
            url: '/event/459/contributions/11473/',
          },
          c16877: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11498/attachments/9723/14201/DynamicSchedulingOversubscription_MartaBertran.pdf',
                  id: 14201,
                  title: 'DynamicSchedulingOversubscription_MartaBertran.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#efebc2',
            conferenceId: 459,
            contributionId: 11498,
            description:
              'The ALICE Grid is designed to perform a realtime comprehensive monitoring of both jobs and execution nodes in order to  maintain a continuous and consistent status of the Grid infrastructure. An extensive database of historical data is available and is periodically analyzed to tune the workflow and data management to optimal performance levels. This data, when evaluated in real time, has the power to trigger decisions for efficient resource management of the currently running payloads, for example to enable the execution of a higher volume of work per unit of time. In this article, we consider scenarios in which, through constant interaction with the monitoring agents, a dynamic adaptation of the running workflows is performed. The target resources are memory and CPU with the objective of using them in their entirety and ensuring optimal utilization fairness between executing jobs.\r\n\r\nGrid resources are heterogeneous and of different generations, which means that some of them have superior hardware characteristics than the minimum required to execute ALICE jobs. Our middleware, JAliEn, works on the basis of allocating 2GB of RAM memory per job (allowing up to 8GB when including SWAP). Many of the worker nodes have higher memory per core ratios than these basic limits, thus  in terms of available memory they have free resources to accommodate extra jobs. The running jobs may have different behaviors and unequal resource usage depending on their nature. For example, analysis tasks are I/O bound while MonteCarlo tasks are CPU intensive. Running additional jobs with complementary resource usage patterns on a worker node has a great potential to increase the total efficiency of the worker nodes. This paper presents the methodology to exploit the different resource usage profiles by oversubscribing the executing nodes with extra jobs taking into account their CPU resource usage levels and memory capacity.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 28,
            id: 'c16877',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11498/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Bertran Ferrer, Marta'],
                emailHash: '52a8825d6326c1b1c5a615507a596723',
                familyName: 'Bertran Ferrer',
                firstName: 'Marta',
                name: 'Marta Bertran Ferrer',
              },
            ],
            references: [],
            room: 'Marriott Ballroom II-III',
            sessionCode: '',
            sessionId: 2038,
            sessionSlotEntryId: 12187,
            sessionSlotId: 2750,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Dynamic scheduling using CPU oversubscription in the ALICE Grid',
            uniqueId: 'c16877',
            url: '/event/459/contributions/11498/',
          },
        },
        entryType: 'Session',
        friendlyId: 8,
        id: 's12187',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2038/session-timetable.pdf',
        room: 'Marriott Ballroom II-III',
        sessionCode: '',
        sessionId: 2038,
        sessionSlotId: 2750,
        slotTitle: 'Workload Management',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 4 - Distributed Computing',
        uniqueId: 's12187',
        url: '/event/459/sessions/2038/',
      },
      s12188: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfe555',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: '',
            displayOrderKey: [0, 'Eulisse, Giulio'],
            emailHash: '7a6c8f49981d093b41b365a4b522378d',
            familyName: 'Eulisse',
            firstName: 'Giulio',
            name: 'Giulio Eulisse',
          },
          {
            affiliation: 'University of Liverpool',
            displayOrderKey: [0, 'Rodrigues, Eduardo'],
            emailHash: '69164b74ffee995c119954e624bd24ec',
            familyName: 'Rodrigues',
            firstName: 'Eduardo',
            name: 'Eduardo Rodrigues',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16907: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11551/attachments/9677/14115/chep-2023-knoepfel.pdf',
                  id: 14115,
                  title: 'chep-2023-knoepfel.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11551,
            description:
              'HEP data-processing frameworks are essential ingredients in getting from raw data to physics results.  But they are often tricky to use well, and they present a significant learning barrier for the beginning HEP physicist.  In addition, existing frameworks typically support rigid, collider-based data models, which do not map well to neutrino-physics experiments like DUNE.  Neutrino physicists thus expend significant effort working around framework limitations instead of using a framework that directly supports their needs.\r\n\r\nIn this talk, I present Meld, a Fermilab R\u0026D project, which intends to address these limitations.  By leveraging modern C++ capabilities, state-of-the-art concurrency libraries, and a flexible data model, it is possible for beginning (and seasoned) HEP physicists to execute framework programs easily and efficiently, with minimal coupling to framework-specific constructs.  Meld aims to directly support the frameworks needs of neutrino experiments like DUNE as well as the more common collider-based experiments.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 57,
            id: 'c16907',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11551/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermi National Accelerator Laboratory',
                displayOrderKey: [1, 'Knoepfel, Kyle'],
                emailHash: '8ad121ca011a999be5805f7b551dcb7d',
                familyName: 'Knoepfel',
                firstName: 'Kyle',
                name: 'Kyle Knoepfel',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12188,
            sessionSlotId: 2751,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Meld: Exploring the feasibility of a framework-less framework',
            uniqueId: 'c16907',
            url: '/event/459/contributions/11551/',
          },
          c16908: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11534/attachments/9589/13919/Belle2SoftwarePublication_Kuhr.pdf',
                  id: 13919,
                  title: 'Belle2SoftwarePublication_Kuhr.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11534,
            description:
              'The Belle II software was developed as closed source. As several HEP experiments released their code to the public, the topic of open source software was also discussed within the Belle II collaboration. A task force analyzed advantages and disadvantages and proposed a policy which was adopted by the collaboration in 2020. The Belle II offline software was then released under an open source license on github in 2021. In this contribution we will review the technical, social, and political challenges that had to be overcome for the publication of the Belle II software and take a look at the current status and prospects.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 67,
            id: 'c16908',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11534/contribution.pdf',
            presenters: [
              {
                affiliation: 'LMU Munich',
                displayOrderKey: [4, 'Kuhr, Thomas'],
                emailHash: '19c5a8c24b0013e148257724540e98e0',
                familyName: 'Kuhr',
                firstName: 'Thomas',
                name: 'Thomas Kuhr',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12188,
            sessionSlotId: 2751,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Publication of the Belle II Software',
            uniqueId: 'c16908',
            url: '/event/459/contributions/11534/',
          },
          c16909: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11535/attachments/9733/14234/230511_chep_key4hep.pdf',
                  id: 14234,
                  title: '230511_chep_key4hep.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11535,
            description:
              'Detector studies for future experiments rely on advanced software\r\ntools to estimate performance and optimize their design and technology\r\nchoices. The Key4hep project provides a flexible turnkey solution for\r\nthe full experiment life-cycle based on established community tools\r\nsuch as ROOT, Geant4, DD4hep, Gaudi, podio and spack. Members of the\r\nCEPC, CLIC, EIC, FCC, and ILC communities have joined to develop this\r\nframework and have merged, or are in the progress of merging, their\r\nrespective software environments into the Key4hep stack.\r\n\r\nThis presentation will give an overview over the recent progress in\r\nthe Key4hep project: covering the developments towards adaptation of\r\nstate-of-the-art tools for simulation (DD4hep, Gaussino), track and\r\ncalorimeter reconstruction (ACTS, CLUE), particle flow (PandoraPFA),\r\nanalysis via RDataFrame, and visualization with Phoenix. We will also\r\ncover the testing of some workflows on heterogeneous\r\ncomputing resources. Finally, we will show how new communities can adopt the\r\nKey4hep solution for their own purposes.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 518,
            id: 'c16909',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11535/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Sailer, Andre'],
                emailHash: '84f3f721370dba12b47a611b1a36081e',
                familyName: 'Sailer',
                firstName: 'Andre',
                name: 'Andre Sailer',
              },
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Ganis, Gerardo'],
                emailHash: '5464ee52c2740264980ed36d4553e207',
                familyName: 'Ganis',
                firstName: 'Gerardo',
                name: 'Gerardo Ganis',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12188,
            sessionSlotId: 2751,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'Key4hep Progress Report on Integrations',
            uniqueId: 'c16909',
            url: '/event/459/contributions/11535/',
          },
          c16910: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11546/attachments/9685/14200/2023-05-11-Salzburger-Open-Data-Detector.pdf',
                  id: 14200,
                  title: '2023-05-11-Salzburger-Open-Data-Detector.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#dfe555',
            conferenceId: 459,
            contributionId: 11546,
            description:
              'Open Data Detector (ODD) is a detector for algorithm research and development. The tracking system is an evolution of the detector used in the successful Tracking Machine Learning Challenge. It offers a more realistic design, with a support structure, cables, and cooling pipes. The ODD got extended with granular calorimetry and can be completed in future with a muon system. The magnetic field in the detector can be created with a solenoid located either in front or behind the calorimeters, providing two alternative options for detector studies.\r\nImplementation in DD4hep allows to perform ACTS based simulation (Fatras) of tracking detector, and Geant4 simulation of the full detector using key4HEP software. The goal of the ODD is to create a benchmark detector with public simulation data released and available for algorithm studies. Such data can be used for all the ongoing activities in the areas such as fast simulation or reconstruction.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 576,
            id: 'c16910',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11546/contribution.pdf',
            presenters: [
              {
                affiliation: '',
                displayOrderKey: [1, 'Salzburger, Andreas'],
                emailHash: '2cbb21e728f5ee0f6e1cb54ec854a4a7',
                familyName: 'Salzburger',
                firstName: 'Andreas',
                name: 'Dr Andreas Salzburger',
              },
            ],
            references: [],
            room: 'Chesapeake Meeting Room',
            sessionCode: '',
            sessionId: 2039,
            sessionSlotEntryId: 12188,
            sessionSlotId: 2751,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#202020',
            title: 'The Open Data Detector',
            uniqueId: 'c16910',
            url: '/event/459/contributions/11546/',
          },
        },
        entryType: 'Session',
        friendlyId: 9,
        id: 's12188',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2039/session-timetable.pdf',
        room: 'Chesapeake Meeting Room',
        sessionCode: '',
        sessionId: 2039,
        sessionSlotId: 2751,
        slotTitle: 'Sustainable Frameworks',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#202020',
        title: 'Track 5 - Sustainable and Collaborative Software Engineering',
        uniqueId: 's12188',
        url: '/event/459/sessions/2039/',
      },
      s12189: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#feffbf',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Manchester',
            displayOrderKey: [0, 'Skidmore, Nicole'],
            emailHash: 'a9b5fe5778eaecee042fcc8cab09ec26',
            familyName: 'Skidmore',
            firstName: 'Nicole',
            name: 'Nicole Skidmore',
          },
          {
            affiliation: 'University of Wisconsin\u2013Madison',
            displayOrderKey: [0, 'Held, Alexander'],
            emailHash: 'fed5998fb15c9323f97c094991d6da70',
            familyName: 'Held',
            firstName: 'Alexander',
            name: 'Alexander Held',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c16958: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11567/attachments/9493/13759/CHEP23_CPAlgs.pdf',
                  id: 13759,
                  title: 'CHEP23_CPAlgs.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11567,
            description:
              'During Run 2 the ATLAS experiment employed a large number of different user frameworks to perform the final corrections of its event data. For Run 3 a common framework was developed that incorporates the lessons learned from existing frameworks. Besides providing analysis standardization it also incorporates optimizations that lead to a substantial reduction in computing needs during analysis.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 385,
            id: 'c16958',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11567/contribution.pdf',
            presenters: [
              {
                affiliation: 'Iowa State University (US)',
                displayOrderKey: [1, 'Krumnack, Nils'],
                emailHash: '9aa8aab852d2208db54db51d005243a7',
                familyName: 'Krumnack',
                firstName: 'Nils',
                name: 'Nils Krumnack',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12189,
            sessionSlotId: 2752,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'CP Algorithms: A common corrections framework for ATLAS',
            uniqueId: 'c16958',
            url: '/event/459/contributions/11567/',
          },
          c16959: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11583/attachments/9494/13760/CHEP23_ColAna.pdf',
                  id: 13760,
                  title: 'CHEP23_ColAna.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11583,
            description:
              'Current analysis at ATLAS always involves a step of producing an analysis-specific n-tuple that incorporates the final step of calibrations as well as systematic variations. The goal for Run 4 is to make these analysis-specific corrections fast enough that they can be applied "on-the-fly" without the need for an intermediate n-tuple. The main complications for this are that some of these corrections have very complex implementations, and that a wide range of environments needs to be supported. An early prototype will be presented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 386,
            id: 'c16959',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11583/contribution.pdf',
            presenters: [
              {
                affiliation: 'Iowa State University (US)',
                displayOrderKey: [0, 'Krumnack, Nils'],
                emailHash: '9aa8aab852d2208db54db51d005243a7',
                familyName: 'Krumnack',
                firstName: 'Nils',
                name: 'Nils Krumnack',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12189,
            sessionSlotId: 2752,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Columnar analysis and on-the-fly analysis corrections at ATLAS',
            uniqueId: 'c16959',
            url: '/event/459/contributions/11583/',
          },
          c16960: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11586/attachments/9703/14161/atlas_physlite.pdf',
                  id: 14161,
                  title: 'atlas_physlite.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11586,
            description:
              'ATLAS is one of the main experiments at the Large Hadron Collider, with a diverse physics program covering precision measurements as well as new physics searches in countless final states, carried out by more than 2600 active authors. The High Luminosity LHC (HL-LHC) era brings unprecedented computing challenges that call for novel approaches to reduce the amount of data and MC that is stored, while continuing to support the rich physics program.\r\nWith the beginning of LHC Run 3, ATLAS introduced a new common data format, PHYS, that replaces most of the individual formats that were used in Run 2, and therefore reduces the disk storage significantly. ATLAS also launched the prototype of another common format, PHYSLITE, that is about a third of the size of PHYS. PHYSLITE will be the main format for the HL-LHC, and aims to serve 80% of all physics analyses. To simplify analysis workloads and further reduce disk usage it is designed to largely replace user-defined analysis n-tuples and consequently contains pre-calibrated objects. PHYSLITE is also intended to support \u201ccolumnar\u201d data processing techniques, which for some analyses may have significant advantages over the traditional event-loop analysis style. The evolution of data formats, the design principles for PHYSLITE, techniques for file size reductions, and various forms of validations will be discussed.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 462,
            id: 'c16960',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11586/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Washington (US)',
                displayOrderKey: [1, 'Schaarschmidt, Jana'],
                emailHash: '6f556891cf4ef30c552a4ac4aa1c87dc',
                familyName: 'Schaarschmidt',
                firstName: 'Jana',
                name: 'Jana Schaarschmidt',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12189,
            sessionSlotId: 2752,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'PHYSLITE - a new reduced common data format for ATLAS',
            uniqueId: 'c16960',
            url: '/event/459/contributions/11586/',
          },
          c16961: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11578/attachments/9711/14175/podio_chep2023.pdf',
                  id: 14175,
                  title: 'podio_chep2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11578,
            description:
              'A performant and easy-to-use event data model (EDM) is a key component of any HEP software stack.The podio EDM toolkit provides a user friendly way of generating such a performant implementation in C++ from a high level description in yaml format. Finalizing a few important developments, we release v1.0 of podio, a stable release with backward compatibility for datafiles written with podio from now on. We present an overview of the podio basics, and go into slighty more technical detail on the most important topics and developments. These include: schema evolution for generated EDMs, multithreading with podio generated EDMs, the implementation of them as well as the basics of I/O. Using EDM4hep, the common and shared EDM of the Key4hep project, we highlight a few of the smaller features in action as well as some lessons learned during the development of EDM4hep and podio. Finally, we show how podio has been integrated into the Gaudi based event processing framework that is used by Key4hep, before we conclude with a brief outlook on potential developments after v1.0.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 487,
            id: 'c16961',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11578/contribution.pdf',
            presenters: [
              {
                affiliation: 'DESY',
                displayOrderKey: [4, 'Madlener, Thomas'],
                emailHash: '67b5cf607bde7ae0696beeaa1d7bbc00',
                familyName: 'Madlener',
                firstName: 'Thomas',
                name: 'Thomas Madlener',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12189,
            sessionSlotId: 2752,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'podio v1.0 - A first stable release of the EDM toolkit',
            uniqueId: 'c16961',
            url: '/event/459/contributions/11578/',
          },
          c16963: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11576/attachments/9676/14197/chep2023_talk_dlersch.pdf',
                  id: 14197,
                  title: 'chep2023_talk_dlersch.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11576,
            description:
              'In the last two decades, there have been major breakthroughs in Quantum Chromodynamics (QCD), the theory of the strong interaction of quark and gluons, as well as major advances in the accelerator and detector technologies that allow us to map the spatial distribution and motion of the quarks and gluons in terms of quantum correlation functions (QCF). This field of research broadly known as Nuclear Femtography enters into a new era of exploration with the data from the 12 GeV Science program at Jefferson Lab being available and the construction of the Electron-Ion Collider and its detectors. Nuclear Femtography promises dramatic breakthroughs in our understanding of the subatomic world. It is now timely to rethink theoretical and experimental workflows for studying QCF and take advantage of recent algorithmic advances and unprecedented  computing resources  of the powerful new computers at the exascale to constrain QCF precisely in five or more kinematic dimensions. \r\n\r\nThe QUAntum chromodynamics Nuclear TOMography (QuantOm) Collaboration is proposing a unique event-level inference framework to obtain a quark and gluon tomography of nucleons and nuclei from high-energy scattering data. This new event-level approach stands to have a transformational impact on the data analysis workflow that connects theory with experiment, and will help ensure that current and future facilities, such as Jefferson Lab and the Electron-Ion Collider, deliver on their science mission to reveal the inner structure of the visible universe at the femtometer scale.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 602,
            id: 'c16963',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11576/contribution.pdf',
            presenters: [
              {
                affiliation: 'Florida State University',
                displayOrderKey: [2, 'Lersch, Daniel'],
                emailHash: 'd00ca36abe98a38e005b868fe6ccfc5b',
                familyName: 'Lersch',
                firstName: 'Daniel',
                name: 'Daniel Lersch',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12189,
            sessionSlotId: 2752,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'The QuantOm Event-Level Inference Framework',
            uniqueId: 'c16963',
            url: '/event/459/contributions/11576/',
          },
          c17482: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11604/attachments/9559/13868/ComPWA%20CHEP%20poster.pdf',
                  id: 13868,
                  title: 'ComPWA CHEP2023 poster.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11604/attachments/9559/14218/ComPWA%20CHEP2023%20talk.pdf',
                  id: 14218,
                  title: 'ComPWA CHEP2023 talk.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11604/attachments/9559/14181/go',
                  id: 14181,
                  title: 'Slides',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#feffbf',
            conferenceId: 459,
            contributionId: 11604,
            description:
              'In the ideal world, we describe our models with recognizable mathematical expressions and directly fit those models to large data sample with high performance. It turns out that this can be done with a CAS, using its symbolic expression trees as template to computational back-ends like JAX. The CAS can in fact further simplify the expression tree, which can result in speed-ups in the numerical back-end.\r\n\r\nThe [ComPWA project](https://compwa-org.rtfd.io) offers Python libraries that use this principle to formulate large expressions for amplitude analysis, so that the user has the flexibility to quickly implement different formalisms and can also easily perform fast computations on large data samples. The CAS additionally allows the project to standardize and automatically document these formalisms as they are being implemented.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 443,
            id: 'c17482',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11604/contribution.pdf',
            presenters: [
              {
                affiliation: 'Ruhr University Bochum',
                displayOrderKey: [0, 'de Boer, Remco'],
                emailHash: '5317be2dc25db1afa191bbdb6fbf4aa0',
                familyName: 'de Boer',
                firstName: 'Remco',
                name: 'Remco de Boer',
              },
            ],
            references: [],
            room: 'Hampton Roads Ballroom VIII',
            sessionCode: '',
            sessionId: 2040,
            sessionSlotEntryId: 12189,
            sessionSlotId: 2752,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#1f1f02',
            title: 'Speeding up amplitude analysis with a Computer Algebra System',
            uniqueId: 'c17482',
            url: '/event/459/contributions/11604/',
          },
        },
        entryType: 'Session',
        friendlyId: 10,
        id: 's12189',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2040/session-timetable.pdf',
        room: 'Hampton Roads Ballroom VIII',
        sessionCode: '',
        sessionId: 2040,
        sessionSlotId: 2752,
        slotTitle: 'PM Parallel',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#1f1f02',
        title: 'Track 6 - Physics Analysis Tools',
        uniqueId: 's12189',
        url: '/event/459/sessions/2040/',
      },
      s12190: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#1a3f14',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Wiebalck, Arne'],
            emailHash: 'a7f74ac17e363dd0ab1ffe2342ba5c5f',
            familyName: 'Wiebalck',
            firstName: 'Arne',
            name: 'Arne Wiebalck',
          },
          {
            affiliation: 'KEK',
            displayOrderKey: [0, 'Kishimoto, Tomoe'],
            emailHash: '1c2a9b3c698c4de9040fbe367da6851b',
            familyName: 'Kishimoto',
            firstName: 'Tomoe',
            name: 'Tomoe Kishimoto',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17117: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11621/attachments/9607/13972/cmsweb_security_chep_presentation_2023.pdf',
                  id: 13972,
                  title: 'cmsweb_security_chep_presentation_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11621,
            description:
              'The CMSWEB cluster is pivotal to the activities of the Compact Muon Solenoid (CMS) experiment, as it hosts critical services required for the operational needs of the CMS experiment. The security of these services and the corresponding data is crucial to CMS. Any malicious attack can compromise the availability of our services. Therefore, it is important to construct a robust security infrastructure. In this work, we discuss some new security features introduced to the CMSWEB kubernetes ("k8s") cluster. The new features include the implementation of network policies, deployment of Open Policy Agent (OPA), enforcement of OPA policies, and lastly, the integration of Vault. The network policies act as an inside-the-cluster firewall to limit the network communication between the pods to the minimum necessary, and its dynamic nature allows us to work with microservices. The OPA validates the objects against some custom-defined policies during create, update, and delete operations to further enhance security. Without recompiling or changing the configuration of the Kubernetes API server, it can apply customized policies on Kubernetes objects and their audit functionality enabling us to detect pre-existing conflicts and issues. Although Kubernetes incorporates the concepts of secrets, they are only base64 encoded and are not dynamically configured; once the configuration of any secret is changed, the service restart is required for the configuration to be incorporated. This is where Vault comes into play: Vault dynamically secures, stores, and tightly controls access to sensitive data. This way, the secret information is encrypted, secured, and centralized, making it more scalable and easier to manage. Thus, the implementation of these three security features will corroborate the enhanced security and reliability of the CMSWEB Kubernetes infrastructure.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 71,
            id: 'c17117',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11621/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of California San Diego',
                displayOrderKey: [6, 'Mascheroni, Marco'],
                emailHash: '91b6e0458cfc38c6e2b8701f5a7b9a24',
                familyName: 'Mascheroni',
                firstName: 'Marco',
                name: 'Marco Mascheroni',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12190,
            sessionSlotId: 2753,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Implementation of New Security Features in the CMSWEB Kubernetes Cluster at CERN',
            uniqueId: 'c17117',
            url: '/event/459/contributions/11621/',
          },
          c17118: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11629/attachments/9614/13985/CHEP_beermann_hifis_gitops.pdf',
                  id: 13985,
                  title: 'CHEP_beermann_hifis_gitops.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11629,
            description:
              'Helmholtz Federated IT Services (HIFIS) provides shared IT services in across all fields and centres in the Helmholtz Association. HIFIS is a joint platform in which most of the research centres in the Helmholtz Association collaborate and offer cloud and fundamental backbone services free of charge to scientists in Helmholtz and their partners. Furthermore, HIFIS provides a federated authentication and authorization infrastructure that enables a unified login across all services.\r\nDESY provides several of those cloud services in this Helmholtz Cloud, which include HedgeDoc, JupyterHub, the HIFIS Cloud Portal and more. The Cloud Portal is developed at DESY and provides a central entry point for users to find and access services in the Helmholtz Cloud. These services have been deployed on a shared Kubernetes cluster to ensure availability and scalability. To manage the deployments, a GitOps approach was taken to provide an automated deployment process that facilitates fast rollouts and accountability. This was achieved by using Gitlab and FluxCD to manage the configurations and apply them to the cluster. Additionally, for the Cloud Portal Gitlab pipelines have been developed that automatically deploy completely separated review environments for merge requests that enable full end-to-end tests.\r\nThis contribution describes the process to employ this new operational framework and the challenges that had to be overcome as well as organizational agreements taken within the administrating team.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 394,
            id: 'c17118',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11629/contribution.pdf',
            presenters: [
              {
                affiliation: 'DESY',
                displayOrderKey: [0, 'Beermann, Thomas'],
                emailHash: '431d69830a81c8da6cef96b43368bea6',
                familyName: 'Beermann',
                firstName: 'Thomas',
                name: 'Thomas Beermann',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12190,
            sessionSlotId: 2753,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Adapting GitOps to manage Helmholtz Cloud services at DESY',
            uniqueId: 'c17118',
            url: '/event/459/contributions/11629/',
          },
          c17119: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11648/attachments/9709/14170/CHEP23%20-%20Architecting%20the%20OpenSearch%20service%20at%20CERN.pdf',
                  id: 14170,
                  title: 'CHEP23 - Architecting the OpenSearch service at CERN.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11648,
            description:
              'The centralised Elasticsearch service has already been running at CERN for over 6 years, providing the search and analytics engine for numerous CERN users. The service has been based on the open-source version of Elasticsearch, surrounded by a set of external open-source plugins offering security, multi-tenancy, extra visualization types and more. The evaluation of OpenDistro for Elasticsearch, also based on the open-source Elasticsearch core, started couple of years ago. It offered similar functionality using a set of different modules, with the main difference being that everything was bundled together, making it easier to deploy new versions. Later on, after a restrictive license change from Elastic, this evaluation became much more critical. The OpenDistro project got re-branded as OpenSearch, having now a forked version of Elasticsearch at core. Motivated by the license change and by the streamlined deployment of the feature-rich OpenSearch project as a 100% open-source environment, the decision was taken to migrate the service at CERN towards it. Adjusting the service to the new modules required the full redesign of the architecture. This had to be achieved while maintaining the high standards of resource-efficiency already in place. In addition to the plethora of new capabilities, the new architecture enables a streamlined service deployment that overcomes long-standing maintainability issues, while covering the ever-rising demand of use-cases. At the time of writing this article, over 30 OpenSearch clusters are in production using our service. This contribution covers the motivation, design and implementation of this change for the diverse use-cases around CERN, as well as the challenges emerged along the road.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 294,
            id: 'c17119',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11648/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Papadopoulos, Sokratis'],
                emailHash: 'dec634e3347890b5bd290460ecdadee4',
                familyName: 'Papadopoulos',
                firstName: 'Sokratis',
                name: 'Sokratis Papadopoulos',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12190,
            sessionSlotId: 2753,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Architecting the OpenSearch service at CERN',
            uniqueId: 'c17119',
            url: '/event/459/contributions/11648/',
          },
          c17120: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11631/attachments/9670/14101/Jain_CHEP23.pptx',
                  id: 14101,
                  title: 'Jain_CHEP23.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11631,
            description:
              'The LCAPE project develops artificial intelligence to improve operations in the FNAL control room by reducing the time to identify the cause of an outage, improving the reproducibility of labeling it, predicting their duration and forecasting their occurrence. \r\nWe present our solution for incorporating information from ~2.5k monitored devices to distinguish between dozens of different causes of down time. \r\nWe discuss the performance of different techniques for modeling the state of health of the facility and of different unsupervised clustering techniques to distinguish between different causes of down time.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 612,
            id: 'c17120',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11631/contribution.pdf',
            presenters: [
              {
                affiliation: 'PNNL',
                displayOrderKey: [1, 'Jain, Milan'],
                emailHash: '0f35cad7c5a4f41ca6b010b27c85d8ef',
                familyName: 'Jain',
                firstName: 'Milan',
                name: 'Milan Jain',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12190,
            sessionSlotId: 2753,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Artificial Intelligence for improved facilities operation',
            uniqueId: 'c17120',
            url: '/event/459/contributions/11631/',
          },
          c17121: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11638/attachments/9708/14174/chep23_stackstorm.pptx',
                  id: 14174,
                  title: 'chep23_stackstorm.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11638,
            description:
              'Batch@CERN team manages over 250k CPU cores for Batch processing of LHC data with our HTCondor cluster comprising ~5k nodes. We will present a lifecycle management solution of our systems with our in-house developed state-manager daemon BrainSlug and how it handles draining, rebooting, interventions and other actions on the worker nodes, with Rundeck as our human-interaction endpoint and using StackStorm for automated procedures of remediating minor alarms, health-checks and enabling an overall self-healing infrastructure. We will demonstrate how these processes and reduced the manual overhead of handling daily operations by a 10x factor with StackStorm enabled workflows, and how we can enable operators to schedule and manage interventions while having granular control on the actions exposed to such operators.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 473,
            id: 'c17121',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11638/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [2, 'Jones, Ben'],
                emailHash: 'f14203dee7593a809ed518bad2f3be9e',
                familyName: 'Jones',
                firstName: 'Ben',
                name: 'Mr Ben Jones',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12190,
            sessionSlotId: 2753,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title:
              'Management of Batch worker node lifecycle and maintenance at CERN with BrainSlug (our in-house state-manager daemon), Rundeck and StackStorm',
            uniqueId: 'c17121',
            url: '/event/459/contributions/11638/',
          },
          c17122: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11618/attachments/9663/14087/Kelsey11may23-IPv6.pdf',
                  id: 14087,
                  title: 'Kelsey11may23-IPv6.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#1a3f14',
            conferenceId: 459,
            contributionId: 11618,
            description:
              'The transition of WLCG storage services to dual-stack IPv4/IPv6 is nearing completion after more than 5 years, thus enabling the use of IPv6-only CPU resources as agreed by the WLCG Management Board and presented by us at earlier CHEP conferences. Much of the data is transferred by the LHC experiments over IPv6. All Tier-1 storage and over 90% of Tier-2 storage is now IPv6-enabled, yet we still see IPv4 transfers happening when both endpoints have IPv6 available or when remote data is accessed over the network from worker nodes.\r\n\r\nThe monitoring and tracking of all data transfers is essential, together with the ability to understand the relative use of IPv6 and IPv4. This paper presents the status of monitoring IPv6 data flows within WLCG and the plans to improve the ability to distinguish between IPv6 and IPv4. Furthermore, the Research Networking Technical Working Group has identified marking the IPv6 packet header as one approach for understanding complex large data flows. This provides another driver for full transition to the use of IPv6 in WLCG data transfers.\r\n\r\nThe agreed endpoint of the WLCG transition to IPv6 remains the deployment of IPv6-only services, thereby removing the complexity and security concerns of operating dual stacks. The working group is identifying where IPv4 can be removed and investigating the obstacles to the use of IPv6 in WLCG. Why do transfers between two dual-stack endpoints still use IPv4? This work is presented together with the obstacles defeated, those remaining, and those outside of our control.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 312,
            id: 'c17122',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11618/contribution.pdf',
            presenters: [
              {
                affiliation: 'UKRI-STFC',
                displayOrderKey: [1, 'Kelsey, David'],
                emailHash: 'f93592852bac53b82654044c3bcd6fe9',
                familyName: 'Kelsey',
                firstName: 'David',
                name: 'David Kelsey',
              },
            ],
            references: [],
            room: 'Marriott Ballroom IV',
            sessionCode: '',
            sessionId: 2041,
            sessionSlotEntryId: 12190,
            sessionSlotId: 2753,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#f1ffef',
            title: 'Overcoming obstacles to IPv6 on WLCG',
            uniqueId: 'c17122',
            url: '/event/459/contributions/11618/',
          },
        },
        entryType: 'Session',
        friendlyId: 11,
        id: 's12190',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2041/session-timetable.pdf',
        room: 'Marriott Ballroom IV',
        sessionCode: '',
        sessionId: 2041,
        sessionSlotId: 2753,
        slotTitle: 'Deployment, Management and Monitoring',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#f1ffef',
        title: 'Track 7 - Facilities and Virtualization',
        uniqueId: 's12190',
        url: '/event/459/sessions/2041/',
      },
      s12192: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#92b6db',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'INFN-CNAF',
            displayOrderKey: [0, 'Dal Pra, Stefano'],
            emailHash: '4bb37fa7bb6103869e0d7cdb7c104ea6',
            familyName: 'Dal Pra',
            firstName: 'Stefano',
            name: 'Stefano Dal Pra',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'VALLECORSA, SOFIA'],
            emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
            familyName: 'VALLECORSA',
            firstName: 'SOFIA',
            name: 'SOFIA VALLECORSA',
          },
          {
            affiliation: 'University of Washington (US)',
            displayOrderKey: [0, 'Schaarschmidt, Jana'],
            emailHash: '6f556891cf4ef30c552a4ac4aa1c87dc',
            familyName: 'Schaarschmidt',
            firstName: 'Jana',
            name: 'Jana Schaarschmidt',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17194: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11755/attachments/9588/13918/2023_05_11_ML_ATLAS_LAr_Johann_Voigt.pdf',
                  id: 13918,
                  title: '2023_05_11_ML_ATLAS_LAr_Johann_Voigt.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11755,
            description:
              'The Large Hadron Collider (LHC) at CERN is the largest and most powerful particle collider today. The Phase-II Upgrade of the LHC will increase the instantaneous luminosity by a factor of 7 leading to the High Luminosity LHC (HL-LHC). At the HL-LHC, the number of proton-proton collisions in one bunch crossing (called pileup) increases significantly, putting more stringent requirements on the LHC detector electronics and real-time data processing capabilities. \r\nThe ATLAS Liquid Argon (LAr) calorimeter measures the energy of particles produced in LHC collisions. This calorimeter also feeds the ATLAS trigger to identify interesting events. In order to enhance the ATLAS detector physics discovery potential, in the blurred environment created by the pileup, an excellent resolution of the deposited energy and an accurate detection of the deposited time are crucial. \r\nThe computation of the deposited energy will be performed in real-time using dedicated data acquisition electronic boards based on FPGAs. FPGAs are chosen for their capacity to treat large amounts of data with very low latency. The computation of the deposited energy is currently done using optimal filtering algorithms that assume a nominal pulse shape of the electronic signal. These filter algorithms are adapted to the LHC conditions with very limited pileup and no timing overlap of the electronic pulses in the detector. However, with the increased luminosity and pileup at HL-LHC, the performance of the filter algorithms decreases significantly and no further extension nor tuning of these algorithms could recover the lost performance. \r\nThe off-detector electronic boards for the Phase-II Upgrade of the LAr calorimeter will use the next high-end generation of INTEL FPGAs with increased processing power and memory. This is a unique opportunity to develop the necessary tools, enabling the use of more complex algorithms on these boards. We developed several neural networks (NNs) with significant performance improvements with respect to the optimal filtering algorithms. The main challenge is to efficiently implement these NNs into the dedicated data acquisition electronics. Special effort was dedicated to minimising the needed computational power while optimising the NNs architectures. \r\nFive NN algorithms based on CNN, RNN, and LSTM architectures will be presented. The improvement of the energy resolution and the accuracy on the deposited time compared to the legacy filter algorithms, especially for overlapping pulses, will be discussed. The implementation of these networks in firmware will be shown. Two implementation categories in VHDL and Quartus HLS code are considered. The implementation results on Stratix 10 INTEL FPGAs, including the resource usage, the latency, and operation frequency will be reported. Approximations for the firmware implementations, including the use of fixed-point precision arithmetic and lookup tables for activation functions, will be discussed. Implementations including time multiplexing to reduce resource usage will be presented. We will show that two of these NNs implementations are viable solutions that fit the stringent data processing requirements on the latency (O(100ns)) and bandwidth (O(1Tb/s) per FPGA) needed for the ATLAS detector operation. The results of the tests of one of the NNs on the hardware will be presented along with the test setup. \r\nThis development is completely new and targets a technological breakthrough in the usage of neural networks implemented in readout electronic boards of particle physics detectors. We show that this goal is achievable for the HL-LHC upgrade. The results from this work are published in a special edition of the Computing and Software for Big Science journal.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 48,
            id: 'c17194',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11755/contribution.pdf',
            presenters: [
              {
                affiliation: 'TU Dresden',
                displayOrderKey: [1, 'Voigt, Johann'],
                emailHash: 'a8ff2dc41c4422697dd0141bc0ceb3d9',
                familyName: 'Voigt',
                firstName: 'Johann',
                name: 'Johann Voigt',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12192,
            sessionSlotId: 2755,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Machine Learning for Real-Time Processing of ATLAS Liquid Argon Calorimeter Signals with FPGAs',
            uniqueId: 'c17194',
            url: '/event/459/contributions/11755/',
          },
          c17195: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11738/attachments/9635/14031/Delaney_LipMonLHCb_CHEP23.pdf',
                  id: 14031,
                  title: 'Delaney_LipMonLHCb_CHEP23.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11738,
            description:
              'The data-taking conditions expected of Run 3 pose unprecedented challenges for the DAQ systems of the LHCb experiment at the LHC. The LHCb collaboration is pioneering the adoption of a fully-software trigger to cope with the expected increase in luminosity and, thus, event rate. The upgraded trigger system has required advances in the use of hardware architectures, software and algorithms. Among the last, the LHCb collaboration can be quoted for using Lipschitz monotonic neural networks for the first time. These are particularly appealing, owing to their robustness under varying detector conditions and sensitivity to highly displaced, high-momentum beauty candidates. An overview of the applications of such architectures within the LHCb trigger system is presented. Emphasis is placed on the topological triggers, devoted to selecting b-hadron candidates inclusively by exploiting the kinematics and decay topology characteristic of beauty decays.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 218,
            id: 'c17195',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11738/contribution.pdf',
            presenters: [
              {
                affiliation: 'Massachusetts Institute of Technology',
                displayOrderKey: [1, 'Delaney, Blaise'],
                emailHash: '97afae0234e1138d9fe62360b3800369',
                familyName: 'Delaney',
                firstName: 'Blaise',
                name: 'Blaise Delaney',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12192,
            sessionSlotId: 2755,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Applications of Lipschitz neural networks to the Run 3 LHCb trigger system',
            uniqueId: 'c17195',
            url: '/event/459/contributions/11738/',
          },
          c17196: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11751/attachments/9710/14172/CBrown_ContinualLearning_CHEP_1105.pdf',
                  id: 14172,
                  title: 'CBrown_ContinualLearning_CHEP_1105.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11751,
            description:
              'The High-Luminosity LHC upgrade of the CMS experiment will utilise a large number of Machine Learning (ML) based algorithms in its hardware-based trigger. These ML algorithms will facilitate the selection of potentially interesting events for storage and offline analysis. Strict latency and resource requirements limit the size and complexity of these models due to their use in a high-speed trigger setting and deployment on FPGA hardware.\r\n\r\nIt is envisaged that these ML models will be trained on large, carefully tuned, Monte Carlo (MC) datasets and subsequently deployed in a real-world detector environment. Not only is there a potentially large difference between the MC training data and real-world conditions but these detector conditions could change over time leading to a shift in model output which could degrade trigger performance.\r\n\r\nThe studies presented explore different techniques to reduce the impact of this effect, using the CMS track finding and vertex trigger algorithms as a test case. The studies compare a baseline retraining and redeployment of the model, uncertainty quantification of the model output, and episodic training of a model as new data arrives in a continual learning context. The results show that a continually learning algorithm outperforms a simple retrained model when degradation in detector performance is applied to the training data and is a viable option for maintaining performance in an evolving environment such as the High-Luminosity LHC.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 76,
            id: 'c17196',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11751/contribution.pdf',
            presenters: [
              {
                affiliation: 'Imperial College (GB)',
                displayOrderKey: [0, 'Brown, Christopher'],
                emailHash: '610367ada20ec71819062f9df3a1ded0',
                familyName: 'Brown',
                firstName: 'Christopher',
                name: 'Christopher Brown',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12192,
            sessionSlotId: 2755,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'The Deployment of Realtime ML in Changing Environments',
            uniqueId: 'c17196',
            url: '/event/459/contributions/11751/',
          },
          c17197: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11756/attachments/9700/14154/IsabelHaide_CHEP_Talk_110523.pdf',
                  id: 14154,
                  title: 'IsabelHaide_CHEP_Talk_110523.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11756,
            description:
              'For the Belle II experiment, the electromagnetic calorimeter (ECL) plays a crucial role in both the trigger decisions and the offline analysis. \r\nThe performance of existing clustering algorithms degrades with rising backgrounds that are expected for the increasing luminosity in Belle II. In offline analyses, this mostly impacts the energy resolution for low-energy photons; for the trigger, it\u0027s most challenging to keep a high efficiency with low fake rates, especially for low-energy or overlapping clusters. \r\nIn the case of offline reconstruction, we developed a soft clustering algorithm based on graph neural networks (GNN) that improves the energy resolution for photons. We report a significant improvement over the current Belle II algorithm with better resolution for low-energy photons, particularly for the increased background rates expected with higher instantaneous luminosity. \r\nFor online reconstruction, we implemented a resource-efficient GNN-based algorithm for object condensation that is able to detect an unknown number of clusters and their respective position and energy inside the calorimeter, despite the presence of background energy and the irregular geometry of the ECL. This is compared to the current trigger algorithm in Belle II and could provide an improved trigger decision, especially for higher background rates.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 129,
            id: 'c17197',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11756/contribution.pdf',
            presenters: [
              {
                affiliation: 'Karlsruhe Institute of Technology',
                displayOrderKey: [1, 'Haide, Isabel'],
                emailHash: 'a9a672106ef3e516b6cb1e154994e6f7',
                familyName: 'Haide',
                firstName: 'Isabel',
                name: 'Isabel Haide',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12192,
            sessionSlotId: 2755,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Improved Clustering in the Belle II Electromagnetic Calorimeter with Graph Neural Networks',
            uniqueId: 'c17197',
            url: '/event/459/contributions/11756/',
          },
          c17198: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11719/attachments/9682/14122/go',
                  id: 14122,
                  title: 'Dataset',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11719/attachments/9682/14245/DFEI_CHEP2023_jonas_live.pdf',
                  id: 14245,
                  title: 'DFEI_CHEP2023_jonas_live.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11719/attachments/9682/14246/go',
                  id: 14246,
                  title: 'Paper',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11719,
            description:
              'In a decade from now, the Upgrade II of LHCb experiment will face an instantaneous luminosity ten times higher than in the current Run 3 conditions. This will bring LHCb to a new era, with huge event sizes and typically several signal heavy-hadron decays per event. The trigger scope will shift from selecting interesting events to select interesting parts of multi-signal events. To allow for an inclusive, automatic and accurate multi-signal selection per event, we propose evolving from the current signal-based trigger to a Deep-learning based Full Event Interpretation (DFEI) approach. We have designed the first prototype for the DFEI algorithm, leveraging the power of Graph Neural Networks (GNN). The algorithm takes as input the final-state particles and has a two-folded goal: select the sub-set of particles originated in heavy-hadron decays, and reconstruct the decay chains in which they were produced. In this talk, we describe the design and development of this prototype, its current performance on simulated data for Run 3 conditions, and the studies and developments done so far towards and eventual integration in the Real Time Analysis (RTA) system of LHCb.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 170,
            id: 'c17198',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11719/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Z\u00fcrich',
                displayOrderKey: [3, 'Eschle, Jonas'],
                emailHash: '1f0e37ec0dc64bda351dfa513d6367b4',
                familyName: 'Eschle',
                firstName: 'Jonas',
                name: 'Mr Jonas Eschle',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12192,
            sessionSlotId: 2755,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title:
              'Development of a Deep-learning based Full Event Interpretation (DFEI) algorithm for the future LHCb trigger',
            uniqueId: 'c17198',
            url: '/event/459/contributions/11719/',
          },
          c17199: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11726/attachments/9695/14145/2023-05-11%20CHEP%20Acceleration%20of%20a%20CMS%20DNN%20based%20Algorithm.pdf',
                  id: 14145,
                  title: '2023-05-11 CHEP Acceleration of a CMS DNN based Algorithm.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#92b6db',
            conferenceId: 459,
            contributionId: 11726,
            description:
              'The search for exotic long-lived particles (LLP) is a key area of the current LHC physics programme and is expected to remain so into the High-Luminosity (HL)-LHC era. As in many areas of the LHC physics programme Machine Learning algorithms play a crucial role in this area, in particular Deep Neural Networks (DNN), which are able to use large numbers of low-level features to achieve enhanced search sensitivity. Sophisticated algorithms however present computing challenges, especially looking forward to the data rates and volumes of the HL-LHC era. Accelerated computing, using heterogeneous hardware such as GPUs and FPGAs alongside CPUs, offers a solution to the computing challenges, both in offline processing and realtime triggering applications. Demonstrating state-of-the-art algorithms on such hardware is a key benchmark for developing this computing model for the future.\r\n \r\nThe studies presented describe the implementation of a DNN based LLP jet-tagging algorithm, published by the CMS experiment, on an FPGA. Novel optimisations in the design of this DNN are presented, including the adoption of cyclic random access memories for a simplified convolution operation, the reuse of multiply accumulate operations for a flexibly selectable tradeoff between throughput and resource usage, and storing matrices distributed over many RAM memories with elements grouped by index as opposed to traditional storage methods. An evaluation of potential dataflow hardware architectures is also included. It is shown that the proposed optimisations can yield performance enhancements by factors up to an order of magnitude compared to other FPGA implementations. They can also lead to smaller FPGA footprints and accordingly reduce power consumption, allowing for instance duplication of compute units to achieve increases in effective throughput, as well as deployment on a wider range of devices and applications.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 548,
            id: 'c17199',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11726/contribution.pdf',
            presenters: [
              {
                affiliation: 'Imperial College London',
                displayOrderKey: [3, 'Barbone, Marco'],
                emailHash: '67f6e235a1ea8baeb03e2fbe5fd43fd9',
                familyName: 'Barbone',
                firstName: 'Marco',
                name: 'Marco Barbone',
              },
            ],
            references: [],
            room: 'Hampton Roads VII',
            sessionCode: '',
            sessionId: 2043,
            sessionSlotEntryId: 12192,
            sessionSlotId: 2755,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#03070f',
            title: 'Acceleration of a CMS DNN based Algorithm',
            uniqueId: 'c17199',
            url: '/event/459/contributions/11726/',
          },
        },
        entryType: 'Session',
        friendlyId: 13,
        id: 's12192',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2043/session-timetable.pdf',
        room: 'Hampton Roads VII',
        sessionCode: '',
        sessionId: 2043,
        sessionSlotId: 2755,
        slotTitle: 'Online Applications',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#03070f',
        title: 'Track 9 - Artificial Intelligence and Machine Learning',
        uniqueId: 's12192',
        url: '/event/459/sessions/2043/',
      },
      s12194: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#4f144e',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'CSIC',
            displayOrderKey: [0, 'Campos, Isabel'],
            emailHash: '1ba624fa939fe1db5c536ca7116de83b',
            familyName: 'Campos',
            firstName: 'Isabel',
            name: 'Isabel Campos',
          },
          {
            affiliation: 'Nikhef National institute  for subatomic physics (NL)',
            displayOrderKey: [0, 'Aaij, Roel'],
            emailHash: '6fecef0aff50325ef5ea685927dec4a5',
            familyName: 'Aaij',
            firstName: 'Roel',
            name: 'Roel Aaij',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17247: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11819/attachments/9684/14125/chep2023_lhcb_gpu_tensorrt_inference_mvanveghel_v3.pdf',
                  id: 14125,
                  title: 'chep2023_lhcb_gpu_tensorrt_inference_mvanveghel_v3.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11819,
            description:
              'The first stage of the LHCb High Level Trigger is implemented as a GPU application. In 2023 it will run on 400 NVIDIA GPUs and its goal is to reduce the rate of incoming data from 5 TB/s to approximately 100 GB/s. A broad scala of reconstruction algorithms is implemented as approximately 70 kernels. Machine Learning algorithms are attractive to further extend the physics reach of the application, but inference must be integrated into the existing GPU application and be very fast to maintain the required throughput of at least 10 GB/s per GPU. We investigate the use of NVIDIA TensorRT for flexible loading of Machine Learning models and fast inference, benchmark its performance for a range of interesting Machine Learning models and compare it to hand-coded implementations where possible.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 398,
            id: 'c17247',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11819/contribution.pdf',
            presenters: [
              {
                affiliation: 'Nikhef National institute  for subatomic physics (NL)',
                displayOrderKey: [4, 'Aaij, Roel'],
                emailHash: '6fecef0aff50325ef5ea685927dec4a5',
                familyName: 'Aaij',
                firstName: 'Roel',
                name: 'Roel Aaij',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12194,
            sessionSlotId: 2757,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'High-Throughput Machine Learning Inference with NVIDIA TensorRT',
            uniqueId: 'c17247',
            url: '/event/459/contributions/11819/',
          },
          c17249: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: null,
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11832,
            description:
              'In mainstream machine learning, transformers are gaining widespread usage. As Vision Transformers rise in popularity in computer vision, they now aim to tackle a wide variety of machine learning applications. In particular, transformers for High Energy Physics (HEP) experiments continue to be investigated for tasks including jet tagging, particle reconstruction, and pile-up mitigation.\r\n\r\nIn a first of its kind a Quantum Vision Transformer (QViT) with a Quantum-enhanced attention mechanism (and thereby quantum-enhanced self-attention) is introduced and discussed. A shallow circuit is proposed for each component of self-attention to leverage current Noisy Intermediate Scale Quantum (NISQ) devices. Variations of the hybrid architecture/model are explored and analyzed.\r\n\r\nThe results demonstrate a successful proof of concept for the QViT, and establish a competitive performance benchmark for the proposed design and implementation. The findings also provide strong motivation to experiment with different architectures, hyperparameters, and datasets, setting the stage for implementation in HEP environments where transformers are increasingly used in state of the art machine learning solutions.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 610,
            id: 'c17249',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11832/contribution.pdf',
            presenters: [
              {
                affiliation: 'UC Santa Cruz / CERN',
                displayOrderKey: [1, 'Pasquali, Dominic'],
                emailHash: '2eadcbdbcf984afea95a813d6681733f',
                familyName: 'Pasquali',
                firstName: 'Dominic',
                name: 'Dominic Pasquali',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12194,
            sessionSlotId: 2757,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'First Measurements With A Quantum Vision Transformer: A Naive Approach',
            uniqueId: 'c17249',
            url: '/event/459/contributions/11832/',
          },
          c17250: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11834/attachments/9732/14226/quantumRL_chep23.pptx',
                  id: 14226,
                  title: 'quantumRL_chep23.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11834,
            description:
              'Free energy-based reinforcement learning (FERL) with clamped quantum Boltzmann machines (QBM) was shown to significantly improve the learning efficiency compared to classical Q-learning with the restriction, however, to discrete state-action space environments. We extended FERL to continuous state-action space environments by developing a hybrid actor-critic scheme combining a classical actor-network with a QBM-based critic. Results obtained with quantum annealing, both simulated and with D-Wave quantum annealing hardware, are discussed, and the performance is compared to classical reinforcement learning methods. The method is applied to a variety of particle accelerator environments among which is the actual electron beam line of the Advanced Plasma Wakefield Experiment (AWAKE) at CERN.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 437,
            id: 'c17250',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11834/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'VALLECORSA, SOFIA'],
                emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
                familyName: 'VALLECORSA',
                firstName: 'SOFIA',
                name: 'SOFIA VALLECORSA',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12194,
            sessionSlotId: 2757,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'Hybrid actor-critic scheme for quantum reinforcement learning',
            uniqueId: 'c17250',
            url: '/event/459/contributions/11834/',
          },
          c17251: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11837/attachments/9417/13658/chep23-ms.pdf',
                  id: 13658,
                  title: 'chep23-ms.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11837,
            description:
              'With the emergence of the research field Quantum Machine Learning, interest in finding advantageous real-world applications is growing as well.\r\nHowever challenges concerning the number of available qubits on Noisy Intermediate Scale Quantum (NISQ) devices and accuracy losses due to hardware imperfections still remain and limit the applicability of such approaches in real-world scenarios.\r\nFor simplification, most studies therefore assume nearly noise-free conditions as they are expected with logical, i.e. error-corrected, qubits instead of real qubits provided by hardware.\r\nHowever, the number of logical qubits is expected to scale slowly as they require a high number of real qubits for error correction.\r\nThis is our motivation to deal with noise as an unavoidable, non-negligible problem on NISQ devices.\r\nUsing the example of particle decay tree reconstruction as a highly complex combinatoric problem in High Energy Physics we investigate methods to reduce the noise impact of such devices and propose a hybrid architecture where a classical graph neural network is extended by a parameterized quantum circuit.\r\nWhile we have shown that such a hybrid architecture enables a reduction of the amount of trainable parameters compared to the fully classical case, we are now specifically interested in the actual performance on real quantum devices.\r\nUsing simple synthetic Decay Trees, we train the network in classical simulations to allow for efficient optimization of the parameters.\r\nThe trained parameters are validated on NISQ devices by "IBM Quantum" and are used in interpretability and significance studies, enabling improvements in the accuracy on real devices.\r\nIn summary we improved the results of the existing architecture in terms of the validation performance, on real quantum devices.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 471,
            id: 'c17251',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11837/contribution.pdf',
            presenters: [
              {
                affiliation: 'Karlsruhe Institute of Technology',
                displayOrderKey: [1, 'Strobl, Melvin'],
                emailHash: 'dc3138b08888f5ef792483f78e776076',
                familyName: 'Strobl',
                firstName: 'Melvin',
                name: 'Melvin Strobl',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12194,
            sessionSlotId: 2757,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'Improving Noisy Hybrid Quantum Graph Neural Networks for Particle Decay Tree Reconstruction',
            uniqueId: 'c17251',
            url: '/event/459/contributions/11837/',
          },
          c17252: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11831/attachments/9662/14084/Symmetery_Invariant_CHEP_Jamie_Heredge.pdf',
                  id: 14084,
                  title: 'Symmetery_Invariant_CHEP_Jamie_Heredge.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11831,
            description:
              'In the search for advantage in Quantum Machine Learning, appropriate encoding of the underlying classical data into a quantum state is a critical step. Our previous work [1] implemented an encoding method inspired by underlying physics and achieved AUC scores higher than that of classical techniques for a reduced dataset of B Meson Continuum Suppression data. A particular problem faced by these Quantum SVM techniques was overfitting of training data. One possible method of tackling this is by reducing the expressibility of the model by exploiting symmetries in the data using symmetry invariant encodings. There are often several natural symmetries present in Particle Physics data (permutation of particle order and rotational symmetry) that can be targeted. This presentation demonstrates a method of encoding that guarantees invariance under permuting the ordering of particles in the data input. A downside of this model for more general applicability is the quadratic scaling of ancilla qubits as the number of states to be symmetrised increases. However, data from Particle Physics may only contain a small number of particles in each event, meaning this scaling is not too prohibitive and suggests particle data is well suited to this approach. In addition we explore solutions to this scaling using approximately invariant encoding created through genetic algorithms. As quantum technology develops over the coming decade it is hoped the methods discussed here can form a basis for Quantum Machine Learning model development in a Particle Physics context.\r\n\r\n\r\nReferences:\r\n[1] Heredge, J., Hill, C., Hollenberg, L., Sevior, M., Quantum Support Vector Machines for Continuum Suppression in B Meson Decays. Comput Softw Big Sci 5, 27 (2021). https://doi.org/10.1007/s41781-021-00075-x',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 466,
            id: 'c17252',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11831/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Melbourne',
                displayOrderKey: [1, 'Heredge, Jamie'],
                emailHash: 'b5ad4cbff0c36dad8a3dfdeee35e5305',
                familyName: 'Heredge',
                firstName: 'Jamie',
                name: 'Mr Jamie Heredge',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12194,
            sessionSlotId: 2757,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title:
              'Symmetry Invariant Quantum Machine Learning models for classification problems in Particle Physics',
            uniqueId: 'c17252',
            url: '/event/459/contributions/11831/',
          },
          c17290: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11851/attachments/9731/14230/Talk%201%20SofiaVallecorsa%20CHEP23%20.pdf',
                  id: 14230,
                  title: 'Talk 1 SofiaVallecorsa CHEP23 .pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#4f144e',
            conferenceId: 459,
            contributionId: 11851,
            description:
              'A new theoretical framework in Quantum Machine Learning (QML) allows to compare the performances of Quantum and Classical ML models on supervised learning tasks. We assess the performance of a quantum and classic support vector machine for a High Energy Physics dataset: the Higgs tt \u0304H(b \u0304b) decay dataset, grounding our study in a new theoretical framework based on three metrics: the geometry between the classical and quantum learning spaces, the dimensionality of the feature space, and the complexity of the ML models. Hence, we can exclude those areas where we do not expect any advantage in using quantum models and guide our study through the best parameter configurations. We observe, in a vast parameter region, that the used classical rbf kernel model overtakes the performances of the devised quantum kernels. According to the adopted quantum encoding, the Higgs dataset has been proved to be low dimensional in the quantum feature space. Nevertheless, including a projected quantum kernel, able to reduce the expressivity of the traditional fidelity quantum, a clever optimization of the parameters revealed a potential window of quantum advantage where quantum kernel is able to better classify the Higgs boson events and surpass the classical ML model.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 531,
            id: 'c17290',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11851/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [2, 'VALLECORSA, SOFIA'],
                emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
                familyName: 'VALLECORSA',
                firstName: 'SOFIA',
                name: 'SOFIA VALLECORSA',
              },
            ],
            references: [],
            room: 'Marriott Ballroom VII',
            sessionCode: '',
            sessionId: 2045,
            sessionSlotEntryId: 12194,
            sessionSlotId: 2757,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#ffefff',
            title: 'The Role of Data in Projected Quantum Kernels: the Higgs Boson Discrimination',
            uniqueId: 'c17290',
            url: '/event/459/contributions/11851/',
          },
        },
        entryType: 'Session',
        friendlyId: 15,
        id: 's12194',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2045/session-timetable.pdf',
        room: 'Marriott Ballroom VII',
        sessionCode: '',
        sessionId: 2045,
        sessionSlotId: 2757,
        slotTitle: 'Quantum Computing Applications',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#ffefff',
        title:
          'Track X - Exascale Science, Heterogeneous Computing and Accelerators, and Quantum Computing',
        uniqueId: 's12194',
        url: '/event/459/sessions/2045/',
      },
      s17157: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#b9cbca',
        conferenceId: 459,
        contribDuration: 15.0,
        conveners: [
          {
            affiliation: 'University of Pittsburgh',
            displayOrderKey: [0, 'Bandieramonte, Marilena'],
            emailHash: 'f4b45dbfcfef5c90c1b484b28e777c71',
            familyName: 'Bandieramonte',
            firstName: 'Marilena',
            name: 'Marilena Bandieramonte',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Wenzel, Sandro'],
            emailHash: '7e8bcc2855b51c4e93b19e095a8b2ec9',
            familyName: 'Wenzel',
            firstName: 'Sandro',
            name: 'Sandro Wenzel',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-11',
          time: '15:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17200: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11759/attachments/9730/14224/CHEP_2023.pdf',
                  id: 14224,
                  title: 'CHEP_2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11759,
            description:
              'Reliably simulating detector response to hadrons is crucial for almost all physics programs at the Large Hadron Collider. The core component of such simulation is the modeling of hadronic interactions. Unfortunately, there is no first-principle theory guidance. The current state-of-the-art simulation tool, Geant4, exploits phenomenology-inspired parametric models, each simulating a specific range of hadron energies for some hadron flavor types. These models must be combined to simulate all hadron flavors at all energy ranges. Parameters in each model and the transition region between models must be tuned to match the experimental measurements. Those models may be updated to cope with new measurements. Our work is to make the modeling of hadronic interactions differentiable so that it is easy to tune and to unify all parametric models with one machine learning-based model so that it is easy to maintain and update. To this end, we exploit the conditional normalizing flow models and train them with simulated data. Our work is the first step toward developing a fully differentiable and data-driven simulation model for hadronic interactions for High Energy and Nuclear Physics.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 100,
            id: 'c17200',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11759/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Wisconsin-Madison',
                displayOrderKey: [4, 'Pham, Tuan Minh'],
                emailHash: 'afd5ec0497bd60b3befcbe88c27dc0a7',
                familyName: 'Pham',
                firstName: 'Tuan Minh',
                name: 'Mr Tuan Minh Pham',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17157,
            sessionSlotId: 3654,
            startDate: {
              date: '2023-05-11',
              time: '14:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title: 'Simulation of Hadronic Interactions with Deep Generative Models',
            uniqueId: 'c17200',
            url: '/event/459/contributions/11759/',
          },
          c17201: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11716/attachments/9654/14222/main.pdf',
                  id: 14222,
                  title: 'main.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11716,
            description:
              'The full simulation of particle colliders incurs a significant computational cost. Among the most resource-intensive steps are detector simulations. It is expected that future developments, such as higher collider luminosities and highly granular calorimeters, will increase the computational resource requirement for simulation beyond availability. One possible solution is generative neural networks that can accelerate simulations. Normalizing flows are a promising approach in this pursuit. It has been previously demonstrated, that such flows can generate showers in low-complexity calorimeters with high accuracy. We show how normalizing flows can be improved and adapted for precise shower simulation in significantly more complex calorimeter geometries.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 523,
            id: 'c17201',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11716/contribution.pdf',
            presenters: [
              {
                affiliation: 'Universit\u00e4t Hamburg',
                displayOrderKey: [0, 'Buss, Thorsten'],
                emailHash: 'dd39668462196ff1f45ac4792376ee50',
                familyName: 'Buss',
                firstName: 'Thorsten',
                name: 'Mr Thorsten Buss',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17157,
            sessionSlotId: 3654,
            startDate: {
              date: '2023-05-11',
              time: '14:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title:
              'Generating Accurate Showers in Highly Granular Calorimeters Using Normalizing Flows',
            uniqueId: 'c17201',
            url: '/event/459/contributions/11716/',
          },
          c17202: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11731/attachments/9707/14228/CaloChallenge.pptx',
                  id: 14228,
                  title: 'CaloChallenge.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11731,
            description:
              'For High Energy Physics (HEP) experiments, the calorimeter is a key detector to measure the energy of particles. Particles interact with the material of the calorimeter, creating cascades of secondary particles, the so-called showers. Description of the showering process relies on simulation methods that precisely describe all particle interactions with matter. Constrained by the complexity of the calorimeter geometry and the need to accurately simulate the interaction with each material, the simulation of calorimeters is inherently slow and constitutes a bottleneck for current and future HEP analysis. In order to spur the development and benchmarking of fast and high-fidelity simulation, the first-ever fast calorimeter simulation challenge \u201cCaloChallenge\u201d was proposed. The challenge offers a common benchmark of performance metrics and  three realistic datasets, ranging in difficulty from easy to medium to hard.  This contribution highlights an initial analysis of submitted results using new approaches of generative models.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 549,
            id: 'c17202',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11731/contribution.pdf',
            presenters: [
              {
                affiliation: 'INFN Roma Tor Vergata',
                displayOrderKey: [6, 'Faucci Giannelli, Michele'],
                emailHash: 'b599f4a9469b5ebe0ca1932a087a8722',
                familyName: 'Faucci Giannelli',
                firstName: 'Michele',
                name: 'Dr Michele Faucci Giannelli',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17157,
            sessionSlotId: 3654,
            startDate: {
              date: '2023-05-11',
              time: '14:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title: 'The CaloChallenge insights \u0026 findings',
            uniqueId: 'c17202',
            url: '/event/459/contributions/11731/',
          },
          c17203: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11736/attachments/9599/14176/CHEP23_CaloDiffusion.pdf',
                  id: 14176,
                  title: 'CHEP23_CaloDiffusion.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11736,
            description:
              'Simulation is a crucial part of all aspects of collider data analysis. However, the computing challenges of the High Luminosity era will require simulation to use a smaller fraction of computing resources, at the same time as more complex detectors are introduced, requiring more detailed simulation. This motivates the use of machine learning (ML) models as surrogates to replace full physics-based detector simulation. Recently in the ML community, a new class of models based on diffusion have become state of the art for generating high quality images with reasonable computation times. In this work, we study the application of diffusion models to generate simulated calorimeter showers. In order to reduce the computational burden of the method, we explore compressing the calorimeter shower into a smaller latent space for the diffusion process. Optimization of this latent space, the handling of irregular detector geometries, and comparisons to other generative models will be discussed. We will also discuss the possibility of using diffusion models to enhance, or denoise, existing physics-based fast simulations as an alternative to the fully generative approach.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 54,
            id: 'c17203',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11736/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermilab',
                displayOrderKey: [1, 'Amram, Oz'],
                emailHash: '09d4ef9715fd5d3fedef533f57fd2e18',
                familyName: 'Amram',
                firstName: 'Oz',
                name: 'Oz Amram',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17157,
            sessionSlotId: 3654,
            startDate: {
              date: '2023-05-11',
              time: '14:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title: 'Fast and Accurate Calorimeter Simulation with Diffusion Models',
            uniqueId: 'c17203',
            url: '/event/459/contributions/11736/',
          },
          c17204: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11742/attachments/9625/14221/Copy%20of%20CHEP\u002723%20Transformers%20for%20FastSim.pdf',
                  id: 14221,
                  title: 'Copy of CHEP\u002723 Transformers for FastSim.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/11742/attachments/9625/14007/go',
                  id: 14007,
                  title: 'Transformers FastSim',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11742,
            description:
              'Recently, transformers have proven to be a generalized architecture for various data modalities, i.e., ranging from text (BERT, GPT3), time series (PatchTST) to images (ViT) and even a combination of them (Dall-E 2, OpenAI Whisper). Additionally, when given enough data, transformers can learn better representations than other deep learning models thanks to the absence of inductive bias, better modeling of long-range dependencies, and interpolation and extrapolation capabilities. Therefore, the transformer is a promising model to be explored for fast shower simulation, where the goal is to generate synthetic particle showers, i.e., the energy depositions in the calorimeter. The transformer should accurately model the non-trivial structure of particle showers, as well as quickly adapt to new detector geometries. Furthermore, the attention mechanism in transformers enables the model to better learn the complex conditional distribution of energy depositions in the detector. In this work, we will present how transformers can be used for accurate and fast shower simulation, as well as the know-how on transformer architecture, input data representation, sequence formation, and learning mechanism.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 405,
            id: 'c17204',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11742/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [1, 'Raikwar, Piyush'],
                emailHash: '51834ded552abd49a3e367f9c6e4e260',
                familyName: 'Raikwar',
                firstName: 'Piyush',
                name: 'Piyush Raikwar',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17157,
            sessionSlotId: 3654,
            startDate: {
              date: '2023-05-11',
              time: '15:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title: 'Transformers for Generalized Fast Shower Simulation',
            uniqueId: 'c17204',
            url: '/event/459/contributions/11742/',
          },
          c17205: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/11725/attachments/9653/14070/20230511chep_MoritzWolf_Refinement_v2.pdf',
                  id: 14070,
                  title: '20230511chep_MoritzWolf_Refinement_v2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#b9cbca',
            conferenceId: 459,
            contributionId: 11725,
            description:
              'At the CMS experiment, a growing reliance on the fast Monte Carlo application (FastSim) will accompany the high luminosity and detector granularity expected in Phase 2. The FastSim chain is roughly 10 times faster than the application based on the GEANT4 detector simulation and full reconstruction referred to as FullSim. However, this advantage comes at the price of decreased accuracy in some of the final analysis observables. In this contribution, a machine learning-based technique to refine those observables is presented. We employ a regression neural network trained with a sophisticated combination of multiple loss functions to provide post-hoc corrections to samples produced by the FastSim chain. The results show considerably improved agreement with the FullSim output and an improvement in correlations among output observables and external parameters. This technique is a promising replacement for existing correction factors, providing higher accuracy and thus contributing to the wider usage of FastSim.',
            duration: 15.0,
            endDate: {
              date: '2023-05-11',
              time: '15:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 83,
            id: 'c17205',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/11725/contribution.pdf',
            presenters: [
              {
                affiliation: 'Hamburg University',
                displayOrderKey: [0, 'Wolf, Moritz'],
                emailHash: 'bf5133a0a907dfea658b301e56fde64f',
                familyName: 'Wolf',
                firstName: 'Moritz',
                name: 'Moritz Wolf',
              },
            ],
            references: [],
            room: 'Marriott Ballroom I',
            sessionCode: '',
            sessionId: 2561,
            sessionSlotEntryId: 17157,
            sessionSlotId: 3654,
            startDate: {
              date: '2023-05-11',
              time: '15:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#0f0202',
            title: 'Refining fast simulation using machine learning',
            uniqueId: 'c17205',
            url: '/event/459/contributions/11725/',
          },
        },
        entryType: 'Session',
        friendlyId: 17,
        id: 's17157',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2561/session-timetable.pdf',
        room: 'Marriott Ballroom I',
        sessionCode: '',
        sessionId: 2561,
        sessionSlotId: 3654,
        slotTitle: 'FastSim',
        startDate: {
          date: '2023-05-11',
          time: '14:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f0202',
        title: 'Track 3+9 Crossover',
        uniqueId: 's17157',
        url: '/event/459/sessions/2561/',
      },
    },
    '20230512': {
      b12140: {
        _fossil: 'breakTimeSchEntry',
        _type: 'BreakTimeSchEntry',
        color: '#dfebff',
        conferenceId: 459,
        description: '',
        duration: 30.0,
        endDate: {
          date: '2023-05-12',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        entryType: 'Break',
        id: 'b12140',
        inheritLoc: false,
        inheritRoom: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom (3rd floor)',
        sessionCode: null,
        sessionId: null,
        sessionSlotEntryId: null,
        sessionSlotId: null,
        startDate: {
          date: '2023-05-12',
          time: '10:30:00',
          tz: 'US/Eastern',
        },
        textColor: '#0f264f',
        title: 'AM Break',
        uniqueId: 'b12140',
      },
      s12138: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#dfdfdf',
        conferenceId: 459,
        contribDuration: 600.0,
        conveners: [],
        description: '',
        duration: 60.0,
        endDate: {
          date: '2023-05-12',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        entries: {},
        entryType: 'Session',
        friendlyId: 4,
        id: 's12138',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2034/session-timetable.pdf',
        room: '',
        sessionCode: '',
        sessionId: 2034,
        sessionSlotId: 2707,
        slotTitle: 'Registration',
        startDate: {
          date: '2023-05-12',
          time: '08:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#151515',
        title: 'Registration',
        uniqueId: 's12138',
        url: '/event/459/sessions/2034/',
      },
      s12139: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'University Nebraska-Lincoln (US)',
            displayOrderKey: [0, 'Shadura, Oksana'],
            emailHash: 'f5e7378e16e9fee96ad835deb6454086',
            familyName: 'Shadura',
            firstName: 'Oksana',
            name: 'Oksana Shadura',
          },
          {
            affiliation: 'BNL',
            displayOrderKey: [0, 'Laycock, Paul'],
            emailHash: 'e96ab9c0e5bf8a76e37b517cc58021c0',
            familyName: 'Laycock',
            firstName: 'Paul',
            name: 'Paul Laycock',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-12',
          time: '10:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17462: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12622/attachments/9739/14268/track1_summary.pdf',
                  id: 14268,
                  title: 'track1_summary.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12622,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '09:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 659,
            id: 'c17462',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12622/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Barisits, Martin'],
                emailHash: '46207806844c107a8fefe566f669b206',
                familyName: 'Barisits',
                firstName: 'Martin',
                name: 'Martin Barisits',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12139,
            sessionSlotId: 2708,
            startDate: {
              date: '2023-05-12',
              time: '09:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 1 Highlights',
            uniqueId: 'c17462',
            url: '/event/459/contributions/12622/',
          },
          c17463: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12623/attachments/9736/14272/CHEP2023_summary_track2.pdf',
                  id: 14272,
                  title: 'CHEP2023_summary_track2.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12623,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '09:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 660,
            id: 'c17463',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12623/contribution.pdf',
            presenters: [
              {
                affiliation: 'KEK',
                displayOrderKey: [0, 'Yamada, Satoru'],
                emailHash: 'f2ae8a15fd4a3fafb55ff71831b9c00c',
                familyName: 'Yamada',
                firstName: 'Satoru',
                name: 'Satoru Yamada',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12139,
            sessionSlotId: 2708,
            startDate: {
              date: '2023-05-12',
              time: '09:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 2 Highlights',
            uniqueId: 'c17463',
            url: '/event/459/contributions/12623/',
          },
          c17464: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12624/attachments/9741/14271/Track3_Highligths_MBandieramonte.pdf',
                  id: 14271,
                  title: 'Track3_Highligths_MBandieramonte.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12624,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '09:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 661,
            id: 'c17464',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12624/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Pittsburgh',
                displayOrderKey: [0, 'Bandieramonte, Marilena'],
                emailHash: 'f4b45dbfcfef5c90c1b484b28e777c71',
                familyName: 'Bandieramonte',
                firstName: 'Marilena',
                name: 'Marilena Bandieramonte',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12139,
            sessionSlotId: 2708,
            startDate: {
              date: '2023-05-12',
              time: '09:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 3 Highlights',
            uniqueId: 'c17464',
            url: '/event/459/contributions/12624/',
          },
          c17465: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url: '/event/459/contributions/12625/attachments/9743/14276/Track4.pdf',
                  id: 14276,
                  title: 'Track4.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12625,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '10:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 662,
            id: 'c17465',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12625/contribution.pdf',
            presenters: [
              {
                affiliation: 'STFC-RAL',
                displayOrderKey: [0, 'Ellis, Katy'],
                emailHash: 'e4c8a78292759108e5511297386cac3a',
                familyName: 'Ellis',
                firstName: 'Katy',
                name: 'Katy Ellis',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12139,
            sessionSlotId: 2708,
            startDate: {
              date: '2023-05-12',
              time: '09:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 4 Highlights',
            uniqueId: 'c17465',
            url: '/event/459/contributions/12625/',
          },
          c17466: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12626/attachments/9738/15837/CHEP2023%20Track%205%20Summary%20Talk%20(2).pdf',
                  id: 15837,
                  title: 'CHEP2023 Track 5 Summary Talk (2).pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12626,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '10:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 663,
            id: 'c17466',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12626/contribution.pdf',
            presenters: [
              {
                affiliation: 'FNAL',
                displayOrderKey: [0, 'Sexton, Elizabeth'],
                emailHash: 'e8fb00f4f09841337efaeff677169cd2',
                familyName: 'Sexton',
                firstName: 'Elizabeth',
                name: 'Elizabeth Sexton',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12139,
            sessionSlotId: 2708,
            startDate: {
              date: '2023-05-12',
              time: '10:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 5 Highlights',
            uniqueId: 'c17466',
            url: '/event/459/contributions/12626/',
          },
          c17467: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12627/attachments/9740/14269/23.05.12%20-%20CHEP%20Track%206%20-%20SH.pdf',
                  id: 14269,
                  title: '23.05.12 - CHEP Track 6 - SH.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12627,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '10:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 664,
            id: 'c17467',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12627/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'Hageboeck, Stephan'],
                emailHash: '64d6d832b57aa9e7597d4011228b0da0',
                familyName: 'Hageboeck',
                firstName: 'Stephan',
                name: 'Stephan Hageboeck',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12139,
            sessionSlotId: 2708,
            startDate: {
              date: '2023-05-12',
              time: '10:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 6 Highlights',
            uniqueId: 'c17467',
            url: '/event/459/contributions/12627/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's12139',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2708,
        slotTitle: 'Track 1--6 Highlights',
        startDate: {
          date: '2023-05-12',
          time: '09:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's12139',
        url: '/event/459/sessions/2024/',
      },
      s12141: {
        attachments: {
          files: null,
          folders: [],
        },
        code: '',
        color: '#0d316f',
        conferenceId: 459,
        contribDuration: 10.0,
        conveners: [
          {
            affiliation: 'INFN - Genova',
            displayOrderKey: [0, 'De Vita, Raffaella'],
            emailHash: 'a0b7e3fd0fff74d1182c043e3be1dc3c',
            familyName: 'De Vita',
            firstName: 'Raffaella',
            name: 'Raffaella De Vita',
          },
          {
            affiliation: 'CERN',
            displayOrderKey: [0, 'Espinal, Xavier'],
            emailHash: 'bce5c14595f76db054a5c96d090c8d11',
            familyName: 'Espinal',
            firstName: 'Xavier',
            name: 'Xavier Espinal',
          },
        ],
        description: '',
        duration: 90.0,
        endDate: {
          date: '2023-05-12',
          time: '12:30:00',
          tz: 'US/Eastern',
        },
        entries: {
          c17468: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12628/attachments/9744/14277/Track7_Summary.pdf',
                  id: 14277,
                  title: 'Track7_Summary.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12628,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 665,
            id: 'c17468',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12628/contribution.pdf',
            presenters: [
              {
                affiliation: 'University of Massachusetts Amherst',
                displayOrderKey: [0, 'Martinez Outschoorn, Verena Ingrid'],
                emailHash: 'ab783ab972ac7d26e85752a261953200',
                familyName: 'Martinez Outschoorn',
                firstName: 'Verena Ingrid',
                name: 'Verena Ingrid Martinez Outschoorn',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12141,
            sessionSlotId: 2709,
            startDate: {
              date: '2023-05-12',
              time: '11:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 7 Highlights',
            uniqueId: 'c17468',
            url: '/event/459/contributions/12628/',
          },
          c17469: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12629/attachments/9747/14282/Track%208%20Highlights%20-%20CHEP%202023.pdf',
                  id: 14282,
                  title: 'Track 8 Highlights - CHEP 2023.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12629,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 666,
            id: 'c17469',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12629/contribution.pdf',
            presenters: [
              {
                affiliation: 'DESY',
                displayOrderKey: [0, 'Hernandez Villanueva, Michel'],
                emailHash: '19113800d40b0ae945d3d902953e2b1c',
                familyName: 'Hernandez Villanueva',
                firstName: 'Michel',
                name: 'Michel Hernandez Villanueva',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12141,
            sessionSlotId: 2709,
            startDate: {
              date: '2023-05-12',
              time: '11:15:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 8 Highlights',
            uniqueId: 'c17469',
            url: '/event/459/contributions/12629/',
          },
          c17470: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12630/attachments/9746/14281/CHEP2023%20Track9%20highlights.pptx',
                  id: 14281,
                  title: 'CHEP2023 Track9 highlights.pptx',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12630,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 667,
            id: 'c17470',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12630/contribution.pdf',
            presenters: [
              {
                affiliation: 'CERN',
                displayOrderKey: [0, 'VALLECORSA, SOFIA'],
                emailHash: '42eeb1ebb171454cf2f5b652c7168a8c',
                familyName: 'VALLECORSA',
                firstName: 'SOFIA',
                name: 'SOFIA VALLECORSA',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12141,
            sessionSlotId: 2709,
            startDate: {
              date: '2023-05-12',
              time: '11:30:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track 9 Highlights',
            uniqueId: 'c17470',
            url: '/event/459/contributions/12630/',
          },
          c17471: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12631/attachments/9742/14283/trackx_summaryslides.pdf',
                  id: 14283,
                  title: 'trackx_summaryslides.pdf',
                },
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12631/attachments/9742/14275/trackx_talk_notes.rtf',
                  id: 14275,
                  title: 'trackx_talk_notes.rtf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12631,
            description: '',
            duration: 15.0,
            endDate: {
              date: '2023-05-12',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 668,
            id: 'c17471',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12631/contribution.pdf',
            presenters: [
              {
                affiliation: 'Fermi National Accelerator Laboratory',
                displayOrderKey: [0, 'Timm, Steven'],
                emailHash: '40848b6b99ba5ca40be682891f2cc812',
                familyName: 'Timm',
                firstName: 'Steven',
                name: 'Steven Timm',
              },
            ],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12141,
            sessionSlotId: 2709,
            startDate: {
              date: '2023-05-12',
              time: '11:45:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Track X Highlights',
            uniqueId: 'c17471',
            url: '/event/459/contributions/12631/',
          },
          c17472: {
            _fossil: 'contribSchEntryDisplay',
            _type: 'ContribSchEntry',
            attachments: {
              files: [
                {
                  _fossil: 'attachment',
                  _type: 'Attachment',
                  download_url:
                    '/event/459/contributions/12632/attachments/9745/14280/CHEP23-LOC-Close.pdf',
                  id: 14280,
                  title: 'CHEP23-LOC-Close.pdf',
                },
              ],
              folders: [],
            },
            board_number: '',
            code: '',
            color: '#0d316f',
            conferenceId: 459,
            contributionId: 12632,
            description: '',
            duration: 20.0,
            endDate: {
              date: '2023-05-12',
              time: '12:20:00',
              tz: 'US/Eastern',
            },
            entryType: 'Contribution',
            friendlyId: 669,
            id: 'c17472',
            inheritLoc: true,
            inheritRoom: true,
            location: 'Norfolk Waterside Marriott',
            pdf: '/event/459/contributions/12632/contribution.pdf',
            presenters: [],
            references: [],
            room: 'Norfolk Ballroom III-V',
            sessionCode: '',
            sessionId: 2024,
            sessionSlotEntryId: 12141,
            sessionSlotId: 2709,
            startDate: {
              date: '2023-05-12',
              time: '12:00:00',
              tz: 'US/Eastern',
            },
            textColor: '#eff5ff',
            title: 'Conference Wrap-up',
            uniqueId: 'c17472',
            url: '/event/459/contributions/12632/',
          },
        },
        entryType: 'Session',
        friendlyId: 2,
        id: 's12141',
        inheritLoc: true,
        inheritRoom: true,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        pdf: '/event/459/sessions/2024/session-timetable.pdf',
        room: 'Norfolk Ballroom III-V',
        sessionCode: '',
        sessionId: 2024,
        sessionSlotId: 2709,
        slotTitle: 'Track 7--X Highlights and Close out',
        startDate: {
          date: '2023-05-12',
          time: '11:00:00',
          tz: 'US/Eastern',
        },
        textColor: '#eff5ff',
        title: 'Plenary Session',
        uniqueId: 's12141',
        url: '/event/459/sessions/2024/',
      },
    },
  };
  
  export const eventInfo = {
    _type: 'Conference',
    endDate: {date: '2023-05-12', time: '16:00:00', tz: 'US/Eastern'},
    id: '459',
    isConference: true,
    sessions: {
      '2023': {
        _type: 'Session',
        address: '235 East Main Street\nNorfolk, VA 23510',
        color: '#ecc495',
        description: '',
        id: 2023,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: '',
        roomFullname: '',
        textColor: '#1f1100',
        title: 'Workshop',
        url: '/event/459/sessions/2023/',
      },
      '2024': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#0d316f',
        description: '',
        id: 2024,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Norfolk Ballroom III-V',
        roomFullname: 'Norfolk Ballroom III-V',
        textColor: '#eff5ff',
        title: 'Plenary Session',
        url: '/event/459/sessions/2024/',
      },
      '2033': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#5f171a',
        description: '',
        id: 2033,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom and Foyer Area',
        roomFullname: 'Hampton Roads Ballroom and Foyer Area',
        textColor: '#ffffff',
        title: 'Poster Session',
        url: '/event/459/sessions/2033/',
      },
      '2034': {
        _type: 'Session',
        address: '235 East Main Street\nNorfolk, VA 23510',
        color: '#dfdfdf',
        description: '',
        id: 2034,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: '',
        roomFullname: '',
        textColor: '#151515',
        title: 'Registration',
        url: '/event/459/sessions/2034/',
      },
      '2035': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#6f390d',
        description: '',
        id: 2035,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Norfolk Ballroom III-V',
        roomFullname: 'Norfolk Ballroom III-V',
        textColor: '#ffeddf',
        title: 'Track 1 - Data and Metadata Organization, Management and Access',
        url: '/event/459/sessions/2035/',
      },
      '2036': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#d0c296',
        description: '',
        id: 2036,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Marriott Ballroom V-VI',
        roomFullname: 'Marriott Ballroom V-VI',
        textColor: '#000000',
        title: 'Track 2 - Online Computing',
        url: '/event/459/sessions/2036/',
      },
      '2037': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#d9dfc3',
        description: '',
        id: 2037,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom VI',
        roomFullname: 'Hampton Roads Ballroom VI',
        textColor: '#272f09',
        title: 'Track 3 - Offline Computing',
        url: '/event/459/sessions/2037/',
      },
      '2038': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#efebc2',
        description: '',
        id: 2038,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Marriott Ballroom II-III',
        roomFullname: 'Marriott Ballroom II-III',
        textColor: '#202020',
        title: 'Track 4 - Distributed Computing',
        url: '/event/459/sessions/2038/',
      },
      '2039': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#dfe555',
        description: '',
        id: 2039,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Chesapeake Meeting Room',
        roomFullname: 'Chesapeake Meeting Room',
        textColor: '#202020',
        title: 'Track 5 - Sustainable and Collaborative Software Engineering',
        url: '/event/459/sessions/2039/',
      },
      '2040': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#feffbf',
        description: '',
        id: 2040,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads Ballroom VIII',
        roomFullname: 'Hampton Roads Ballroom VIII',
        textColor: '#1f1f02',
        title: 'Track 6 - Physics Analysis Tools',
        url: '/event/459/sessions/2040/',
      },
      '2041': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#1a3f14',
        description: '',
        id: 2041,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Marriott Ballroom IV',
        roomFullname: 'Marriott Ballroom IV',
        textColor: '#f1ffef',
        title: 'Track 7 - Facilities and Virtualization',
        url: '/event/459/sessions/2041/',
      },
      '2042': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#e3f2d3',
        description: '',
        id: 2042,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Marriott Ballroom I',
        roomFullname: 'Marriott Ballroom I',
        textColor: '#253f08',
        title: 'Track 8 - Collaboration, Reinterpretation, Outreach and Education',
        url: '/event/459/sessions/2042/',
      },
      '2043': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#92b6db',
        description: '',
        id: 2043,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Hampton Roads VII',
        roomFullname: 'Hampton Roads VII',
        textColor: '#03070f',
        title: 'Track 9 - Artificial Intelligence and Machine Learning',
        url: '/event/459/sessions/2043/',
      },
      '2044': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#c2ecef',
        description: '',
        id: 2044,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: '',
        roomFullname: '',
        textColor: '#0d1e1f',
        title: 'Track 10 - Exascale Science',
        url: '/event/459/sessions/2044/',
      },
      '2045': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#4f144e',
        description: '',
        id: 2045,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Marriott Ballroom VII',
        roomFullname: 'Marriott Ballroom VII',
        textColor: '#ffefff',
        title:
          'Track X - Exascale Science, Heterogeneous Computing and Accelerators, and Quantum Computing',
        url: '/event/459/sessions/2045/',
      },
      '2046': {
        _type: 'Session',
        address: '235 East Main Street Norfolk, VA 23510',
        color: '#eee0ef',
        description: '',
        id: 2046,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: '',
        roomFullname: '',
        textColor: '#1d041f',
        title: 'Track 12 - Quantum Computing',
        url: '/event/459/sessions/2046/',
      },
      '2561': {
        _type: 'Session',
        address: '235 East Main Street\nNorfolk, VA 23510',
        color: '#b9cbca',
        description: '',
        id: 2561,
        isPoster: false,
        location: 'Norfolk Waterside Marriott',
        room: 'Marriott Ballroom I',
        roomFullname: 'Marriott Ballroom I',
        textColor: '#0f0202',
        title: 'Track 3+9 Crossover',
        url: '/event/459/sessions/2561/',
      },
    },
    startDate: {date: '2023-05-08', time: '08:00:00', tz: 'US/Eastern'},
    title:
      '26TH INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY \u0026 NUCLEAR PHYSICS (CHEP2023)',
  };